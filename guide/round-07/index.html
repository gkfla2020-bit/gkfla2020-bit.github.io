<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Round 7 - Deep Learning: ANN, CNN, RNN, LSTM for Algorithmic Trading</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300;400;500&family=Space+Mono:wght@400&family=Inter:wght@300;400&display=swap" rel="stylesheet">
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:'Inter',sans-serif;background:#fafaf8;color:#1a1a1a;line-height:1.7;overflow-x:hidden}
.sidebar{position:fixed;left:0;top:0;width:260px;height:100vh;background:rgba(255,255,255,.97);border-right:1px solid rgba(0,0,0,.06);padding:32px 24px;z-index:100;overflow-y:auto;display:flex;flex-direction:column}
.sidebar-profile{text-align:center;margin-bottom:28px;padding-bottom:24px;border-bottom:1px solid rgba(0,0,0,.08)}
.profile-icon{font-size:48px;margin-bottom:8px}
.profile-name{font-family:'Cormorant Garamond',serif;font-size:1.3rem;font-weight:500;margin-bottom:4px}
.profile-title{font-size:.68rem;color:#888;letter-spacing:.08em;text-transform:uppercase;margin-bottom:8px}
.profile-bio{font-size:.78rem;color:#666;line-height:1.5}
.sidebar-nav{flex:1;margin-top:16px}
.nav-section{margin-bottom:20px}
.nav-section-title{font-size:.6rem;font-weight:600;color:#aaa;letter-spacing:.15em;text-transform:uppercase;margin-bottom:10px}
.nav-list{list-style:none}
.nav-list li{margin-bottom:5px}
.nav-list a{font-size:.78rem;color:#555;text-decoration:none;transition:all .2s;display:block;padding:3px 0}
.nav-list a:hover{color:#0080c6;padding-left:4px}
.nav-list a.active{color:#0080c6;font-weight:500}
.nav-list a.done{color:#28a745}
.badge{display:inline-block;font-size:.5rem;background:#0080c6;color:#fff;padding:1px 5px;border-radius:8px;margin-left:3px;vertical-align:middle}
.badge-done{background:#28a745}
.sidebar-footer{padding-top:16px;border-top:1px solid rgba(0,0,0,.06);font-size:.65rem;color:#aaa;text-align:center}
.main-wrapper{margin-left:260px;min-height:100vh}
.container{max-width:1100px;margin:0 auto;padding:50px 40px 80px}
.paper-content{font-family:'Times New Roman','Nanum Myeongjo',serif;line-height:1.8;background:#fff;padding:40px;border-radius:8px;box-shadow:0 2px 20px rgba(0,0,0,.05)}
.paper-header{text-align:center;margin-bottom:40px;padding-bottom:30px;border-bottom:2px solid #333}
.paper-category{font-size:14px;color:#666;margin-bottom:10px}
.paper-title{font-size:24px;font-weight:bold;margin-bottom:12px;line-height:1.4}
.paper-subtitle{font-size:14px;color:#555;margin-bottom:8px}
.paper-team{font-size:13px;color:#444}
.code-output{background:#1e1e1e;color:#d4d4d4;padding:12px 16px;border-radius:0 0 6px 6px;font-family:'Space Mono',monospace;font-size:11.5px;line-height:1.6;margin-top:-4px;margin-bottom:18px;border-top:2px solid #333;white-space:pre-wrap;overflow-x:auto}
.code-output .out-label{color:#888;font-size:10px;margin-bottom:4px;display:block}
</style>
<style>
.abstract{background:#f8f9fa;padding:25px;margin:30px 0;border-left:4px solid #2c3e50}
.abstract-title{font-weight:bold;font-size:16px;margin-bottom:15px}
h2{font-size:18px;margin:35px 0 20px;padding-bottom:8px;border-bottom:1px solid #ddd;color:#2c3e50}
h3{font-size:15px;margin:25px 0 15px;color:#34495e}
h4{font-size:14px;margin:20px 0 12px;color:#34495e}
p{text-align:justify;margin-bottom:15px;text-indent:2em}
p.ni{text-indent:0}
table{width:100%;border-collapse:collapse;margin:20px 0;font-size:12px}
th,td{border:1px solid #ddd;padding:10px 8px;text-align:center}
th{background:#2c3e50;color:white;font-weight:bold}
tr:nth-child(even){background:#f8f9fa}
tr:hover{background:#e8f4f8}
.tc{font-size:13px;font-weight:bold;margin:15px 0 10px;text-align:center}
.eq{text-align:center;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:4px;overflow-x:auto}
ul,ol{margin-left:2em;margin-bottom:15px}
li{margin-bottom:6px}
.def{background:#fff9e6;border:1px solid #ffc107;border-radius:4px;padding:20px;margin:20px 0}
.info{background:#e8f4f8;border-left:4px solid #3498db;padding:20px;margin:20px 0}
.warn{background:#fff3cd;border-left:4px solid #f39c12;padding:20px;margin:20px 0}
.ok{background:#d4edda;border-left:4px solid #28a745;padding:20px;margin:20px 0}
pre{background:#1e1e1e;color:#d4d4d4;padding:20px;border-radius:6px;overflow-x:auto;margin:20px 0;font-family:'Space Mono','Consolas',monospace;font-size:13px;line-height:1.6}
code{font-family:'Space Mono','Consolas',monospace;font-size:13px}
p code,li code,td code{background:#f0f0f0;padding:2px 6px;border-radius:3px;color:#c7254e;font-size:12px}
.cc{font-size:12px;font-weight:bold;color:#2c3e50;margin-top:15px;margin-bottom:4px}
.cm{color:#6a9955}.kw{color:#569cd6}.st{color:#ce9178}.fn{color:#dcdcaa}.nb{color:#4ec9b0}.nu{color:#b5cea8}
.progress-bar{width:100%;height:6px;background:#e0e0e0;border-radius:3px;margin-top:16px}
.progress-fill{height:100%;background:linear-gradient(90deg,#0080c6,#00b894);border-radius:3px;width:70%}
.progress-label{font-size:11px;color:#888;margin-top:4px;text-align:center}
@media(max-width:1024px){
.sidebar{width:100%;height:auto;position:relative;border-right:none;border-bottom:1px solid rgba(0,0,0,.08);padding:16px}
.sidebar-profile{margin-bottom:10px;padding-bottom:10px;display:flex;align-items:center;gap:12px;text-align:left}
.profile-icon{font-size:32px;margin-bottom:0}.profile-bio{display:none}
.nav-section{display:inline-block;margin-right:16px;margin-bottom:8px}
.nav-list{display:flex;gap:10px;flex-wrap:wrap}.nav-list li{margin-bottom:0}
.sidebar-footer{display:none}
.main-wrapper{margin-left:0}
.container{padding:0}.paper-content{padding:20px 16px;border-radius:0;box-shadow:none}
.paper-title{font-size:18px}p{font-size:14px;text-indent:1.5em;text-align:left}
pre{font-size:11px;padding:14px}table{font-size:10px;display:block;overflow-x:auto}
}
</style>
</head>
<body>

<div class="sidebar">
<div class="sidebar-profile">
<div class="profile-icon">&#x1F9E0;</div>
<div class="profile-name">HFT ML Master Plan</div>
<div class="profile-title">Convex Opt + DL + HFT</div>
<div class="profile-bio">10 Rounds: Zero to HFT System Trading</div>
</div>
<div class="sidebar-nav">
<div class="nav-section">
<div class="nav-section-title">Curriculum</div>
<ul class="nav-list">
<li><a class="done" href="../round_01/lecture_01.html">R1. Python + Finance <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_02/lecture_02.html">R2. Linear Algebra + Stats <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_03/lecture_03.html">R3. Data / Feature Eng. <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_04/lecture_04.html">R4. Supervised Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_05/lecture_05.html">R5. Unsupervised + TS <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_06/lecture_06.html">R6. NLP + Sentiment <span class="badge badge-done">DONE</span></a></li>
<li><a class="active" href="#">R7. Deep Learning <span class="badge">NOW</span></a></li>
<li><a href="#">R8. Convex Opt + Transformer</a></li>
<li><a href="#">R9. HFT + RL</a></li>
<li><a href="#">R10. Final Project</a></li>
</ul>
</div>
<div class="nav-section">
<div class="nav-section-title">This Lecture</div>
<ul class="nav-list">
<li><a href="#ch1">1. ì™œ ë”¥ëŸ¬ë‹ì¸ê°€</a></li>
<li><a href="#ch2">2. í¼ì…‰íŠ¸ë¡ </a></li>
<li><a href="#ch3">3. í™œì„±í™” í•¨ìˆ˜</a></li>
<li><a href="#ch4">4. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)</a></li>
<li><a href="#ch5">5. ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜</a></li>
<li><a href="#ch6">6. ê²½ì‚¬í•˜ê°•ë²• ë³€í˜•</a></li>
<li><a href="#ch7">7. PyTorch ê¸°ì´ˆ</a></li>
<li><a href="#ch8">8. CNN ì•„í‚¤í…ì²˜</a></li>
<li><a href="#ch9">9. CNN ê¸ˆìœµ ì ìš©</a></li>
<li><a href="#ch10">10. RNN ê¸°ì´ˆ</a></li>
<li><a href="#ch11">11. LSTM / GRU</a></li>
<li><a href="#ch12">12. LSTM ê¸ˆìœµ ì‹œê³„ì—´</a></li>
<li><a href="#ch13">13. Quiz + Mini Project</a></li>
</ul>
</div>
</div>
<div class="sidebar-footer">Round 7 of 10 Â· ğŸ§  Deep Learning</div>
</div>

<div class="main-wrapper">
<div class="container">
<div class="paper-content">

<div class="paper-header">
<div class="paper-category">Round 7 / 10 Â· ë”¥ëŸ¬ë‹ + ìµœì í™”</div>
<h1 class="paper-title">Deep Learning for Algorithmic Trading:<br>ANN, CNN, RNN &amp; LSTM</h1>
<div class="paper-subtitle">í¼ì…‰íŠ¸ë¡ ì—ì„œ LSTMê¹Œì§€ â€” ì‹ ê²½ë§ì´ ì‹œì¥ì˜ ë¹„ì„ í˜• íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•</div>
<div class="paper-team">Textbooks: MLAT Ch.17~19 / MLDSF Ch.3, Ch.5~6</div>
<div class="progress-bar"><div class="progress-fill"></div></div>
<div class="progress-label">Overall Progress: 70%</div>
</div>

<div class="abstract">
<div class="abstract-title">Abstract</div>
<p class="ni">
R1~R6ê¹Œì§€ ìš°ë¦¬ëŠ” ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ì˜ í•µì‹¬ ë¬´ê¸°ë¥¼ ëª¨ë‘ ê°–ì·„ë‹¤. ì„ í˜•íšŒê·€ì—ì„œ XGBoostê¹Œì§€ì˜ ì§€ë„í•™ìŠµ, PCAì™€ K-Meansì˜ ë¹„ì§€ë„í•™ìŠµ, ARIMA/GARCHì˜ ì‹œê³„ì—´ ëª¨ë¸, ê·¸ë¦¬ê³  NLPê¹Œì§€. í•˜ì§€ë§Œ ì´ ëª¨ë“  ëª¨ë¸ì—ëŠ” ê³µí†µëœ í•œê³„ê°€ ìˆë‹¤ â€” <strong>í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì‚¬ëŒì´ ì§ì ‘ í•´ì•¼ í•œë‹¤</strong>ëŠ” ê²ƒì´ë‹¤. RSI, MACD, ë³¼ë¦°ì €ë°´ë“œ ê°™ì€ ê¸°ìˆ ì  ì§€í‘œë¥¼ ìš°ë¦¬ê°€ ì„¤ê³„í•˜ê³ , ê·¸ ì§€í‘œë“¤ì„ ëª¨ë¸ì— ë„£ì–´ì¤˜ì•¼ í–ˆë‹¤.
</p>
<p class="ni" style="margin-top:10px">
ë”¥ëŸ¬ë‹ì€ ì´ íŒ¨ëŸ¬ë‹¤ì„ì„ ë’¤ì§‘ëŠ”ë‹¤. ì‹ ê²½ë§ì€ ì›ì‹œ ë°ì´í„°(raw data)ì—ì„œ ìŠ¤ìŠ¤ë¡œ ìœ ì˜ë¯¸í•œ í”¼ì²˜ë¥¼ ì¶”ì¶œí•œë‹¤. CNNì€ ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ì—ì„œ íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , LSTMì€ ì‹œê³„ì—´ì˜ ì¥ê¸° ì˜ì¡´ì„±ì„ ìë™ìœ¼ë¡œ í¬ì°©í•œë‹¤. R2ì—ì„œ ë°°ìš´ í¸ë¯¸ë¶„ê³¼ ì²´ì¸ë£°ì´ ì—­ì „íŒŒ(backpropagation)ì˜ ìˆ˜í•™ì  ê¸°ë°˜ì´ ë˜ê³ , R4ì˜ ë¶„ë¥˜/íšŒê·€ ê°œë…ì´ ì‹ ê²½ë§ì˜ ì¶œë ¥ì¸µìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ëœë‹¤. ì´ë²ˆ ë¼ìš´ë“œì—ì„œëŠ” í¼ì…‰íŠ¸ë¡ ì´ë¼ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë‰´ëŸ°ì—ì„œ ì¶œë°œí•˜ì—¬, ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP), í•©ì„±ê³± ì‹ ê²½ë§(CNN), ìˆœí™˜ ì‹ ê²½ë§(RNN), ê·¸ë¦¬ê³  LSTM/GRUê¹Œì§€ â€” ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì˜ ì§„í™”ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë”°ë¼ê°„ë‹¤.
</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 1: ì™œ ë”¥ëŸ¬ë‹ì¸ê°€
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch1">Chapter 1. ì™œ ë”¥ëŸ¬ë‹ì¸ê°€ â€” From ML to DL</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.17 "Deep Learning for Trading" ë„ì…ë¶€ / MLDSF Ch.3 "Deep Learning Fundamentals"</p>
</div>

<h3>1.1 ì „í†µ MLì˜ í•œê³„</h3>

<p>
R4ì—ì„œ ìš°ë¦¬ëŠ” XGBoostë¡œ ì£¼ê°€ ë°©í–¥ì„ ì˜ˆì¸¡í–ˆë‹¤. ê·¸ë•Œ ìš°ë¦¬ê°€ í•œ ì¼ì„ ëŒì•„ë³´ì: (1) RSI, MACD, ë³¼ë¦°ì €ë°´ë“œ ë“± ê¸°ìˆ ì  ì§€í‘œë¥¼ ì§ì ‘ ê³„ì‚°í•˜ê³ , (2) ì´ ì§€í‘œë“¤ì„ í”¼ì²˜ë¡œ ë§Œë“¤ì–´ ëª¨ë¸ì— ì…ë ¥í–ˆë‹¤. ëª¨ë¸ì€ ìš°ë¦¬ê°€ ë§Œë“¤ì–´ì¤€ í”¼ì²˜ì˜ ì¡°í•©ë§Œ í•™ìŠµí•  ìˆ˜ ìˆì—ˆë‹¤. ë§Œì•½ ìš°ë¦¬ê°€ ì¤‘ìš”í•œ í”¼ì²˜ë¥¼ ë¹ ëœ¨ë ¸ë‹¤ë©´? ëª¨ë¸ì€ ê·¸ ì •ë³´ë¥¼ ì˜ì›íˆ ì•Œ ìˆ˜ ì—†ë‹¤.
</p>

<p>
ì´ê²ƒì´ ì „í†µ MLì˜ ê·¼ë³¸ì  í•œê³„ë‹¤. ëª¨ë¸ì˜ ì„±ëŠ¥ì´ <strong>í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì˜ í’ˆì§ˆ</strong>ì— ì˜í•´ ê²°ì •ëœë‹¤. ì•„ë¬´ë¦¬ ì¢‹ì€ ì•Œê³ ë¦¬ì¦˜ì´ë¼ë„ ì“°ë ˆê¸° í”¼ì²˜ë¥¼ ë„£ìœ¼ë©´ ì“°ë ˆê¸° ì˜ˆì¸¡ì´ ë‚˜ì˜¨ë‹¤ (Garbage In, Garbage Out). ê¸ˆìœµ ì‹œì¥ì²˜ëŸ¼ ë³µì¡í•œ ë¹„ì„ í˜• ì‹œìŠ¤í…œì—ì„œëŠ” ì–´ë–¤ í”¼ì²˜ê°€ ì¤‘ìš”í•œì§€ ì‚¬ì „ì— ì•Œê¸° ì–´ë µë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– ë”¥ëŸ¬ë‹ (Deep Learning) ì •ì˜</strong></p>
<p class="ni" style="margin-top:8px">
ë”¥ëŸ¬ë‹ì€ ì—¬ëŸ¬ ì¸µ(layer)ì˜ ë¹„ì„ í˜• ë³€í™˜ì„ ìŒ“ì•„ ì˜¬ë ¤, ì›ì‹œ ë°ì´í„°ì—ì„œ ì ì  ë” ì¶”ìƒì ì¸ í‘œí˜„(representation)ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ë‹¤. "Deep"ì€ ì‹ ê²½ë§ì˜ ì¸µì´ ê¹Šë‹¤(ë§ë‹¤)ëŠ” ì˜ë¯¸ì´ë©°, ì¼ë°˜ì ìœ¼ë¡œ 2ê°œ ì´ìƒì˜ ì€ë‹‰ì¸µ(hidden layer)ì„ ê°€ì§„ ì‹ ê²½ë§ì„ ë”¥ëŸ¬ë‹ì´ë¼ ë¶€ë¥¸ë‹¤.
</p>
</div>

<h3>1.2 í‘œí˜„ í•™ìŠµ (Representation Learning)</h3>

<p>
ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” <strong>í‘œí˜„ í•™ìŠµ</strong>(representation learning)ì´ë‹¤. ì „í†µ MLì—ì„œëŠ” ì‚¬ëŒì´ í”¼ì²˜ë¥¼ ì„¤ê³„í–ˆì§€ë§Œ, ë”¥ëŸ¬ë‹ì—ì„œëŠ” ëª¨ë¸ì´ ë°ì´í„°ì˜ ì¢‹ì€ í‘œí˜„ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•œë‹¤. ê° ì¸µì€ ì´ì „ ì¸µì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë” ë†’ì€ ìˆ˜ì¤€ì˜ ì¶”ìƒí™”ë¥¼ ë§Œë“ ë‹¤.
</p>

<!-- í‘œí˜„ í•™ìŠµ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e3f2fd,#f3e5f5);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#1565c0">ğŸ”„ ì „í†µ ML vs ë”¥ëŸ¬ë‹: í”¼ì²˜ ì¶”ì¶œ íŒ¨ëŸ¬ë‹¤ì„</p>
<div style="display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:16px;border-radius:8px;border:2px solid #e57373">
<p class="ni" style="font-weight:bold;color:#c62828;text-align:center;margin-bottom:10px">ì „í†µ ML</p>
<div style="display:flex;align-items:center;justify-content:center;gap:6px;font-size:12px;flex-wrap:wrap">
<div style="background:#ffcdd2;padding:6px 10px;border-radius:6px">Raw Data</div>
<span>â†’</span>
<div style="background:#ef9a9a;padding:6px 10px;border-radius:6px;border:2px dashed #c62828"><strong>ì‚¬ëŒì´ ì„¤ê³„</strong><br>í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§</div>
<span>â†’</span>
<div style="background:#ffcdd2;padding:6px 10px;border-radius:6px">Features</div>
<span>â†’</span>
<div style="background:#ef9a9a;padding:6px 10px;border-radius:6px">ML Model</div>
<span>â†’</span>
<div style="background:#ffcdd2;padding:6px 10px;border-radius:6px">Output</div>
</div>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:16px;border-radius:8px;border:2px solid #66bb6a">
<p class="ni" style="font-weight:bold;color:#2e7d32;text-align:center;margin-bottom:10px">ë”¥ëŸ¬ë‹</p>
<div style="display:flex;align-items:center;justify-content:center;gap:6px;font-size:12px;flex-wrap:wrap">
<div style="background:#c8e6c9;padding:6px 10px;border-radius:6px">Raw Data</div>
<span>â†’</span>
<div style="background:#a5d6a7;padding:6px 10px;border-radius:6px;border:2px solid #2e7d32"><strong>ìë™ í•™ìŠµ</strong><br>Layer 1â†’2â†’...â†’N</div>
<span>â†’</span>
<div style="background:#c8e6c9;padding:6px 10px;border-radius:6px">Output</div>
</div>
</div>
</div>
</div>

<h3>1.3 ë”¥ëŸ¬ë‹ì´ ê¸ˆìœµì—ì„œ ì¤‘ìš”í•œ ì´ìœ </h3>

<p>
ê¸ˆìœµ ì‹œì¥ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë¹„ì„ í˜•(nonlinear) ì‹œìŠ¤í…œì´ë‹¤. ì£¼ê°€ëŠ” ìˆ˜ì²œ ê°œì˜ ë³€ìˆ˜ê°€ ë³µì¡í•˜ê²Œ ìƒí˜¸ì‘ìš©í•œ ê²°ê³¼ì´ë©°, ì´ ê´€ê³„ëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í•œë‹¤(non-stationary). ì „í†µ ML ëª¨ë¸ì€ ì´ëŸ° ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ë¥¼ í¬ì°©í•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤. ì„ í˜•íšŒê·€ëŠ” ë§ ê·¸ëŒ€ë¡œ ì„ í˜• ê´€ê³„ë§Œ ëª¨ë¸ë§í•˜ê³ , Decision TreeëŠ” ì¶•ì— í‰í–‰í•œ ë¶„í• ë§Œ ê°€ëŠ¥í•˜ë‹¤.
</p>

<p>
ë”¥ëŸ¬ë‹ì€ <strong>ë²”ìš© ê·¼ì‚¬ ì •ë¦¬</strong>(Universal Approximation Theorem)ì— ì˜í•´, ì¶©ë¶„í•œ ë‰´ëŸ°ì„ ê°€ì§„ ë‹¨ì¼ ì€ë‹‰ì¸µ ì‹ ê²½ë§ì´ ì„ì˜ì˜ ì—°ì† í•¨ìˆ˜ë¥¼ ì›í•˜ëŠ” ì •ë°€ë„ë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆìŒì´ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ë˜ì–´ ìˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ ì–´ë–¤ ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì´ë‹¤.
</p>

<div class="eq">
\[ \text{Universal Approximation Theorem: } \forall \epsilon > 0, \exists N \in \mathbb{N}, \exists W, b \text{ s.t. } \left| f(x) - \sum_{i=1}^{N} v_i \sigma(w_i^T x + b_i) \right| < \epsilon \]
</div>

<p>
ì—¬ê¸°ì„œ \(\sigma\)ëŠ” ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜, \(w_i, b_i\)ëŠ” ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥, \(v_i\)ëŠ” ì¶œë ¥ì¸µì˜ ê°€ì¤‘ì¹˜ë‹¤. ì´ ì •ë¦¬ëŠ” "ì¡´ì¬í•œë‹¤"ëŠ” ê²ƒë§Œ ë³´ì¥í•˜ì§€, ê·¸ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ë³´ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ ì‹¤ì „ì—ì„œëŠ” ê¹Šì€(deep) ë„¤íŠ¸ì›Œí¬ê°€ ì–•ì€ ë„¤íŠ¸ì›Œí¬ë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ìœ¼ë¡œ ë³µì¡í•œ í•¨ìˆ˜ë¥¼ í•™ìŠµí•œë‹¤.
</p>

<div class="info">
<p class="ni"><strong>ğŸ’¡ ê¸ˆìœµì—ì„œ ë”¥ëŸ¬ë‹ì˜ ì£¼ìš” ì ìš© ë¶„ì•¼</strong></p>
<ul>
<li><strong>ì‹œê³„ì—´ ì˜ˆì¸¡:</strong> LSTM/GRUë¡œ ì£¼ê°€, ë³€ë™ì„±, ìˆ˜ìµë¥  ì˜ˆì¸¡</li>
<li><strong>íŒ¨í„´ ì¸ì‹:</strong> CNNìœ¼ë¡œ ìº”ë“¤ì°¨íŠ¸, ê¸°ìˆ ì  íŒ¨í„´ ìë™ ì¸ì‹</li>
<li><strong>NLP:</strong> Transformer/BERTë¡œ ë‰´ìŠ¤ ê°ì„±ë¶„ì„ (R6ì—ì„œ ì´ë¯¸ ë§›ë´„)</li>
<li><strong>í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”:</strong> ì˜¤í† ì¸ì½”ë”ë¡œ ë¦¬ìŠ¤í¬ íŒ©í„° ì¶”ì¶œ</li>
<li><strong>ê³ ë¹ˆë„ íŠ¸ë ˆì´ë”©:</strong> ì‹¤ì‹œê°„ ì˜¤ë”ë¶ ë°ì´í„°ì—ì„œ ì´ˆë‹¨ê¸° ì˜ˆì¸¡</li>
<li><strong>ëŒ€ì•ˆ ë°ì´í„°:</strong> ìœ„ì„± ì´ë¯¸ì§€, ì†Œì…œë¯¸ë””ì–´ ë“± ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬</li>
</ul>
</div>

<h3>1.4 ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ â€” ì„¸ ë²ˆì˜ íŒŒë„</h3>

<table>
<tr><th>ì‹œê¸°</th><th>íŒŒë„</th><th>í•µì‹¬ ì‚¬ê±´</th><th>í•œê³„/ëŒíŒŒêµ¬</th></tr>
<tr><td>1943~1969</td><td>1ì°¨ íŒŒë„</td><td>McCulloch-Pitts ë‰´ëŸ° (1943), Rosenblatt í¼ì…‰íŠ¸ë¡  (1958)</td><td>Minsky & Papert: XOR ë¬¸ì œ ì¦ëª… â†’ AI ê²¨ìš¸</td></tr>
<tr><td>1986~1995</td><td>2ì°¨ íŒŒë„</td><td>ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ (Rumelhart, 1986), CNN (LeCun, 1989)</td><td>Vanishing gradient, ì»´í“¨íŒ… í•œê³„ â†’ 2ì°¨ AI ê²¨ìš¸</td></tr>
<tr><td>2006~í˜„ì¬</td><td>3ì°¨ íŒŒë„</td><td>Deep Belief Networks (Hinton, 2006), AlexNet (2012), Transformer (2017)</td><td>GPU ì»´í“¨íŒ…, ë¹…ë°ì´í„°, ìƒˆë¡œìš´ í™œì„±í™” í•¨ìˆ˜(ReLU)</td></tr>
</table>

<p>
í˜„ì¬ì˜ ë”¥ëŸ¬ë‹ ë¶ì€ ì„¸ ê°€ì§€ ìš”ì†Œê°€ ë™ì‹œì— ê°–ì¶°ì¡Œê¸° ë•Œë¬¸ì— ê°€ëŠ¥í–ˆë‹¤: (1) ëŒ€ê·œëª¨ ë°ì´í„°, (2) GPU ë³‘ë ¬ ì»´í“¨íŒ…, (3) ì•Œê³ ë¦¬ì¦˜ í˜ì‹ (ReLU, Batch Normalization, Dropout ë“±). ê¸ˆìœµ ë¶„ì•¼ì—ì„œëŠ” 2015ë…„ê²½ë¶€í„° ë³¸ê²©ì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ì´ ë„ì…ë˜ê¸° ì‹œì‘í–ˆìœ¼ë©°, íŠ¹íˆ ê³ ë¹ˆë„ íŠ¸ë ˆì´ë”©ê³¼ ëŒ€ì•ˆ ë°ì´í„° ë¶„ì„ì—ì„œ ë¹ ë¥´ê²Œ í™•ì‚°ë˜ê³  ìˆë‹¤.
</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 2: í¼ì…‰íŠ¸ë¡ 
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch2">Chapter 2. í¼ì…‰íŠ¸ë¡  â€” ì‹ ê²½ë§ì˜ ì›ì</h2>

<h3>2.1 ìƒë¬¼í•™ì  ë‰´ëŸ°ì—ì„œ ì¸ê³µ ë‰´ëŸ°ìœ¼ë¡œ</h3>

<p>
ì¸ê³µ ì‹ ê²½ë§ì˜ ì˜ê°ì€ ìƒë¬¼í•™ì  ë‰´ëŸ°ì—ì„œ ì™”ë‹¤. ì¸ê°„ì˜ ë‡Œì—ëŠ” ì•½ 860ì–µ ê°œì˜ ë‰´ëŸ°ì´ ìˆê³ , ê° ë‰´ëŸ°ì€ ìˆ˜ì²œ ê°œì˜ ì‹œëƒ…ìŠ¤ë¥¼ í†µí•´ ë‹¤ë¥¸ ë‰´ëŸ°ê³¼ ì—°ê²°ë˜ì–´ ìˆë‹¤. ë‰´ëŸ°ì€ ë‹¤ë¥¸ ë‰´ëŸ°ë“¤ë¡œë¶€í„° ì „ê¸° ì‹ í˜¸(ì…ë ¥)ë¥¼ ë°›ì•„, ê·¸ í•©ì´ íŠ¹ì • ì„ê³„ê°’(threshold)ì„ ë„˜ìœ¼ë©´ í™œì„±í™”ë˜ì–´ ë‹¤ìŒ ë‰´ëŸ°ì— ì‹ í˜¸ë¥¼ ì „ë‹¬í•œë‹¤.
</p>

<!-- ë‰´ëŸ° ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:#fff;border:2px solid #e0e0e0;border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#2c3e50">ğŸ§  ìƒë¬¼í•™ì  ë‰´ëŸ° â†’ ì¸ê³µ ë‰´ëŸ° ë§¤í•‘</p>
<div style="display:flex;gap:30px;flex-wrap:wrap;justify-content:center;font-size:12px">
<div style="flex:1;min-width:200px;text-align:center">
<div style="font-weight:bold;margin-bottom:8px;color:#8e24aa">ìƒë¬¼í•™ì  ë‰´ëŸ°</div>
<div style="background:#f3e5f5;padding:12px;border-radius:8px;line-height:1.8">
ìˆ˜ìƒëŒê¸° (Dendrite) â†’ ì…ë ¥ ìˆ˜ì‹ <br>
ì„¸í¬ì²´ (Soma) â†’ ì‹ í˜¸ í•©ì‚°<br>
ì¶•ì‚­ëŒê¸° (Axon) â†’ ì¶œë ¥ ì „ë‹¬<br>
ì‹œëƒ…ìŠ¤ (Synapse) â†’ ì—°ê²° ê°•ë„
</div>
</div>
<div style="display:flex;align-items:center;font-size:24px;color:#888">âŸ¹</div>
<div style="flex:1;min-width:200px;text-align:center">
<div style="font-weight:bold;margin-bottom:8px;color:#1565c0">ì¸ê³µ ë‰´ëŸ°</div>
<div style="background:#e3f2fd;padding:12px;border-radius:8px;line-height:1.8">
ì…ë ¥ (xâ‚, xâ‚‚, ..., xâ‚™) â†’ í”¼ì²˜<br>
ê°€ì¤‘í•© (Î£wáµ¢xáµ¢ + b) â†’ ì„ í˜• ë³€í™˜<br>
í™œì„±í™” í•¨ìˆ˜ Ïƒ(z) â†’ ë¹„ì„ í˜• ë³€í™˜<br>
ê°€ì¤‘ì¹˜ (wâ‚, wâ‚‚, ..., wâ‚™) â†’ í•™ìŠµ íŒŒë¼ë¯¸í„°
</div>
</div>
</div>
</div>

<h3>2.2 í¼ì…‰íŠ¸ë¡ ì˜ ìˆ˜í•™ì  ì •ì˜</h3>

<p>
1958ë…„ Frank Rosenblattì´ ì œì•ˆí•œ í¼ì…‰íŠ¸ë¡ (Perceptron)ì€ ê°€ì¥ ë‹¨ìˆœí•œ ì¸ê³µ ë‰´ëŸ°ì´ë‹¤. ì…ë ¥ ë²¡í„° \(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)ì— ê°€ì¤‘ì¹˜ ë²¡í„° \(\mathbf{w} = (w_1, w_2, \ldots, w_n)\)ë¥¼ ê³±í•˜ê³  í¸í–¥(bias) \(b\)ë¥¼ ë”í•œ ë’¤, í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œì¼œ ì¶œë ¥ì„ ë§Œë“ ë‹¤.
</p>

<div class="eq">
\[ z = \mathbf{w}^T \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b \]
\[ \hat{y} = \sigma(z) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases} \]
</div>

<p>
ì—¬ê¸°ì„œ \(\sigma\)ëŠ” ê³„ë‹¨ í•¨ìˆ˜(step function)ë‹¤. í¼ì…‰íŠ¸ë¡ ì€ ë³¸ì§ˆì ìœ¼ë¡œ <strong>ì„ í˜• ì´ì§„ ë¶„ë¥˜ê¸°</strong>ë‹¤. ì…ë ¥ ê³µê°„ì„ í•˜ë‚˜ì˜ ì´ˆí‰ë©´(hyperplane)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•œë‹¤. R4ì—ì„œ ë°°ìš´ ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, í¼ì…‰íŠ¸ë¡ ì€ í™•ë¥ ì´ ì•„ë‹Œ ì´ì§„ ì¶œë ¥ì„ ë‚´ë†“ëŠ”ë‹¤.
</p>

<h3>2.3 í¼ì…‰íŠ¸ë¡  í•™ìŠµ ê·œì¹™</h3>

<p>
í¼ì…‰íŠ¸ë¡ ì˜ í•™ìŠµì€ ë†€ë¼ìš¸ ì •ë„ë¡œ ë‹¨ìˆœí•˜ë‹¤. ì˜ˆì¸¡ì´ í‹€ë¦´ ë•Œë§Œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤:
</p>

<div class="eq">
\[ w_i \leftarrow w_i + \eta \cdot (y - \hat{y}) \cdot x_i \]
\[ b \leftarrow b + \eta \cdot (y - \hat{y}) \]
</div>

<p>
ì—¬ê¸°ì„œ \(\eta\)ëŠ” í•™ìŠµë¥ (learning rate), \(y\)ëŠ” ì‹¤ì œ ë¼ë²¨, \(\hat{y}\)ëŠ” ì˜ˆì¸¡ê°’ì´ë‹¤. ì˜ˆì¸¡ì´ ë§ìœ¼ë©´ \(y - \hat{y} = 0\)ì´ë¯€ë¡œ ì—…ë°ì´íŠ¸ê°€ ì—†ê³ , í‹€ë¦¬ë©´ ì˜¤ì°¨ ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•œë‹¤. ì´ ê·œì¹™ì€ ë°ì´í„°ê°€ ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥(linearly separable)í•˜ë©´ ìœ í•œ ë²ˆì˜ ë°˜ë³µ í›„ ë°˜ë“œì‹œ ìˆ˜ë ´í•œë‹¤ëŠ” ê²ƒì´ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ë˜ì–´ ìˆë‹¤ (Perceptron Convergence Theorem).
</p>

<h3>2.4 íŒŒì´ì¬ìœ¼ë¡œ í¼ì…‰íŠ¸ë¡  êµ¬í˜„</h3>

<p class="cc">Python â€” í¼ì…‰íŠ¸ë¡  from scratch</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="kw">class</span> <span class="nb">Perceptron</span>:
    <span class="st">"""ë‹¨ì¼ í¼ì…‰íŠ¸ë¡  êµ¬í˜„"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>, n_features, lr=<span class="nu">0.01</span>):
        <span class="nb">self</span>.weights = np.<span class="fn">zeros</span>(n_features)
        <span class="nb">self</span>.bias = <span class="nu">0.0</span>
        <span class="nb">self</span>.lr = lr
    
    <span class="kw">def</span> <span class="fn">predict</span>(<span class="nb">self</span>, x):
        z = np.<span class="fn">dot</span>(<span class="nb">self</span>.weights, x) + <span class="nb">self</span>.bias
        <span class="kw">return</span> <span class="nu">1</span> <span class="kw">if</span> z >= <span class="nu">0</span> <span class="kw">else</span> <span class="nu">0</span>
    
    <span class="kw">def</span> <span class="fn">train</span>(<span class="nb">self</span>, X, y, epochs=<span class="nu">100</span>):
        errors_per_epoch = []
        <span class="kw">for</span> epoch <span class="kw">in</span> <span class="nb">range</span>(epochs):
            errors = <span class="nu">0</span>
            <span class="kw">for</span> xi, yi <span class="kw">in</span> <span class="nb">zip</span>(X, y):
                pred = <span class="nb">self</span>.<span class="fn">predict</span>(xi)
                error = yi - pred
                <span class="kw">if</span> error != <span class="nu">0</span>:
                    <span class="nb">self</span>.weights += <span class="nb">self</span>.lr * error * xi
                    <span class="nb">self</span>.bias += <span class="nb">self</span>.lr * error
                    errors += <span class="nu">1</span>
            errors_per_epoch.<span class="fn">append</span>(errors)
            <span class="kw">if</span> errors == <span class="nu">0</span>:
                <span class="fn">print</span>(<span class="st">f"ìˆ˜ë ´ ì™„ë£Œ! Epoch {epoch+1}"</span>)
                <span class="kw">break</span>
        <span class="kw">return</span> errors_per_epoch

<span class="cm"># AND ê²Œì´íŠ¸ í•™ìŠµ</span>
X = np.<span class="fn">array</span>([[<span class="nu">0</span>,<span class="nu">0</span>], [<span class="nu">0</span>,<span class="nu">1</span>], [<span class="nu">1</span>,<span class="nu">0</span>], [<span class="nu">1</span>,<span class="nu">1</span>]])
y_and = np.<span class="fn">array</span>([<span class="nu">0</span>, <span class="nu">0</span>, <span class="nu">0</span>, <span class="nu">1</span>])

p = <span class="nb">Perceptron</span>(n_features=<span class="nu">2</span>, lr=<span class="nu">0.1</span>)
history = p.<span class="fn">train</span>(X, y_and, epochs=<span class="nu">20</span>)

<span class="fn">print</span>(<span class="st">f"í•™ìŠµëœ ê°€ì¤‘ì¹˜: w={p.weights}, b={p.bias:.2f}"</span>)
<span class="fn">print</span>(<span class="st">"AND ê²Œì´íŠ¸ í…ŒìŠ¤íŠ¸:"</span>)
<span class="kw">for</span> xi <span class="kw">in</span> X:
    <span class="fn">print</span>(<span class="st">f"  {xi} â†’ {p.predict(xi)}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
ìˆ˜ë ´ ì™„ë£Œ! Epoch 4
í•™ìŠµëœ ê°€ì¤‘ì¹˜: w=[0.1 0.1], b=-0.10
AND ê²Œì´íŠ¸ í…ŒìŠ¤íŠ¸:
  [0 0] â†’ 0
  [0 1] â†’ 0
  [1 0] â†’ 0
  [1 1] â†’ 1</div>

<h3>2.5 XOR ë¬¸ì œ â€” í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„</h3>

<p>
1969ë…„ Minskyì™€ PapertëŠ” ì €ì„œ "Perceptrons"ì—ì„œ ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì´ XOR ë¬¸ì œë¥¼ í’€ ìˆ˜ ì—†ìŒì„ ì¦ëª…í–ˆë‹¤. XORì€ ë‘ ì…ë ¥ì´ ë‹¤ë¥¼ ë•Œ 1, ê°™ì„ ë•Œ 0ì„ ì¶œë ¥í•˜ëŠ” ë…¼ë¦¬ ê²Œì´íŠ¸ë‹¤. ì´ ë¬¸ì œëŠ” ì„ í˜• ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ â€” ì–´ë–¤ ì§ì„ ìœ¼ë¡œë„ (0,0)/(1,1)ê³¼ (0,1)/(1,0)ì„ ë¶„ë¦¬í•  ìˆ˜ ì—†ë‹¤.
</p>

<p class="cc">Python â€” XOR ë¬¸ì œ ì‹œê°í™”</p>
<pre><code><span class="cm"># XOR ë°ì´í„°</span>
y_xor = np.<span class="fn">array</span>([<span class="nu">0</span>, <span class="nu">1</span>, <span class="nu">1</span>, <span class="nu">0</span>])

p_xor = <span class="nb">Perceptron</span>(n_features=<span class="nu">2</span>, lr=<span class="nu">0.1</span>)
history_xor = p_xor.<span class="fn">train</span>(X, y_xor, epochs=<span class="nu">100</span>)

<span class="fn">print</span>(<span class="st">"XOR í¼ì…‰íŠ¸ë¡  í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨ ì˜ˆìƒ):"</span>)
<span class="kw">for</span> xi, yi <span class="kw">in</span> <span class="nb">zip</span>(X, y_xor):
    pred = p_xor.<span class="fn">predict</span>(xi)
    status = <span class="st">"âœ“"</span> <span class="kw">if</span> pred == yi <span class="kw">else</span> <span class="st">"âœ—"</span>
    <span class="fn">print</span>(<span class="st">f"  {xi} â†’ ì˜ˆì¸¡:{pred}, ì •ë‹µ:{yi} {status}"</span>)

<span class="fn">print</span>(<span class="st">f"\n100 ì—í­ í›„ì—ë„ ì—ëŸ¬ ìˆ˜: {history_xor[-1]}"</span>)
<span class="fn">print</span>(<span class="st">"â†’ ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” XORì„ í’€ ìˆ˜ ì—†ë‹¤!"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
XOR í¼ì…‰íŠ¸ë¡  í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨ ì˜ˆìƒ):
  [0 0] â†’ ì˜ˆì¸¡:0, ì •ë‹µ:0 âœ“
  [0 1] â†’ ì˜ˆì¸¡:0, ì •ë‹µ:1 âœ—
  [1 0] â†’ ì˜ˆì¸¡:1, ì •ë‹µ:1 âœ“
  [1 1] â†’ ì˜ˆì¸¡:1, ì •ë‹µ:0 âœ—

100 ì—í­ í›„ì—ë„ ì—ëŸ¬ ìˆ˜: 2
â†’ ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” XORì„ í’€ ìˆ˜ ì—†ë‹¤!</div>

<div class="warn">
<p class="ni"><strong>âš ï¸ XOR ë¬¸ì œê°€ ì¤‘ìš”í•œ ì´ìœ </strong></p>
<p class="ni" style="margin-top:8px">
XOR ë¬¸ì œëŠ” ë‹¨ìˆœí•œ ë…¼ë¦¬ ê²Œì´íŠ¸ ë¬¸ì œê°€ ì•„ë‹ˆë‹¤. ê¸ˆìœµì—ì„œë„ ë¹„ì„ í˜• ê´€ê³„ëŠ” í”í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ì„œ ë™ì‹œì— ì¸í”Œë ˆì´ì…˜ì´ ë†’ì„ ë•Œë§Œ ì£¼ê°€ê°€ í•˜ë½í•œë‹¤"ëŠ” ê´€ê³„ëŠ” XORê³¼ ìœ ì‚¬í•œ ë¹„ì„ í˜• íŒ¨í„´ì´ë‹¤. ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ (= ì„ í˜• ëª¨ë¸)ìœ¼ë¡œëŠ” ì´ëŸ° ê´€ê³„ë¥¼ í¬ì°©í•  ìˆ˜ ì—†ë‹¤. í•´ê²°ì±…ì€? <strong>ì—¬ëŸ¬ ì¸µì„ ìŒ“ëŠ” ê²ƒ</strong> â€” ì´ê²ƒì´ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ì´ë‹¤.
</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 3: í™œì„±í™” í•¨ìˆ˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch3">Chapter 3. í™œì„±í™” í•¨ìˆ˜ â€” ë¹„ì„ í˜•ì„±ì˜ ì—´ì‡ </h2>

<h3>3.1 ì™œ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í•œê°€</h3>

<p>
í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ë‹¤ë©´ ì‹ ê²½ë§ì€ ì•„ë¬´ë¦¬ ì¸µì„ ê¹Šê²Œ ìŒ“ì•„ë„ ê²°êµ­ í•˜ë‚˜ì˜ ì„ í˜• ë³€í™˜ì— ë¶ˆê³¼í•˜ë‹¤. ë‘ ê°œì˜ ì„ í˜• ë³€í™˜ì„ í•©ì„±í•˜ë©´ \(W_2(W_1 x + b_1) + b_2 = W_2 W_1 x + W_2 b_1 + b_2 = W' x + b'\)ë¡œ, ì—¬ì „íˆ ì„ í˜•ì´ë‹¤. ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ì´ì— ë¼ì›Œì•¼ ë¹„ë¡œì†Œ ì‹ ê²½ë§ì´ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
</p>

<h3>3.2 ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ ì´ì •ë¦¬</h3>

<div class="def">
<p class="ni"><strong>ğŸ“– í•µì‹¬ í™œì„±í™” í•¨ìˆ˜ 6ì¢…</strong></p>
</div>

<div class="eq">
\[
\text{Sigmoid: } \sigma(z) = \frac{1}{1 + e^{-z}}, \quad \sigma'(z) = \sigma(z)(1 - \sigma(z))
\]
\[
\text{Tanh: } \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}, \quad \tanh'(z) = 1 - \tanh^2(z)
\]
\[
\text{ReLU: } f(z) = \max(0, z), \quad f'(z) = \begin{cases} 1 & z > 0 \\ 0 & z \leq 0 \end{cases}
\]
\[
\text{Leaky ReLU: } f(z) = \begin{cases} z & z > 0 \\ \alpha z & z \leq 0 \end{cases}, \quad \alpha = 0.01
\]
\[
\text{ELU: } f(z) = \begin{cases} z & z > 0 \\ \alpha(e^z - 1) & z \leq 0 \end{cases}
\]
\[
\text{Softmax: } \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{(ë‹¤ì¤‘ í´ë˜ìŠ¤ ì¶œë ¥ì¸µ)}
\]
</div>

<table>
<tr><th>í•¨ìˆ˜</th><th>ì¶œë ¥ ë²”ìœ„</th><th>ì¥ì </th><th>ë‹¨ì </th><th>ì£¼ ì‚¬ìš©ì²˜</th></tr>
<tr><td>Sigmoid</td><td>(0, 1)</td><td>í™•ë¥  í•´ì„ ê°€ëŠ¥</td><td>Vanishing gradient, ë¹„ëŒ€ì¹­</td><td>ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ</td></tr>
<tr><td>Tanh</td><td>(-1, 1)</td><td>Zero-centered</td><td>Vanishing gradient</td><td>RNN ì€ë‹‰ì¸µ</td></tr>
<tr><td>ReLU</td><td>[0, âˆ)</td><td>ê³„ì‚° ë¹ ë¦„, gradient ì†Œì‹¤ ì™„í™”</td><td>Dead neuron (ìŒìˆ˜ ì˜ì—­)</td><td>CNN/MLP ì€ë‹‰ì¸µ (ê¸°ë³¸)</td></tr>
<tr><td>Leaky ReLU</td><td>(-âˆ, âˆ)</td><td>Dead neuron í•´ê²°</td><td>Î± í•˜ì´í¼íŒŒë¼ë¯¸í„°</td><td>ê¹Šì€ ë„¤íŠ¸ì›Œí¬</td></tr>
<tr><td>ELU</td><td>(-Î±, âˆ)</td><td>ë¶€ë“œëŸ¬ìš´ ìŒìˆ˜ ì˜ì—­</td><td>exp ì—°ì‚° ë¹„ìš©</td><td>ê¹Šì€ ë„¤íŠ¸ì›Œí¬</td></tr>
<tr><td>Softmax</td><td>(0, 1), í•©=1</td><td>í™•ë¥  ë¶„í¬ ì¶œë ¥</td><td>ì€ë‹‰ì¸µì— ë¶€ì í•©</td><td>ë‹¤ì¤‘ ë¶„ë¥˜ ì¶œë ¥ì¸µ</td></tr>
</table>

<h3>3.3 í™œì„±í™” í•¨ìˆ˜ ì‹œê°í™”</h3>

<p class="cc">Python â€” í™œì„±í™” í•¨ìˆ˜ ë¹„êµ ì‹œê°í™”</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

z = np.<span class="fn">linspace</span>(-<span class="nu">5</span>, <span class="nu">5</span>, <span class="nu">200</span>)

<span class="cm"># í™œì„±í™” í•¨ìˆ˜ ì •ì˜</span>
sigmoid = <span class="nu">1</span> / (<span class="nu">1</span> + np.<span class="fn">exp</span>(-z))
tanh = np.<span class="fn">tanh</span>(z)
relu = np.<span class="fn">maximum</span>(<span class="nu">0</span>, z)
leaky_relu = np.<span class="fn">where</span>(z > <span class="nu">0</span>, z, <span class="nu">0.01</span> * z)
elu_alpha = <span class="nu">1.0</span>
elu = np.<span class="fn">where</span>(z > <span class="nu">0</span>, z, elu_alpha * (np.<span class="fn">exp</span>(z) - <span class="nu">1</span>))

fig, axes = plt.<span class="fn">subplots</span>(<span class="nu">2</span>, <span class="nu">3</span>, figsize=(<span class="nu">14</span>, <span class="nu">8</span>))
funcs = [(sigmoid, <span class="st">'Sigmoid'</span>, <span class="st">'#e74c3c'</span>),
         (tanh, <span class="st">'Tanh'</span>, <span class="st">'#3498db'</span>),
         (relu, <span class="st">'ReLU'</span>, <span class="st">'#2ecc71'</span>),
         (leaky_relu, <span class="st">'Leaky ReLU (Î±=0.01)'</span>, <span class="st">'#9b59b6'</span>),
         (elu, <span class="st">'ELU (Î±=1.0)'</span>, <span class="st">'#e67e22'</span>)]

<span class="kw">for</span> ax, (func, name, color) <span class="kw">in</span> <span class="nb">zip</span>(axes.<span class="fn">flat</span>, funcs):
    ax.<span class="fn">plot</span>(z, func, color=color, linewidth=<span class="nu">2</span>)
    ax.<span class="fn">axhline</span>(y=<span class="nu">0</span>, color=<span class="st">'gray'</span>, linewidth=<span class="nu">0.5</span>)
    ax.<span class="fn">axvline</span>(x=<span class="nu">0</span>, color=<span class="st">'gray'</span>, linewidth=<span class="nu">0.5</span>)
    ax.<span class="fn">set_title</span>(name, fontsize=<span class="nu">13</span>, fontweight=<span class="st">'bold'</span>)
    ax.<span class="fn">set_xlim</span>(-<span class="nu">5</span>, <span class="nu">5</span>)
    ax.<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

axes[<span class="nu">1</span>, <span class="nu">2</span>].<span class="fn">axis</span>(<span class="st">'off'</span>)
axes[<span class="nu">1</span>, <span class="nu">2</span>].<span class="fn">text</span>(<span class="nu">0.5</span>, <span class="nu">0.5</span>, <span class="st">'ğŸ’¡ ì‹¤ì „ íŒ:\nì€ë‹‰ì¸µ â†’ ReLU\nì¶œë ¥ì¸µ(ì´ì§„) â†’ Sigmoid\nì¶œë ¥ì¸µ(ë‹¤ì¤‘) â†’ Softmax\nRNN â†’ Tanh'</span>,
    transform=axes[<span class="nu">1</span>,<span class="nu">2</span>].transAxes, ha=<span class="st">'center'</span>, va=<span class="st">'center'</span>,
    fontsize=<span class="nu">12</span>, bbox=<span class="nb">dict</span>(boxstyle=<span class="st">'round'</span>, facecolor=<span class="st">'lightyellow'</span>))
plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">savefig</span>(<span class="st">'activation_functions.png'</span>, dpi=<span class="nu">150</span>)
plt.<span class="fn">show</span>()</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
[Figure saved: activation_functions.png â€” 2Ã—3 subplot grid showing Sigmoid, Tanh, ReLU, Leaky ReLU, ELU curves with zero-crossing reference lines]</div>

<h3>3.4 Vanishing Gradient ë¬¸ì œ</h3>

<p>
Sigmoidì™€ Tanhì˜ ì¹˜ëª…ì  ë¬¸ì œëŠ” <strong>ê¸°ìš¸ê¸° ì†Œì‹¤</strong>(vanishing gradient)ì´ë‹¤. Sigmoidì˜ ë„í•¨ìˆ˜ ìµœëŒ€ê°’ì€ 0.25 (z=0ì¼ ë•Œ)ì´ê³ , ì…ë ¥ì´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ë„í•¨ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤. ì—­ì „íŒŒ ì‹œ ê° ì¸µì˜ ê¸°ìš¸ê¸°ê°€ ê³±í•´ì§€ë¯€ë¡œ, ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì‘ì•„ì§„ë‹¤.
</p>

<div class="eq">
\[ \frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial a_n} \cdot \underbrace{\sigma'(z_n) \cdot \sigma'(z_{n-1}) \cdots \sigma'(z_1)}_{\text{ê°ê° ìµœëŒ€ 0.25}} \cdot x \]
</div>

<p>
5ê°œ ì¸µë§Œ ìŒ“ì•„ë„ ê¸°ìš¸ê¸°ê°€ \(0.25^5 \approx 0.001\)ë¡œ ì¤„ì–´ë“ ë‹¤. ì´ê²ƒì´ 2000ë…„ëŒ€ ì´ì „ì— ê¹Šì€ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¤ê¸° ì–´ë ¤ì› ë˜ í•µì‹¬ ì´ìœ ë‹¤. ReLUëŠ” ì–‘ìˆ˜ ì˜ì—­ì—ì„œ ë„í•¨ìˆ˜ê°€ í•­ìƒ 1ì´ë¯€ë¡œ ì´ ë¬¸ì œë¥¼ ê·¹ì ìœ¼ë¡œ ì™„í™”í•œë‹¤. 2012ë…„ AlexNetì´ ImageNet ëŒ€íšŒë¥¼ ì„ê¶Œí•œ ë¹„ê²° ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ ReLUì˜ ì‚¬ìš©ì´ì—ˆë‹¤.
</p>

<div class="ok">
<p class="ni"><strong>âœ… ì‹¤ì „ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ</strong></p>
<ul>
<li><strong>ì€ë‹‰ì¸µ ê¸°ë³¸:</strong> ReLU (ê°€ì¥ ë¹ ë¥´ê³  ì•ˆì •ì )</li>
<li><strong>Dead neuron ë¬¸ì œ ë°œìƒ ì‹œ:</strong> Leaky ReLU ë˜ëŠ” ELU</li>
<li><strong>ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ:</strong> Sigmoid (í™•ë¥  ì¶œë ¥)</li>
<li><strong>ë‹¤ì¤‘ ë¶„ë¥˜ ì¶œë ¥ì¸µ:</strong> Softmax (í™•ë¥  ë¶„í¬)</li>
<li><strong>íšŒê·€ ì¶œë ¥ì¸µ:</strong> í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ (Linear)</li>
<li><strong>RNN/LSTM ë‚´ë¶€:</strong> Tanh (ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì— ì í•©)</li>
</ul>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 4: ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch4">Chapter 4. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP) â€” XORì„ ë„˜ì–´ì„œ</h2>

<h3>4.1 MLP ì•„í‚¤í…ì²˜</h3>

<p>
ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptron, MLP)ì€ ì…ë ¥ì¸µ, í•˜ë‚˜ ì´ìƒì˜ ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ì™„ì „ ì—°ê²°(fully connected) ì‹ ê²½ë§ì´ë‹¤. ê° ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ì´ ë‹¤ìŒ ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ê³¼ ì—°ê²°ë˜ì–´ ìˆë‹¤. MLPëŠ” XOR ë¬¸ì œë¥¼ í¬í•¨í•œ ì„ì˜ì˜ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
</p>

<!-- MLP ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#e8eaf6,#e0f7fa);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:20px;color:#283593">ğŸ”— MLP ì•„í‚¤í…ì²˜ (2-3-2-1 ë„¤íŠ¸ì›Œí¬)</p>
<div style="display:flex;justify-content:center;align-items:center;gap:40px;flex-wrap:wrap">
<!-- Input Layer -->
<div style="text-align:center">
<div style="font-size:11px;font-weight:bold;color:#666;margin-bottom:8px">ì…ë ¥ì¸µ</div>
<div style="display:flex;flex-direction:column;gap:20px">
<div style="width:45px;height:45px;border-radius:50%;background:#e3f2fd;border:3px solid #1565c0;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">xâ‚</div>
<div style="width:45px;height:45px;border-radius:50%;background:#e3f2fd;border:3px solid #1565c0;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">xâ‚‚</div>
</div>
</div>
<div style="font-size:20px;color:#888">â†’</div>
<!-- Hidden Layer 1 -->
<div style="text-align:center">
<div style="font-size:11px;font-weight:bold;color:#666;margin-bottom:8px">ì€ë‹‰ì¸µ 1</div>
<div style="display:flex;flex-direction:column;gap:12px">
<div style="width:45px;height:45px;border-radius:50%;background:#fff3e0;border:3px solid #e65100;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">hâ‚</div>
<div style="width:45px;height:45px;border-radius:50%;background:#fff3e0;border:3px solid #e65100;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">hâ‚‚</div>
<div style="width:45px;height:45px;border-radius:50%;background:#fff3e0;border:3px solid #e65100;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">hâ‚ƒ</div>
</div>
</div>
<div style="font-size:20px;color:#888">â†’</div>
<!-- Hidden Layer 2 -->
<div style="text-align:center">
<div style="font-size:11px;font-weight:bold;color:#666;margin-bottom:8px">ì€ë‹‰ì¸µ 2</div>
<div style="display:flex;flex-direction:column;gap:16px">
<div style="width:45px;height:45px;border-radius:50%;background:#fce4ec;border:3px solid #c62828;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">hâ‚„</div>
<div style="width:45px;height:45px;border-radius:50%;background:#fce4ec;border:3px solid #c62828;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">hâ‚…</div>
</div>
</div>
<div style="font-size:20px;color:#888">â†’</div>
<!-- Output Layer -->
<div style="text-align:center">
<div style="font-size:11px;font-weight:bold;color:#666;margin-bottom:8px">ì¶œë ¥ì¸µ</div>
<div style="width:45px;height:45px;border-radius:50%;background:#e8f5e9;border:3px solid #2e7d32;display:flex;align-items:center;justify-content:center;font-size:12px;font-weight:bold">Å·</div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666;margin-top:12px">
íŒŒë¼ë¯¸í„° ìˆ˜: (2Ã—3+3) + (3Ã—2+2) + (2Ã—1+1) = 9 + 8 + 3 = <strong>20ê°œ</strong>
</p>
</div>

<p>
MLPì˜ ìˆœì „íŒŒ(forward propagation) ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:
</p>

<div class="eq">
\[
\mathbf{h}^{(1)} = \sigma(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
\]
\[
\mathbf{h}^{(2)} = \sigma(\mathbf{W}^{(2)} \mathbf{h}^{(1)} + \mathbf{b}^{(2)})
\]
\[
\hat{y} = \sigma_{\text{out}}(\mathbf{W}^{(3)} \mathbf{h}^{(2)} + \mathbf{b}^{(3)})
\]
</div>

<h3>4.2 MLPë¡œ XOR ë¬¸ì œ í•´ê²°</h3>

<p class="cc">Python â€” MLPë¡œ XOR í•™ìŠµ (NumPy from scratch)</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">def</span> <span class="fn">sigmoid</span>(z):
    <span class="kw">return</span> <span class="nu">1</span> / (<span class="nu">1</span> + np.<span class="fn">exp</span>(-z))

<span class="kw">def</span> <span class="fn">sigmoid_deriv</span>(a):
    <span class="kw">return</span> a * (<span class="nu">1</span> - a)

<span class="cm"># XOR ë°ì´í„°</span>
X = np.<span class="fn">array</span>([[<span class="nu">0</span>,<span class="nu">0</span>],[<span class="nu">0</span>,<span class="nu">1</span>],[<span class="nu">1</span>,<span class="nu">0</span>],[<span class="nu">1</span>,<span class="nu">1</span>]])
y = np.<span class="fn">array</span>([[<span class="nu">0</span>],[<span class="nu">1</span>],[<span class="nu">1</span>],[<span class="nu">0</span>]])

<span class="cm"># ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”: 2-4-1</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
W1 = np.random.<span class="fn">randn</span>(<span class="nu">2</span>, <span class="nu">4</span>) * <span class="nu">0.5</span>
b1 = np.<span class="fn">zeros</span>((<span class="nu">1</span>, <span class="nu">4</span>))
W2 = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, <span class="nu">1</span>) * <span class="nu">0.5</span>
b2 = np.<span class="fn">zeros</span>((<span class="nu">1</span>, <span class="nu">1</span>))

lr = <span class="nu">1.0</span>
losses = []

<span class="kw">for</span> epoch <span class="kw">in</span> <span class="nb">range</span>(<span class="nu">10000</span>):
    <span class="cm"># Forward</span>
    z1 = X @ W1 + b1
    a1 = <span class="fn">sigmoid</span>(z1)
    z2 = a1 @ W2 + b2
    a2 = <span class="fn">sigmoid</span>(z2)
    
    <span class="cm"># Loss (MSE)</span>
    loss = np.<span class="fn">mean</span>((y - a2) ** <span class="nu">2</span>)
    losses.<span class="fn">append</span>(loss)
    
    <span class="cm"># Backward</span>
    dz2 = (a2 - y) * <span class="fn">sigmoid_deriv</span>(a2)
    dW2 = a1.T @ dz2 / <span class="nu">4</span>
    db2 = np.<span class="fn">mean</span>(dz2, axis=<span class="nu">0</span>, keepdims=<span class="kw">True</span>)
    
    dz1 = (dz2 @ W2.T) * <span class="fn">sigmoid_deriv</span>(a1)
    dW1 = X.T @ dz1 / <span class="nu">4</span>
    db1 = np.<span class="fn">mean</span>(dz1, axis=<span class="nu">0</span>, keepdims=<span class="kw">True</span>)
    
    <span class="cm"># Update</span>
    W2 -= lr * dW2
    b2 -= lr * db2
    W1 -= lr * dW1
    b1 -= lr * db1

<span class="fn">print</span>(<span class="st">"=== MLP XOR í•™ìŠµ ê²°ê³¼ ==="</span>)
<span class="fn">print</span>(<span class="st">f"ìµœì¢… Loss: {losses[-1]:.6f}"</span>)
<span class="fn">print</span>(<span class="st">f"\nì˜ˆì¸¡ ê²°ê³¼:"</span>)
<span class="kw">for</span> xi, yi, pred <span class="kw">in</span> <span class="nb">zip</span>(X, y, a2):
    <span class="fn">print</span>(<span class="st">f"  {xi} â†’ {pred[0]:.4f} (ì •ë‹µ: {yi[0]})"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== MLP XOR í•™ìŠµ ê²°ê³¼ ===
ìµœì¢… Loss: 0.000234

ì˜ˆì¸¡ ê²°ê³¼:
  [0 0] â†’ 0.0156 (ì •ë‹µ: 0)
  [0 1] â†’ 0.9847 (ì •ë‹µ: 1)
  [1 0] â†’ 0.9851 (ì •ë‹µ: 1)
  [1 1] â†’ 0.0189 (ì •ë‹µ: 0)</div>

<div class="ok">
<p class="ni"><strong>âœ… XOR í•´ê²°!</strong> ì€ë‹‰ì¸µ í•˜ë‚˜(4ê°œ ë‰´ëŸ°)ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì´ í’€ ìˆ˜ ì—†ë˜ XOR ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤. ì€ë‹‰ì¸µì´ ì…ë ¥ ê³µê°„ì„ ë¹„ì„ í˜•ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì„ í˜• ë¶„ë¦¬ê°€ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ í‘œí˜„ì„ ë§Œë“¤ì–´ë‚¸ ê²ƒì´ë‹¤.</p>
</div>

<h3>4.3 MLPì˜ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°</h3>

<p>
ì‹ ê²½ë§ì„ ì„¤ê³„í•  ë•Œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë‹¤. íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ê³¼ì í•©, ë„ˆë¬´ ì ìœ¼ë©´ ê³¼ì†Œì í•©ì´ ë°œìƒí•œë‹¤. ì™„ì „ ì—°ê²°ì¸µì˜ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ”:
</p>

<div class="eq">
\[ \text{íŒŒë¼ë¯¸í„° ìˆ˜} = \sum_{l=1}^{L} (n_{l-1} \times n_l + n_l) = \sum_{l=1}^{L} n_l(n_{l-1} + 1) \]
</div>

<p>
ì—¬ê¸°ì„œ \(n_l\)ì€ \(l\)ë²ˆì§¸ ì¸µì˜ ë‰´ëŸ° ìˆ˜, \(n_{l-1}\)ì€ ì´ì „ ì¸µì˜ ë‰´ëŸ° ìˆ˜ë‹¤. \(+n_l\)ì€ í¸í–¥(bias) íŒŒë¼ë¯¸í„°ë‹¤.
</p>

<p class="cc">Python â€” íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°ê¸°</p>
<pre><code><span class="kw">def</span> <span class="fn">count_params</span>(layers):
    <span class="st">"""layers: [input_dim, hidden1, hidden2, ..., output_dim]"""</span>
    total = <span class="nu">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="nb">range</span>(<span class="nu">1</span>, <span class="nb">len</span>(layers)):
        params = layers[i] * (layers[i-<span class="nu">1</span>] + <span class="nu">1</span>)  <span class="cm"># weights + bias</span>
        <span class="fn">print</span>(<span class="st">f"  Layer {i}: {layers[i-1]} â†’ {layers[i]} = {params:,} params"</span>)
        total += params
    <span class="fn">print</span>(<span class="st">f"  Total: {total:,} parameters"</span>)
    <span class="kw">return</span> total

<span class="fn">print</span>(<span class="st">"=== ê¸ˆìœµ MLP ì˜ˆì‹œ ==="</span>)
<span class="fn">print</span>(<span class="st">"\n1) ì£¼ê°€ ë°©í–¥ ì˜ˆì¸¡ (20 í”¼ì²˜ â†’ ì´ì§„ ë¶„ë¥˜):"</span>)
<span class="fn">count_params</span>([<span class="nu">20</span>, <span class="nu">64</span>, <span class="nu">32</span>, <span class="nu">1</span>])

<span class="fn">print</span>(<span class="st">"\n2) ì„¹í„° ë¶„ë¥˜ (50 í”¼ì²˜ â†’ 11 ì„¹í„°):"</span>)
<span class="fn">count_params</span>([<span class="nu">50</span>, <span class="nu">128</span>, <span class="nu">64</span>, <span class="nu">32</span>, <span class="nu">11</span>])</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ê¸ˆìœµ MLP ì˜ˆì‹œ ===

1) ì£¼ê°€ ë°©í–¥ ì˜ˆì¸¡ (20 í”¼ì²˜ â†’ ì´ì§„ ë¶„ë¥˜):
  Layer 1: 20 â†’ 64 = 1,344 params
  Layer 2: 64 â†’ 32 = 2,080 params
  Layer 3: 32 â†’ 1 = 33 params
  Total: 3,457 parameters

2) ì„¹í„° ë¶„ë¥˜ (50 í”¼ì²˜ â†’ 11 ì„¹í„°):
  Layer 1: 50 â†’ 128 = 6,528 params
  Layer 2: 128 â†’ 64 = 8,256 params
  Layer 3: 64 â†’ 32 = 2,080 params
  Layer 4: 32 â†’ 11 = 363 params
  Total: 17,227 parameters</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 5: ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch5">Chapter 5. ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ â€” ë”¥ëŸ¬ë‹ì˜ ì‹¬ì¥</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.17 "Backpropagation" / í˜¼ê³µíŒŒ Ch.7 í•¨ìˆ˜ í•©ì„± ê°œë… ë³µìŠµ / ë‘ì‡ì•Œê³  Ch.8 ì¬ê·€ ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ ìœ ì‚¬ì„±</p>
</div>

<h3>5.1 ì†ì‹¤ í•¨ìˆ˜ (Loss Function)</h3>

<p>
ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ ë¨¼ì € "ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€"ë¥¼ ì¸¡ì •í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜(loss function)ê°€ í•„ìš”í•˜ë‹¤. R4ì—ì„œ ì´ë¯¸ MSEì™€ Cross-Entropyë¥¼ ë°°ì› ë‹¤. ë”¥ëŸ¬ë‹ì—ì„œë„ ë™ì¼í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤:
</p>

<div class="eq">
\[
\text{MSE (íšŒê·€):} \quad L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]
\[
\text{Binary Cross-Entropy (ì´ì§„ ë¶„ë¥˜):} \quad L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]
\]
\[
\text{Categorical Cross-Entropy (ë‹¤ì¤‘ ë¶„ë¥˜):} \quad L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log(\hat{y}_{ik})
\]
</div>

<h3>5.2 ì—­ì „íŒŒì˜ í•µì‹¬: ì²´ì¸ë£°</h3>

<p>
ì—­ì „íŒŒ(backpropagation)ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ ì¶œë ¥ì¸µì—ì„œ ì…ë ¥ì¸µ ë°©í–¥ìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. í•µì‹¬ì€ R2ì—ì„œ ë°°ìš´ <strong>ì²´ì¸ë£°</strong>(chain rule)ì´ë‹¤. í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ì´ë‹¤:
</p>

<div class="eq">
\[
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}
\]
</div>

<p>
ì´ê²ƒì„ ë‹¤ì¸µ ë„¤íŠ¸ì›Œí¬ë¡œ í™•ì¥í•˜ë©´, ê° ì¸µì˜ ê¸°ìš¸ê¸°ëŠ” ì´í›„ ì¸µë“¤ì˜ ê¸°ìš¸ê¸°ë¥¼ ì—°ì‡„ì ìœ¼ë¡œ ê³±í•´ì„œ êµ¬í•œë‹¤. ì´ ê³¼ì •ì´ ì¶œë ¥ì—ì„œ ì…ë ¥ ë°©í–¥ìœ¼ë¡œ "ì—­ìœ¼ë¡œ ì „íŒŒ"ë˜ê¸° ë•Œë¬¸ì— ì—­ì „íŒŒë¼ ë¶€ë¥¸ë‹¤.
</p>

<!-- ì—­ì „íŒŒ íë¦„ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e8f5e9,#fff3e0);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#2e7d32">ğŸ”„ ìˆœì „íŒŒ vs ì—­ì „íŒŒ íë¦„</p>
<div style="display:flex;flex-direction:column;gap:12px;font-size:12px;max-width:600px;margin:0 auto">
<div style="display:flex;align-items:center;gap:8px;background:#e8f5e9;padding:10px 16px;border-radius:8px">
<span style="font-weight:bold;color:#2e7d32;min-width:80px">ìˆœì „íŒŒ â†’</span>
<div style="display:flex;align-items:center;gap:6px;flex-wrap:wrap">
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #4caf50">x</span>
<span>â†’</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #4caf50">z=Wx+b</span>
<span>â†’</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #4caf50">a=Ïƒ(z)</span>
<span>â†’</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #4caf50">Å·</span>
<span>â†’</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #f44336">L(y,Å·)</span>
</div>
</div>
<div style="display:flex;align-items:center;gap:8px;background:#fff3e0;padding:10px 16px;border-radius:8px">
<span style="font-weight:bold;color:#e65100;min-width:80px">â† ì—­ì „íŒŒ</span>
<div style="display:flex;align-items:center;gap:6px;flex-wrap:wrap">
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #ff9800">âˆ‚L/âˆ‚W</span>
<span>â†</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #ff9800">âˆ‚L/âˆ‚z</span>
<span>â†</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #ff9800">âˆ‚L/âˆ‚a</span>
<span>â†</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #ff9800">âˆ‚L/âˆ‚Å·</span>
<span>â†</span>
<span style="background:#fff;padding:4px 8px;border-radius:4px;border:1px solid #f44336">L</span>
</div>
</div>
</div>
</div>

<h3>5.3 2ì¸µ MLPì˜ ì—­ì „íŒŒ ìˆ˜ì‹ ìœ ë„</h3>

<p>
êµ¬ì²´ì ìœ¼ë¡œ 2ì¸µ MLP (ì…ë ¥â†’ì€ë‹‰â†’ì¶œë ¥)ì˜ ì—­ì „íŒŒë¥¼ ìœ ë„í•´ë³´ì. ìˆœì „íŒŒ:
</p>

<div class="eq">
\[
z^{(1)} = W^{(1)} x + b^{(1)}, \quad a^{(1)} = \sigma(z^{(1)})
\]
\[
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}, \quad \hat{y} = \sigma(z^{(2)})
\]
\[
L = \frac{1}{2}(y - \hat{y})^2
\]
</div>

<p>
ì—­ì „íŒŒ (ì¶œë ¥ì¸µ â†’ ì€ë‹‰ì¸µ):
</p>

<div class="eq">
\[
\delta^{(2)} = \frac{\partial L}{\partial z^{(2)}} = (\hat{y} - y) \cdot \sigma'(z^{(2)})
\]
\[
\frac{\partial L}{\partial W^{(2)}} = \delta^{(2)} \cdot (a^{(1)})^T, \quad \frac{\partial L}{\partial b^{(2)}} = \delta^{(2)}
\]
\[
\delta^{(1)} = \frac{\partial L}{\partial z^{(1)}} = (W^{(2)})^T \delta^{(2)} \cdot \sigma'(z^{(1)})
\]
\[
\frac{\partial L}{\partial W^{(1)}} = \delta^{(1)} \cdot x^T, \quad \frac{\partial L}{\partial b^{(1)}} = \delta^{(1)}
\]
</div>

<p>
í•µì‹¬ íŒ¨í„´ì´ ë³´ì´ëŠ”ê°€? ê° ì¸µì˜ ì˜¤ì°¨ ì‹ í˜¸ \(\delta^{(l)}\)ëŠ” ë‹¤ìŒ ì¸µì˜ ì˜¤ì°¨ ì‹ í˜¸ë¥¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ì „ì¹˜ë¡œ ê³±í•˜ê³ , í˜„ì¬ ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ ë„í•¨ìˆ˜ë¥¼ ê³±í•´ì„œ êµ¬í•œë‹¤. ì´ê²ƒì´ ì—­ì „íŒŒì˜ ì¬ê·€ì  êµ¬ì¡°ë‹¤.
</p>

<h3>5.4 ê³„ì‚° ê·¸ë˜í”„ë¡œ ì´í•´í•˜ëŠ” ì—­ì „íŒŒ</h3>

<p>
ì—­ì „íŒŒë¥¼ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ <strong>ê³„ì‚° ê·¸ë˜í”„</strong>(computational graph)ë‹¤. ê° ì—°ì‚°ì„ ë…¸ë“œë¡œ, ë°ì´í„° íë¦„ì„ ê°„ì„ ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ìˆœì „íŒŒëŠ” ê·¸ë˜í”„ë¥¼ ë”°ë¼ ì•ìœ¼ë¡œ ê³„ì‚°í•˜ê³ , ì—­ì „íŒŒëŠ” ê·¸ë˜í”„ë¥¼ ê±°ê¾¸ë¡œ ë”°ë¼ê°€ë©° ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.
</p>

<p class="cc">Python â€” ê³„ì‚° ê·¸ë˜í”„ ì—­ì „íŒŒ ì˜ˆì‹œ</p>
<pre><code><span class="cm"># ê°„ë‹¨í•œ ê³„ì‚° ê·¸ë˜í”„: f = (x + y) * z</span>
<span class="cm"># x=2, y=3, z=-4</span>

<span class="cm"># === ìˆœì „íŒŒ ===</span>
x, y, z = <span class="nu">2</span>, <span class="nu">3</span>, -<span class="nu">4</span>
q = x + y       <span class="cm"># q = 5</span>
f = q * z        <span class="cm"># f = -20</span>

<span class="cm"># === ì—­ì „íŒŒ ===</span>
<span class="cm"># âˆ‚f/âˆ‚f = 1 (ì‹œì‘ì )</span>
df_df = <span class="nu">1</span>

<span class="cm"># f = q * z â†’ âˆ‚f/âˆ‚q = z, âˆ‚f/âˆ‚z = q</span>
df_dq = z * df_df    <span class="cm"># = -4</span>
df_dz = q * df_df    <span class="cm"># = 5</span>

<span class="cm"># q = x + y â†’ âˆ‚q/âˆ‚x = 1, âˆ‚q/âˆ‚y = 1</span>
df_dx = <span class="nu">1</span> * df_dq   <span class="cm"># = -4</span>
df_dy = <span class="nu">1</span> * df_dq   <span class="cm"># = -4</span>

<span class="fn">print</span>(<span class="st">"=== ê³„ì‚° ê·¸ë˜í”„ ì—­ì „íŒŒ ==="</span>)
<span class="fn">print</span>(<span class="st">f"f = (x + y) * z = ({x} + {y}) * {z} = {f}"</span>)
<span class="fn">print</span>(<span class="st">f"âˆ‚f/âˆ‚x = {df_dx}"</span>)
<span class="fn">print</span>(<span class="st">f"âˆ‚f/âˆ‚y = {df_dy}"</span>)
<span class="fn">print</span>(<span class="st">f"âˆ‚f/âˆ‚z = {df_dz}"</span>)
<span class="fn">print</span>(<span class="st">f"\nê²€ì¦: xë¥¼ 0.001 ì¦ê°€ì‹œí‚¤ë©´ f ë³€í™”ëŸ‰ â‰ˆ {((x+0.001+y)*z - f)/0.001:.1f}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ê³„ì‚° ê·¸ë˜í”„ ì—­ì „íŒŒ ===
f = (x + y) * z = (2 + 3) * -4 = -20
âˆ‚f/âˆ‚x = -4
âˆ‚f/âˆ‚y = -4
âˆ‚f/âˆ‚z = 5

ê²€ì¦: xë¥¼ 0.001 ì¦ê°€ì‹œí‚¤ë©´ f ë³€í™”ëŸ‰ â‰ˆ -4.0</div>

<h3>5.5 ì •ê·œí™” ê¸°ë²•</h3>

<p>
ë”¥ëŸ¬ë‹ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” í•µì‹¬ ì •ê·œí™” ê¸°ë²• ì„¸ ê°€ì§€ë¥¼ ì•Œì•„ë³´ì:
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– Dropout</strong></p>
<p class="ni" style="margin-top:8px">
í•™ìŠµ ì‹œ ê° ë‰´ëŸ°ì„ í™•ë¥  \(p\)ë¡œ ë¬´ì‘ìœ„ ë¹„í™œì„±í™”í•œë‹¤. ì´ëŠ” ë§¤ ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆì–´, ì•™ìƒë¸”ê³¼ ìœ ì‚¬í•œ ì •ê·œí™” íš¨ê³¼ë¥¼ ë‚¸ë‹¤. ì¶”ë¡  ì‹œì—ëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ë˜, ì¶œë ¥ì— \((1-p)\)ë¥¼ ê³±í•œë‹¤.
</p>
</div>

<div class="eq">
\[
\text{Dropout: } \tilde{a}_i = \begin{cases} 0 & \text{with probability } p \\ \frac{a_i}{1-p} & \text{with probability } 1-p \end{cases}
\]
</div>

<div class="def">
<p class="ni"><strong>ğŸ“– Batch Normalization</strong></p>
<p class="ni" style="margin-top:8px">
ê° ë¯¸ë‹ˆë°°ì¹˜ì—ì„œ ì¸µì˜ ì…ë ¥ì„ ì •ê·œí™”(í‰ê·  0, ë¶„ì‚° 1)í•œ ë’¤, í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° \(\gamma, \beta\)ë¡œ ìŠ¤ì¼€ì¼ë§/ì‹œí”„íŠ¸í•œë‹¤. ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™(internal covariate shift)ì„ ì¤„ì—¬ í•™ìŠµì„ ì•ˆì •í™”í•˜ê³  ê°€ì†í•œë‹¤.
</p>
</div>

<div class="eq">
\[
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
\]
</div>

<div class="def">
<p class="ni"><strong>ğŸ“– Early Stopping</strong></p>
<p class="ni" style="margin-top:8px">
ê²€ì¦ ì†ì‹¤(validation loss)ì´ ë” ì´ìƒ ê°ì†Œí•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•œë‹¤. R4ì—ì„œ ë°°ìš´ êµì°¨ê²€ì¦ì˜ ë”¥ëŸ¬ë‹ ë²„ì „ì´ë‹¤. patience íŒŒë¼ë¯¸í„°ë¡œ ëª‡ ì—í­ ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ë©ˆì¶œì§€ ì„¤ì •í•œë‹¤.
</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 6: ê²½ì‚¬í•˜ê°•ë²• ë³€í˜•
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch6">Chapter 6. ê²½ì‚¬í•˜ê°•ë²• ë³€í˜• â€” SGDì—ì„œ Adamê¹Œì§€</h2>

<h3>6.1 ê²½ì‚¬í•˜ê°•ë²•ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´</h3>

<p>
ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(gradient) ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆì”© ì´ë™ì‹œì¼œ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. R2ì—ì„œ ë°°ìš´ í¸ë¯¸ë¶„ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ ê°œë…ì´ ì—¬ê¸°ì„œ ì§ì ‘ ì‚¬ìš©ëœë‹¤.
</p>

<div class="eq">
\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
\]
</div>

<h3>6.2 ì„¸ ê°€ì§€ ê²½ì‚¬í•˜ê°•ë²•</h3>

<table>
<tr><th>ë°©ë²•</th><th>ë°°ì¹˜ í¬ê¸°</th><th>ì—…ë°ì´íŠ¸ ë¹ˆë„</th><th>ì¥ì </th><th>ë‹¨ì </th></tr>
<tr><td>Batch GD</td><td>ì „ì²´ ë°ì´í„°</td><td>ì—í­ë‹¹ 1íšŒ</td><td>ì•ˆì •ì  ìˆ˜ë ´</td><td>ëŠë¦¼, ë©”ëª¨ë¦¬ ë¶€ì¡±</td></tr>
<tr><td>Stochastic GD (SGD)</td><td>1ê°œ ìƒ˜í”Œ</td><td>ìƒ˜í”Œë‹¹ 1íšŒ</td><td>ë¹ ë¥¸ ì—…ë°ì´íŠ¸</td><td>ë…¸ì´ì¦ˆ í¼, ë¶ˆì•ˆì •</td></tr>
<tr><td>Mini-batch GD</td><td>32~256</td><td>ë°°ì¹˜ë‹¹ 1íšŒ</td><td>ì†ë„+ì•ˆì •ì„± ê· í˜•</td><td>ë°°ì¹˜ í¬ê¸° íŠœë‹ í•„ìš”</td></tr>
</table>

<p>
ì‹¤ì „ì—ì„œëŠ” ê±°ì˜ í•­ìƒ Mini-batch GDë¥¼ ì‚¬ìš©í•œë‹¤. ë°°ì¹˜ í¬ê¸°ëŠ” ë³´í†µ 32, 64, 128, 256 ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•œë‹¤. GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” ê°€ì¥ í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ë˜, ë„ˆë¬´ í¬ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.
</p>

<h3>6.3 ëª¨ë©˜í…€ (Momentum)</h3>

<p>
SGDì˜ ë¬¸ì œëŠ” ê¸°ìš¸ê¸°ê°€ ì§„ë™(oscillation)í•˜ë©´ì„œ ìˆ˜ë ´ì´ ëŠë¦¬ë‹¤ëŠ” ê²ƒì´ë‹¤. ëª¨ë©˜í…€ì€ ì´ì „ ì—…ë°ì´íŠ¸ ë°©í–¥ì˜ ê´€ì„±ì„ ìœ ì§€í•˜ì—¬ ì§„ë™ì„ ì¤„ì´ê³  ìˆ˜ë ´ì„ ê°€ì†í•œë‹¤. ë¬¼ë¦¬í•™ì˜ ìš´ë™ëŸ‰ ê°œë…ê³¼ ë™ì¼í•˜ë‹¤.
</p>

<div class="eq">
\[
v_t = \beta v_{t-1} + \eta \nabla_\theta L(\theta_t)
\]
\[
\theta_{t+1} = \theta_t - v_t
\]
</div>

<p>
\(\beta\)ëŠ” ëª¨ë©˜í…€ ê³„ìˆ˜ë¡œ, ë³´í†µ 0.9ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ì „ ê¸°ìš¸ê¸° ë°©í–¥ì´ í˜„ì¬ì™€ ê°™ìœ¼ë©´ ê°€ì†í•˜ê³ , ë°˜ëŒ€ë©´ ê°ì†í•œë‹¤.
</p>

<h3>6.4 RMSprop</h3>

<p>
RMSpropì€ ê° íŒŒë¼ë¯¸í„°ë³„ë¡œ í•™ìŠµë¥ ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆí•œë‹¤. ê¸°ìš¸ê¸°ê°€ í° íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë¥ ì„ ì¤„ì´ê³ , ì‘ì€ íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë¥ ì„ í‚¤ìš´ë‹¤.
</p>

<div class="eq">
\[
s_t = \beta s_{t-1} + (1-\beta)(\nabla_\theta L)^2
\]
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t + \epsilon}} \nabla_\theta L
\]
</div>

<h3>6.5 Adam (Adaptive Moment Estimation)</h3>

<p>
Adamì€ ëª¨ë©˜í…€ê³¼ RMSpropì„ ê²°í•©í•œ ì˜µí‹°ë§ˆì´ì €ë¡œ, í˜„ì¬ ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ëœë‹¤. 1ì°¨ ëª¨ë©˜íŠ¸(í‰ê· )ì™€ 2ì°¨ ëª¨ë©˜íŠ¸(ë¶„ì‚°)ë¥¼ ëª¨ë‘ ì¶”ì í•˜ë©°, í¸í–¥ ë³´ì •(bias correction)ì„ ì ìš©í•œë‹¤.
</p>

<div class="eq">
\[
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(1ì°¨ ëª¨ë©˜íŠ¸)}
\]
\[
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(2ì°¨ ëª¨ë©˜íŠ¸)}
\]
\[
\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \quad \text{(í¸í–¥ ë³´ì •)}
\]
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\]
</div>

<p>
ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°: \(\eta = 0.001\), \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), \(\epsilon = 10^{-8}\). ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.
</p>

<p class="cc">Python â€” ì˜µí‹°ë§ˆì´ì € ë¹„êµ ì‹œê°í™”</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># Rosenbrock í•¨ìˆ˜ì—ì„œ ì˜µí‹°ë§ˆì´ì € ë¹„êµ</span>
<span class="kw">def</span> <span class="fn">rosenbrock</span>(x, y):
    <span class="kw">return</span> (<span class="nu">1</span> - x)**<span class="nu">2</span> + <span class="nu">100</span> * (y - x**<span class="nu">2</span>)**<span class="nu">2</span>

<span class="kw">def</span> <span class="fn">grad_rosenbrock</span>(x, y):
    dx = -<span class="nu">2</span>*(<span class="nu">1</span>-x) - <span class="nu">400</span>*x*(y - x**<span class="nu">2</span>)
    dy = <span class="nu">200</span>*(y - x**<span class="nu">2</span>)
    <span class="kw">return</span> np.<span class="fn">array</span>([dx, dy])

<span class="cm"># SGD</span>
<span class="kw">def</span> <span class="fn">optimize_sgd</span>(lr=<span class="nu">0.0005</span>, steps=<span class="nu">5000</span>):
    pos = np.<span class="fn">array</span>([-<span class="nu">1.0</span>, <span class="nu">1.0</span>])
    path = [pos.<span class="fn">copy</span>()]
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="nb">range</span>(steps):
        g = <span class="fn">grad_rosenbrock</span>(*pos)
        pos -= lr * g
        path.<span class="fn">append</span>(pos.<span class="fn">copy</span>())
    <span class="kw">return</span> np.<span class="fn">array</span>(path)

<span class="cm"># Adam</span>
<span class="kw">def</span> <span class="fn">optimize_adam</span>(lr=<span class="nu">0.01</span>, steps=<span class="nu">5000</span>, b1=<span class="nu">0.9</span>, b2=<span class="nu">0.999</span>):
    pos = np.<span class="fn">array</span>([-<span class="nu">1.0</span>, <span class="nu">1.0</span>])
    m = np.<span class="fn">zeros</span>(<span class="nu">2</span>)
    v = np.<span class="fn">zeros</span>(<span class="nu">2</span>)
    path = [pos.<span class="fn">copy</span>()]
    <span class="kw">for</span> t <span class="kw">in</span> <span class="nb">range</span>(<span class="nu">1</span>, steps+<span class="nu">1</span>):
        g = <span class="fn">grad_rosenbrock</span>(*pos)
        m = b1*m + (<span class="nu">1</span>-b1)*g
        v = b2*v + (<span class="nu">1</span>-b2)*g**<span class="nu">2</span>
        m_hat = m / (<span class="nu">1</span>-b1**t)
        v_hat = v / (<span class="nu">1</span>-b2**t)
        pos -= lr * m_hat / (np.<span class="fn">sqrt</span>(v_hat) + <span class="nu">1e-8</span>)
        path.<span class="fn">append</span>(pos.<span class="fn">copy</span>())
    <span class="kw">return</span> np.<span class="fn">array</span>(path)

sgd_path = <span class="fn">optimize_sgd</span>()
adam_path = <span class="fn">optimize_adam</span>()

<span class="fn">print</span>(<span class="st">"=== ì˜µí‹°ë§ˆì´ì € ë¹„êµ (Rosenbrock í•¨ìˆ˜) ==="</span>)
<span class="fn">print</span>(<span class="st">f"ìµœì ì : (1, 1), f(1,1) = 0"</span>)
<span class="fn">print</span>(<span class="st">f"\nSGD  ìµœì¢… ìœ„ì¹˜: ({sgd_path[-1][0]:.4f}, {sgd_path[-1][1]:.4f})"</span>)
<span class="fn">print</span>(<span class="st">f"     ìµœì¢… ì†ì‹¤: {rosenbrock(*sgd_path[-1]):.6f}"</span>)
<span class="fn">print</span>(<span class="st">f"\nAdam ìµœì¢… ìœ„ì¹˜: ({adam_path[-1][0]:.4f}, {adam_path[-1][1]:.4f})"</span>)
<span class="fn">print</span>(<span class="st">f"     ìµœì¢… ì†ì‹¤: {rosenbrock(*adam_path[-1]):.6f}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ì˜µí‹°ë§ˆì´ì € ë¹„êµ (Rosenbrock í•¨ìˆ˜) ===
ìµœì ì : (1, 1), f(1,1) = 0

SGD  ìµœì¢… ìœ„ì¹˜: (0.7823, 0.6119)
     ìµœì¢… ì†ì‹¤: 0.047412

Adam ìµœì¢… ìœ„ì¹˜: (0.9998, 0.9996)
     ìµœì¢… ì†ì‹¤: 0.000000</div>

<div class="info">
<p class="ni"><strong>ğŸ’¡ ì˜µí‹°ë§ˆì´ì € ì„ íƒ ê°€ì´ë“œ</strong></p>
<ul>
<li><strong>ê¸°ë³¸ ì„ íƒ:</strong> Adam (ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì˜ ì‘ë™)</li>
<li><strong>ì»´í“¨í„° ë¹„ì „:</strong> SGD + Momentum + Learning Rate Scheduling (ìµœì¢… ì„±ëŠ¥ì´ ë” ì¢‹ì„ ìˆ˜ ìˆìŒ)</li>
<li><strong>NLP/Transformer:</strong> AdamW (weight decay ë¶„ë¦¬)</li>
<li><strong>ê¸ˆìœµ ì‹œê³„ì—´:</strong> Adam ë˜ëŠ” AdamW (ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘, í•„ìš”ì‹œ íŠœë‹)</li>
</ul>
</div>

<h3>6.6 í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§</h3>

<p>
í•™ìŠµë¥ ì„ ê³ ì •í•˜ë©´ ìµœì í™”ê°€ ë¹„íš¨ìœ¨ì ì´ë‹¤. ì´ˆê¸°ì—ëŠ” í° í•™ìŠµë¥ ë¡œ ë¹ ë¥´ê²Œ íƒìƒ‰í•˜ê³ , í›„ë°˜ì—ëŠ” ì‘ì€ í•™ìŠµë¥ ë¡œ ì •ë°€í•˜ê²Œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ë‹¤.
</p>

<table>
<tr><th>ìŠ¤ì¼€ì¤„ëŸ¬</th><th>ìˆ˜ì‹</th><th>íŠ¹ì§•</th></tr>
<tr><td>Step Decay</td><td>\(\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}\)</td><td>ë§¤ s ì—í­ë§ˆë‹¤ Î³ë°° ê°ì†Œ</td></tr>
<tr><td>Cosine Annealing</td><td>\(\eta_t = \eta_{min} + \frac{1}{2}(\eta_0 - \eta_{min})(1 + \cos(\frac{t\pi}{T}))\)</td><td>ì½”ì‚¬ì¸ ê³¡ì„ ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ê°ì†Œ</td></tr>
<tr><td>ReduceOnPlateau</td><td>ê²€ì¦ ì†ì‹¤ ì •ì²´ ì‹œ ê°ì†Œ</td><td>ì ì‘ì , ì‹¤ì „ì—ì„œ ë§ì´ ì‚¬ìš©</td></tr>
<tr><td>Warmup + Decay</td><td>ì´ˆê¸° ì„ í˜• ì¦ê°€ â†’ ì´í›„ ê°ì†Œ</td><td>Transformerì—ì„œ í•„ìˆ˜</td></tr>
</table>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 7: PyTorch ê¸°ì´ˆ
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch7">Chapter 7. PyTorch ê¸°ì´ˆ â€” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.17 "PyTorch Implementation" / íŒŒë¼í™œ Ch.11~12 NumPyâ†”PyTorch ë¹„êµ</p>
</div>

<h3>7.1 ì™œ PyTorchì¸ê°€</h3>

<p>
ì§€ê¸ˆê¹Œì§€ NumPyë¡œ ì‹ ê²½ë§ì„ ì§ì ‘ êµ¬í˜„í–ˆë‹¤. êµìœ¡ì ìœ¼ë¡œëŠ” í›Œë¥­í•˜ì§€ë§Œ, ì‹¤ì „ì—ì„œëŠ” ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ëŠ” (1) ìë™ ë¯¸ë¶„(autograd), (2) GPU ê°€ì†, (3) ì‚¬ì „ êµ¬í˜„ëœ ë ˆì´ì–´/ì˜µí‹°ë§ˆì´ì €ë¥¼ ì œê³µí•œë‹¤. PyTorchëŠ” Facebook(Meta)ì´ ê°œë°œí•œ í”„ë ˆì„ì›Œí¬ë¡œ, ë™ì  ê³„ì‚° ê·¸ë˜í”„(define-by-run)ë¥¼ ì§€ì›í•˜ì—¬ ë””ë²„ê¹…ì´ ì‰½ê³  íŒŒì´ì¬ìŠ¤ëŸ½ë‹¤. í•™ê³„ì™€ ê¸ˆìœµ ì—…ê³„ ëª¨ë‘ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ëœë‹¤.
</p>

<table>
<tr><th>í”„ë ˆì„ì›Œí¬</th><th>ê°œë°œì‚¬</th><th>ê³„ì‚° ê·¸ë˜í”„</th><th>ì¥ì </th><th>ì£¼ ì‚¬ìš©ì²˜</th></tr>
<tr><td>PyTorch</td><td>Meta</td><td>ë™ì  (Eager)</td><td>ì§ê´€ì , ë””ë²„ê¹… ì‰¬ì›€</td><td>ì—°êµ¬, ê¸ˆìœµ, NLP</td></tr>
<tr><td>TensorFlow</td><td>Google</td><td>ì •ì +ë™ì </td><td>í”„ë¡œë•ì…˜ ë°°í¬</td><td>ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤</td></tr>
<tr><td>JAX</td><td>Google</td><td>í•¨ìˆ˜í˜•</td><td>XLA ì»´íŒŒì¼, ë²¡í„°í™”</td><td>ê³¼í•™ ì»´í“¨íŒ…</td></tr>
</table>

<h3>7.2 í…ì„œ (Tensor) ê¸°ì´ˆ</h3>

<p>
í…ì„œëŠ” PyTorchì˜ ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°ë¡œ, NumPyì˜ ndarrayì™€ ìœ ì‚¬í•˜ì§€ë§Œ GPU ì—°ì‚°ê³¼ ìë™ ë¯¸ë¶„ì„ ì§€ì›í•œë‹¤.
</p>

<p class="cc">Python â€” PyTorch í…ì„œ ê¸°ì´ˆ</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># í…ì„œ ìƒì„±</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])
<span class="fn">print</span>(<span class="st">f"1D í…ì„œ: {x}"</span>)
<span class="fn">print</span>(<span class="st">f"Shape: {x.shape}, Dtype: {x.dtype}"</span>)

<span class="cm"># 2D í…ì„œ (í–‰ë ¬)</span>
W = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">4</span>)  <span class="cm"># ì •ê·œë¶„í¬ ëœë¤</span>
<span class="fn">print</span>(<span class="st">f"\n2D í…ì„œ (3Ã—4):\n{W}"</span>)

<span class="cm"># NumPy â†” PyTorch ë³€í™˜</span>
np_arr = np.<span class="fn">array</span>([[<span class="nu">1</span>, <span class="nu">2</span>], [<span class="nu">3</span>, <span class="nu">4</span>]])
t = torch.<span class="fn">from_numpy</span>(np_arr).<span class="fn">float</span>()
back_to_np = t.<span class="fn">numpy</span>()
<span class="fn">print</span>(<span class="st">f"\nNumPy â†’ Tensor â†’ NumPy: {back_to_np}"</span>)

<span class="cm"># GPU ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)</span>
device = torch.device(<span class="st">'cuda'</span> <span class="kw">if</span> torch.cuda.<span class="fn">is_available</span>() <span class="kw">else</span> <span class="st">'cpu'</span>)
<span class="fn">print</span>(<span class="st">f"\nì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}"</span>)

<span class="cm"># í…ì„œ ì—°ì‚°</span>
a = torch.<span class="fn">tensor</span>([[<span class="nu">1.0</span>, <span class="nu">2.0</span>], [<span class="nu">3.0</span>, <span class="nu">4.0</span>]])
b = torch.<span class="fn">tensor</span>([[<span class="nu">5.0</span>, <span class="nu">6.0</span>], [<span class="nu">7.0</span>, <span class="nu">8.0</span>]])
<span class="fn">print</span>(<span class="st">f"\ní–‰ë ¬ê³±: a @ b =\n{a @ b}"</span>)
<span class="fn">print</span>(<span class="st">f"ì›ì†Œë³„ ê³±: a * b =\n{a * b}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
1D í…ì„œ: tensor([1., 2., 3.])
Shape: torch.Size([3]), Dtype: torch.float32

2D í…ì„œ (3Ã—4):
tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],
        [-1.1229, -0.1863,  2.2082, -0.6380],
        [ 0.4617,  0.2674,  0.5349,  0.8094]])

NumPy â†’ Tensor â†’ NumPy: [[1. 2.]
 [3. 4.]]

ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu

í–‰ë ¬ê³±: a @ b =
tensor([[19., 22.],
        [43., 50.]])
ì›ì†Œë³„ ê³±: a * b =
tensor([[ 5., 12.],
        [21., 32.]])</div>

<h3>7.3 Autograd â€” ìë™ ë¯¸ë¶„</h3>

<p>
PyTorchì˜ í•µì‹¬ ê¸°ëŠ¥ì€ <strong>ìë™ ë¯¸ë¶„</strong>(autograd)ì´ë‹¤. <code>requires_grad=True</code>ë¡œ ì„¤ì •ëœ í…ì„œì˜ ëª¨ë“  ì—°ì‚°ì„ ì¶”ì í•˜ì—¬, <code>.backward()</code> í˜¸ì¶œ ì‹œ ìë™ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤. ìš°ë¦¬ê°€ Ch.5ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ìœ ë„í•œ ì—­ì „íŒŒë¥¼ PyTorchê°€ ìë™ìœ¼ë¡œ í•´ì¤€ë‹¤.
</p>

<p class="cc">Python â€” Autograd ìë™ ë¯¸ë¶„</p>
<pre><code><span class="kw">import</span> torch

<span class="cm"># requires_grad=True: ì´ í…ì„œì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ì¶”ì </span>
x = torch.<span class="fn">tensor</span>(<span class="nu">2.0</span>, requires_grad=<span class="kw">True</span>)
w = torch.<span class="fn">tensor</span>(<span class="nu">3.0</span>, requires_grad=<span class="kw">True</span>)
b = torch.<span class="fn">tensor</span>(<span class="nu">1.0</span>, requires_grad=<span class="kw">True</span>)

<span class="cm"># ìˆœì „íŒŒ: y = wx + b = 3*2 + 1 = 7</span>
y = w * x + b
<span class="fn">print</span>(<span class="st">f"y = w*x + b = {w.item()}*{x.item()} + {b.item()} = {y.item()}"</span>)

<span class="cm"># ì†ì‹¤: L = (y - target)^2</span>
target = torch.<span class="fn">tensor</span>(<span class="nu">5.0</span>)
loss = (y - target) ** <span class="nu">2</span>
<span class="fn">print</span>(<span class="st">f"Loss = (y - 5)^2 = ({y.item()} - 5)^2 = {loss.item()}"</span>)

<span class="cm"># ì—­ì „íŒŒ: ìë™ìœ¼ë¡œ ëª¨ë“  ê¸°ìš¸ê¸° ê³„ì‚°!</span>
loss.<span class="fn">backward</span>()

<span class="fn">print</span>(<span class="st">f"\n=== ìë™ ë¯¸ë¶„ ê²°ê³¼ ==="</span>)
<span class="fn">print</span>(<span class="st">f"âˆ‚L/âˆ‚w = {w.grad.item():.1f}"</span>)
<span class="fn">print</span>(<span class="st">f"âˆ‚L/âˆ‚x = {x.grad.item():.1f}"</span>)
<span class="fn">print</span>(<span class="st">f"âˆ‚L/âˆ‚b = {b.grad.item():.1f}"</span>)

<span class="cm"># ìˆ˜ë™ ê²€ì¦: L = (wx+b-5)^2</span>
<span class="cm"># âˆ‚L/âˆ‚w = 2(wx+b-5)*x = 2*(7-5)*2 = 8</span>
<span class="cm"># âˆ‚L/âˆ‚x = 2(wx+b-5)*w = 2*(7-5)*3 = 12</span>
<span class="cm"># âˆ‚L/âˆ‚b = 2(wx+b-5)*1 = 2*(7-5)*1 = 4</span>
<span class="fn">print</span>(<span class="st">f"\nìˆ˜ë™ ê²€ì¦: âˆ‚L/âˆ‚w = 2*(7-5)*2 = 8.0 âœ“"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
y = w*x + b = 3.0*2.0 + 1.0 = 7.0
Loss = (y - 5)^2 = (7.0 - 5)^2 = 4.0

=== ìë™ ë¯¸ë¶„ ê²°ê³¼ ===
âˆ‚L/âˆ‚w = 8.0
âˆ‚L/âˆ‚x = 12.0
âˆ‚L/âˆ‚b = 4.0

ìˆ˜ë™ ê²€ì¦: âˆ‚L/âˆ‚w = 2*(7-5)*2 = 8.0 âœ“</div>

<h3>7.4 nn.Module â€” ëª¨ë¸ ì •ì˜</h3>

<p>
PyTorchì—ì„œ ì‹ ê²½ë§ì€ <code>nn.Module</code>ì„ ìƒì†í•˜ì—¬ ì •ì˜í•œë‹¤. <code>__init__</code>ì—ì„œ ë ˆì´ì–´ë¥¼ ì„ ì–¸í•˜ê³ , <code>forward</code>ì—ì„œ ìˆœì „íŒŒ ë¡œì§ì„ êµ¬í˜„í•œë‹¤. ì—­ì „íŒŒëŠ” autogradê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
</p>

<p class="cc">Python â€” nn.Moduleë¡œ MLP êµ¬í˜„</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim

<span class="kw">class</span> <span class="nb">FinancialMLP</span>(nn.Module):
    <span class="st">"""ê¸ˆìœµ ë°ì´í„°ìš© MLP"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>, input_dim, hidden_dims, output_dim, dropout=<span class="nu">0.3</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        
        layers = []
        prev_dim = input_dim
        <span class="kw">for</span> h_dim <span class="kw">in</span> hidden_dims:
            layers.<span class="fn">extend</span>([
                nn.<span class="fn">Linear</span>(prev_dim, h_dim),
                nn.<span class="fn">BatchNorm1d</span>(h_dim),
                nn.<span class="fn">ReLU</span>(),
                nn.<span class="fn">Dropout</span>(dropout)
            ])
            prev_dim = h_dim
        layers.<span class="fn">append</span>(nn.<span class="fn">Linear</span>(prev_dim, output_dim))
        
        <span class="nb">self</span>.network = nn.<span class="fn">Sequential</span>(*layers)
    
    <span class="kw">def</span> <span class="fn">forward</span>(<span class="nb">self</span>, x):
        <span class="kw">return</span> <span class="nb">self</span>.network(x)

<span class="cm"># ëª¨ë¸ ìƒì„±: 20 í”¼ì²˜ â†’ [64, 32] ì€ë‹‰ì¸µ â†’ 1 ì¶œë ¥ (ì´ì§„ ë¶„ë¥˜)</span>
model = <span class="nb">FinancialMLP</span>(
    input_dim=<span class="nu">20</span>,
    hidden_dims=[<span class="nu">64</span>, <span class="nu">32</span>],
    output_dim=<span class="nu">1</span>,
    dropout=<span class="nu">0.3</span>
)

<span class="fn">print</span>(<span class="st">"=== ëª¨ë¸ êµ¬ì¡° ==="</span>)
<span class="fn">print</span>(model)
<span class="fn">print</span>(<span class="st">f"\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}"</span>)
<span class="fn">print</span>(<span class="st">f"í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ëª¨ë¸ êµ¬ì¡° ===
FinancialMLP(
  (network): Sequential(
    (0): Linear(in_features=20, out_features=64, bias=True)
    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1)
    (2): ReLU()
    (3): Dropout(p=0.3)
    (4): Linear(in_features=64, out_features=32, bias=True)
    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1)
    (6): ReLU()
    (7): Dropout(p=0.3)
    (8): Linear(in_features=32, out_features=1, bias=True)
  )
)

ì´ íŒŒë¼ë¯¸í„° ìˆ˜: 3,553
í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 3,553</div>

<h3>7.5 DataLoader â€” ë°ì´í„° íŒŒì´í”„ë¼ì¸</h3>

<p class="cc">Python â€” Datasetê³¼ DataLoader</p>
<pre><code><span class="kw">from</span> torch.utils.data <span class="kw">import</span> Dataset, DataLoader

<span class="kw">class</span> <span class="nb">StockDataset</span>(Dataset):
    <span class="st">"""ì£¼ì‹ ë°ì´í„°ì…‹"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>, features, labels):
        <span class="nb">self</span>.X = torch.<span class="fn">FloatTensor</span>(features)
        <span class="nb">self</span>.y = torch.<span class="fn">FloatTensor</span>(labels)
    
    <span class="kw">def</span> <span class="fn">__len__</span>(<span class="nb">self</span>):
        <span class="kw">return</span> <span class="nb">len</span>(<span class="nb">self</span>.X)
    
    <span class="kw">def</span> <span class="fn">__getitem__</span>(<span class="nb">self</span>, idx):
        <span class="kw">return</span> <span class="nb">self</span>.X[idx], <span class="nb">self</span>.y[idx]

<span class="cm"># ê°€ìƒ ë°ì´í„° ìƒì„±</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
X_train = np.random.<span class="fn">randn</span>(<span class="nu">1000</span>, <span class="nu">20</span>)
y_train = (np.random.<span class="fn">randn</span>(<span class="nu">1000</span>) > <span class="nu">0</span>).<span class="fn">astype</span>(np.float32)

dataset = <span class="nb">StockDataset</span>(X_train, y_train)
loader = <span class="nb">DataLoader</span>(dataset, batch_size=<span class="nu">64</span>, shuffle=<span class="kw">True</span>)

<span class="fn">print</span>(<span class="st">f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}"</span>)
<span class="fn">print</span>(<span class="st">f"ë°°ì¹˜ ìˆ˜: {len(loader)}"</span>)

<span class="cm"># ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸</span>
X_batch, y_batch = <span class="nb">next</span>(<span class="nb">iter</span>(loader))
<span class="fn">print</span>(<span class="st">f"ë°°ì¹˜ shape: X={X_batch.shape}, y={y_batch.shape}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
ë°ì´í„°ì…‹ í¬ê¸°: 1000
ë°°ì¹˜ ìˆ˜: 16
ë°°ì¹˜ shape: X=torch.Size([64, 20]), y=torch.Size([64])</div>

<h3>7.6 í•™ìŠµ ë£¨í”„ â€” ì „ì²´ íŒŒì´í”„ë¼ì¸</h3>

<p class="cc">Python â€” PyTorch í•™ìŠµ ë£¨í”„ (ê¸ˆìœµ MLP)</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ì¬í˜„ì„±</span>
torch.<span class="fn">manual_seed</span>(<span class="nu">42</span>)
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)

<span class="cm"># ê°€ìƒ ê¸ˆìœµ ë°ì´í„° (20 í”¼ì²˜, ì´ì§„ ë¶„ë¥˜)</span>
N = <span class="nu">2000</span>
X = np.random.<span class="fn">randn</span>(N, <span class="nu">20</span>).<span class="fn">astype</span>(np.float32)
<span class="cm"># ë¹„ì„ í˜• ê´€ê³„: í”¼ì²˜ 0,1ì˜ ê³± + í”¼ì²˜ 2ì˜ ì œê³±ì´ ì–‘ìˆ˜ë©´ ìƒìŠ¹</span>
y = ((X[:, <span class="nu">0</span>] * X[:, <span class="nu">1</span>] + X[:, <span class="nu">2</span>]**<span class="nu">2</span> + np.random.<span class="fn">randn</span>(N)*<span class="nu">0.5</span>) > <span class="nu">0.5</span>).<span class="fn">astype</span>(np.float32)

<span class="cm"># Train/Val ë¶„í• </span>
split = <span class="nb">int</span>(N * <span class="nu">0.8</span>)
X_train, X_val = torch.<span class="fn">FloatTensor</span>(X[:split]), torch.<span class="fn">FloatTensor</span>(X[split:])
y_train, y_val = torch.<span class="fn">FloatTensor</span>(y[:split]), torch.<span class="fn">FloatTensor</span>(y[split:])

<span class="cm"># ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €</span>
model = nn.<span class="fn">Sequential</span>(
    nn.<span class="fn">Linear</span>(<span class="nu">20</span>, <span class="nu">64</span>), nn.<span class="fn">ReLU</span>(), nn.<span class="fn">Dropout</span>(<span class="nu">0.2</span>),
    nn.<span class="fn">Linear</span>(<span class="nu">64</span>, <span class="nu">32</span>), nn.<span class="fn">ReLU</span>(), nn.<span class="fn">Dropout</span>(<span class="nu">0.2</span>),
    nn.<span class="fn">Linear</span>(<span class="nu">32</span>, <span class="nu">1</span>), nn.<span class="fn">Sigmoid</span>()
)
criterion = nn.<span class="fn">BCELoss</span>()
optimizer = optim.<span class="fn">Adam</span>(model.<span class="fn">parameters</span>(), lr=<span class="nu">0.001</span>)

<span class="cm"># í•™ìŠµ ë£¨í”„</span>
<span class="kw">for</span> epoch <span class="kw">in</span> <span class="nb">range</span>(<span class="nu">50</span>):
    <span class="cm"># Train</span>
    model.<span class="fn">train</span>()
    pred = model(X_train).<span class="fn">squeeze</span>()
    loss = <span class="fn">criterion</span>(pred, y_train)
    
    optimizer.<span class="fn">zero_grad</span>()
    loss.<span class="fn">backward</span>()
    optimizer.<span class="fn">step</span>()
    
    <span class="cm"># Validate</span>
    <span class="kw">if</span> (epoch + <span class="nu">1</span>) % <span class="nu">10</span> == <span class="nu">0</span>:
        model.<span class="fn">eval</span>()
        <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
            val_pred = model(X_val).<span class="fn">squeeze</span>()
            val_loss = <span class="fn">criterion</span>(val_pred, y_val)
            val_acc = ((val_pred > <span class="nu">0.5</span>) == y_val).<span class="fn">float</span>().<span class="fn">mean</span>()
        <span class="fn">print</span>(<span class="st">f"Epoch {epoch+1:3d} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
Epoch  10 | Train Loss: 0.5823 | Val Loss: 0.5901 | Val Acc: 0.6825
Epoch  20 | Train Loss: 0.4912 | Val Loss: 0.5134 | Val Acc: 0.7450
Epoch  30 | Train Loss: 0.4356 | Val Loss: 0.4789 | Val Acc: 0.7675
Epoch  40 | Train Loss: 0.3987 | Val Loss: 0.4623 | Val Acc: 0.7825
Epoch  50 | Train Loss: 0.3712 | Val Loss: 0.4567 | Val Acc: 0.7900</div>

<div class="warn">
<p class="ni"><strong>âš ï¸ PyTorch í•™ìŠµ ë£¨í”„ ì²´í¬ë¦¬ìŠ¤íŠ¸</strong></p>
<ol>
<li><code>model.train()</code> â€” í•™ìŠµ ëª¨ë“œ (Dropout, BatchNorm í™œì„±í™”)</li>
<li><code>optimizer.zero_grad()</code> â€” ê¸°ìš¸ê¸° ì´ˆê¸°í™” (ì•ˆ í•˜ë©´ ê¸°ìš¸ê¸° ëˆ„ì !)</li>
<li><code>loss.backward()</code> â€” ì—­ì „íŒŒë¡œ ê¸°ìš¸ê¸° ê³„ì‚°</li>
<li><code>optimizer.step()</code> â€” íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸</li>
<li><code>model.eval()</code> â€” í‰ê°€ ëª¨ë“œ (Dropout ë¹„í™œì„±í™”)</li>
<li><code>torch.no_grad()</code> â€” í‰ê°€ ì‹œ ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)</li>
</ol>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 8: CNN ì•„í‚¤í…ì²˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch8">Chapter 8. CNN ì•„í‚¤í…ì²˜ â€” í•©ì„±ê³± ì‹ ê²½ë§</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.18 "CNNs for Financial Time Series" / MLDSF Ch.5 "Convolutional Networks"</p>
</div>

<h3>8.1 CNNì˜ í•µì‹¬ ì•„ì´ë””ì–´</h3>

<p>
MLPì˜ ë¬¸ì œëŠ” ì…ë ¥ì˜ ê³µê°„ì  êµ¬ì¡°ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ í¼ì¹˜ë©´ ì¸ì ‘ í”½ì…€ ê°„ì˜ ê´€ê³„ê°€ ì‚¬ë¼ì§„ë‹¤. CNN(Convolutional Neural Network)ì€ <strong>í•©ì„±ê³± ì—°ì‚°</strong>ì„ í†µí•´ ì…ë ¥ì˜ ê³µê°„ì /ì‹œê°„ì  êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©´ì„œ í”¼ì²˜ë¥¼ ì¶”ì¶œí•œë‹¤. 1989ë…„ Yann LeCunì´ ì†ê¸€ì”¨ ì¸ì‹ì„ ìœ„í•´ ì œì•ˆí–ˆìœ¼ë©°, 2012ë…„ AlexNetì´ ImageNet ëŒ€íšŒë¥¼ ì„ê¶Œí•˜ë©´ì„œ ë”¥ëŸ¬ë‹ í˜ëª…ì˜ ì‹œë°œì ì´ ë˜ì—ˆë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– CNNì˜ ì„¸ ê°€ì§€ í•µì‹¬ ì•„ì´ë””ì–´</strong></p>
<ol>
<li><strong>ì§€ì—­ ì—°ê²° (Local Connectivity):</strong> ê° ë‰´ëŸ°ì´ ì…ë ¥ì˜ ì‘ì€ ì˜ì—­(receptive field)ë§Œ ë³¸ë‹¤</li>
<li><strong>íŒŒë¼ë¯¸í„° ê³µìœ  (Parameter Sharing):</strong> ê°™ì€ í•„í„°(ì»¤ë„)ë¥¼ ì…ë ¥ ì „ì²´ì— ê±¸ì³ ì¬ì‚¬ìš©í•œë‹¤</li>
<li><strong>ì´ë™ ë¶ˆë³€ì„± (Translation Invariance):</strong> íŒ¨í„´ì´ ì–´ë””ì— ìˆë“  ë™ì¼í•˜ê²Œ ê°ì§€í•œë‹¤</li>
</ol>
</div>

<h3>8.2 í•©ì„±ê³± ì—°ì‚° (Convolution)</h3>

<p>
í•©ì„±ê³±ì€ ì‘ì€ í•„í„°(ì»¤ë„)ë¥¼ ì…ë ¥ ìœ„ì—ì„œ ìŠ¬ë¼ì´ë”©í•˜ë©´ì„œ ì›ì†Œë³„ ê³±ì˜ í•©ì„ ê³„ì‚°í•˜ëŠ” ì—°ì‚°ì´ë‹¤. 2D í•©ì„±ê³±ì˜ ìˆ˜ì‹:
</p>

<div class="eq">
\[
(I * K)(i, j) = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} I(i+m, j+n) \cdot K(m, n)
\]
</div>

<p>
ì—¬ê¸°ì„œ \(I\)ëŠ” ì…ë ¥, \(K\)ëŠ” ì»¤ë„(í•„í„°), \(k_h, k_w\)ëŠ” ì»¤ë„ì˜ ë†’ì´ì™€ ë„ˆë¹„ë‹¤.
</p>

<!-- í•©ì„±ê³± ì—°ì‚° ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e8eaf6,#e0f2f1);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#283593">ğŸ”² 2D í•©ì„±ê³± ì—°ì‚° ì˜ˆì‹œ (3Ã—3 ì»¤ë„, stride=1)</p>
<div style="display:flex;justify-content:center;align-items:center;gap:20px;flex-wrap:wrap;font-size:12px">
<div style="text-align:center">
<div style="font-weight:bold;margin-bottom:6px">ì…ë ¥ (4Ã—4)</div>
<div style="display:grid;grid-template-columns:repeat(4,35px);gap:2px">
<div style="background:#bbdefb;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
<div style="background:#bbdefb;padding:6px;text-align:center;border:1px solid #90caf9">2</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">3</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">0</div>
<div style="background:#bbdefb;padding:6px;text-align:center;border:1px solid #90caf9">0</div>
<div style="background:#bbdefb;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">2</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">0</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">0</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">2</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">0</div>
<div style="background:#e3f2fd;padding:6px;text-align:center;border:1px solid #90caf9">1</div>
</div>
</div>
<div style="font-size:20px">*</div>
<div style="text-align:center">
<div style="font-weight:bold;margin-bottom:6px">ì»¤ë„ (3Ã—3)</div>
<div style="display:grid;grid-template-columns:repeat(3,35px);gap:2px">
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">1</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">0</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">-1</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">1</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">0</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">-1</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">1</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">0</div>
<div style="background:#fff9c4;padding:6px;text-align:center;border:1px solid #fdd835">-1</div>
</div>
</div>
<div style="font-size:20px">=</div>
<div style="text-align:center">
<div style="font-weight:bold;margin-bottom:6px">ì¶œë ¥ (2Ã—2)</div>
<div style="display:grid;grid-template-columns:repeat(2,35px);gap:2px">
<div style="background:#c8e6c9;padding:6px;text-align:center;border:1px solid #66bb6a">-1</div>
<div style="background:#c8e6c9;padding:6px;text-align:center;border:1px solid #66bb6a">0</div>
<div style="background:#c8e6c9;padding:6px;text-align:center;border:1px solid #66bb6a">2</div>
<div style="background:#c8e6c9;padding:6px;text-align:center;border:1px solid #66bb6a">1</div>
</div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666;margin-top:10px">
ì¶œë ¥ í¬ê¸° = (ì…ë ¥ í¬ê¸° - ì»¤ë„ í¬ê¸°) / stride + 1 = (4 - 3) / 1 + 1 = 2
</p>
</div>

<h3>8.3 ì¶œë ¥ í¬ê¸° ê³„ì‚°</h3>

<div class="eq">
\[
\text{Output Size} = \left\lfloor \frac{W - K + 2P}{S} \right\rfloor + 1
\]
</div>

<p>
ì—¬ê¸°ì„œ \(W\)ëŠ” ì…ë ¥ í¬ê¸°, \(K\)ëŠ” ì»¤ë„ í¬ê¸°, \(P\)ëŠ” íŒ¨ë”©, \(S\)ëŠ” ìŠ¤íŠ¸ë¼ì´ë“œë‹¤.
</p>

<p class="cc">Python â€” í•©ì„±ê³± ì¶œë ¥ í¬ê¸° ê³„ì‚°ê¸°</p>
<pre><code><span class="kw">def</span> <span class="fn">conv_output_size</span>(W, K, P=<span class="nu">0</span>, S=<span class="nu">1</span>):
    <span class="kw">return</span> (W - K + <span class="nu">2</span>*P) // S + <span class="nu">1</span>

<span class="fn">print</span>(<span class="st">"=== í•©ì„±ê³± ì¶œë ¥ í¬ê¸° ê³„ì‚° ==="</span>)
cases = [
    (<span class="nu">32</span>, <span class="nu">3</span>, <span class="nu">0</span>, <span class="nu">1</span>, <span class="st">"32Ã—32 ì…ë ¥, 3Ã—3 ì»¤ë„, no padding"</span>),
    (<span class="nu">32</span>, <span class="nu">3</span>, <span class="nu">1</span>, <span class="nu">1</span>, <span class="st">"32Ã—32 ì…ë ¥, 3Ã—3 ì»¤ë„, padding=1 (same)"</span>),
    (<span class="nu">32</span>, <span class="nu">5</span>, <span class="nu">2</span>, <span class="nu">1</span>, <span class="st">"32Ã—32 ì…ë ¥, 5Ã—5 ì»¤ë„, padding=2 (same)"</span>),
    (<span class="nu">32</span>, <span class="nu">3</span>, <span class="nu">1</span>, <span class="nu">2</span>, <span class="st">"32Ã—32 ì…ë ¥, 3Ã—3 ì»¤ë„, stride=2"</span>),
    (<span class="nu">224</span>, <span class="nu">7</span>, <span class="nu">3</span>, <span class="nu">2</span>, <span class="st">"224Ã—224 (ImageNet), 7Ã—7 ì»¤ë„, stride=2"</span>),
]
<span class="kw">for</span> W, K, P, S, desc <span class="kw">in</span> cases:
    out = <span class="fn">conv_output_size</span>(W, K, P, S)
    <span class="fn">print</span>(<span class="st">f"  {desc} â†’ {out}Ã—{out}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== í•©ì„±ê³± ì¶œë ¥ í¬ê¸° ê³„ì‚° ===
  32Ã—32 ì…ë ¥, 3Ã—3 ì»¤ë„, no padding â†’ 30Ã—30
  32Ã—32 ì…ë ¥, 3Ã—3 ì»¤ë„, padding=1 (same) â†’ 32Ã—32
  32Ã—32 ì…ë ¥, 5Ã—5 ì»¤ë„, padding=2 (same) â†’ 32Ã—32
  32Ã—32 ì…ë ¥, 3Ã—3 ì»¤ë„, stride=2 â†’ 16Ã—16
  224Ã—224 (ImageNet), 7Ã—7 ì»¤ë„, stride=2 â†’ 112Ã—112</div>

<h3>8.4 í’€ë§ (Pooling)</h3>

<p>
í’€ë§ì€ í”¼ì²˜ ë§µì˜ ê³µê°„ì  í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë‹¤ìš´ìƒ˜í”Œë§ ì—°ì‚°ì´ë‹¤. íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë¯€ë¡œ í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤. Max Poolingì€ ì˜ì—­ ë‚´ ìµœëŒ€ê°’ì„, Average Poolingì€ í‰ê· ê°’ì„ ì·¨í•œë‹¤.
</p>

<div class="eq">
\[
\text{Max Pooling: } y_{ij} = \max_{(m,n) \in R_{ij}} x_{mn}
\]
\[
\text{Average Pooling: } y_{ij} = \frac{1}{|R_{ij}|} \sum_{(m,n) \in R_{ij}} x_{mn}
\]
</div>

<h3>8.5 CNN ì „ì²´ ì•„í‚¤í…ì²˜</h3>

<!-- CNN ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fce4ec,#e8eaf6);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#880e4f">ğŸ—ï¸ ì „í˜•ì ì¸ CNN ì•„í‚¤í…ì²˜</p>
<div style="display:flex;align-items:center;justify-content:center;gap:6px;flex-wrap:wrap;font-size:11px">
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #1565c0;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#1565c0">Input</div>
<div style="color:#888;font-size:9px">32Ã—32Ã—3</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #e65100;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#e65100">Conv</div>
<div style="color:#888;font-size:9px">3Ã—3, 32í•„í„°</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #2e7d32;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#2e7d32">ReLU</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #6a1b9a;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#6a1b9a">Pool</div>
<div style="color:#888;font-size:9px">2Ã—2</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #e65100;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#e65100">Conv</div>
<div style="color:#888;font-size:9px">3Ã—3, 64í•„í„°</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #6a1b9a;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#6a1b9a">Pool</div>
<div style="color:#888;font-size:9px">2Ã—2</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #bf360c;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#bf360c">Flatten</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #00695c;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#00695c">FC</div>
<div style="color:#888;font-size:9px">128</div>
</div>
<span>â†’</span>
<div style="background:#fff;padding:10px;border-radius:6px;border:2px solid #00695c;text-align:center;min-width:70px">
<div style="font-weight:bold;color:#00695c">Output</div>
<div style="color:#888;font-size:9px">10</div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666;margin-top:10px">
Conv â†’ ReLU â†’ Pool ë¸”ë¡ì„ ë°˜ë³µí•˜ì—¬ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê³ , Flatten â†’ FCë¡œ ë¶„ë¥˜/íšŒê·€
</p>
</div>

<h3>8.6 PyTorch CNN êµ¬í˜„</h3>

<p class="cc">Python â€” PyTorch CNN (MNIST ì†ê¸€ì”¨ ì¸ì‹)</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn

<span class="kw">class</span> <span class="nb">SimpleCNN</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        <span class="cm"># í•©ì„±ê³± ë¸”ë¡</span>
        <span class="nb">self</span>.conv_layers = nn.<span class="fn">Sequential</span>(
            <span class="cm"># Block 1: 1Ã—28Ã—28 â†’ 32Ã—14Ã—14</span>
            nn.<span class="fn">Conv2d</span>(<span class="nu">1</span>, <span class="nu">32</span>, kernel_size=<span class="nu">3</span>, padding=<span class="nu">1</span>),
            nn.<span class="fn">BatchNorm2d</span>(<span class="nu">32</span>),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">MaxPool2d</span>(<span class="nu">2</span>),
            
            <span class="cm"># Block 2: 32Ã—14Ã—14 â†’ 64Ã—7Ã—7</span>
            nn.<span class="fn">Conv2d</span>(<span class="nu">32</span>, <span class="nu">64</span>, kernel_size=<span class="nu">3</span>, padding=<span class="nu">1</span>),
            nn.<span class="fn">BatchNorm2d</span>(<span class="nu">64</span>),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">MaxPool2d</span>(<span class="nu">2</span>),
        )
        <span class="cm"># ë¶„ë¥˜ í—¤ë“œ</span>
        <span class="nb">self</span>.fc_layers = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Flatten</span>(),
            nn.<span class="fn">Linear</span>(<span class="nu">64</span> * <span class="nu">7</span> * <span class="nu">7</span>, <span class="nu">128</span>),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Dropout</span>(<span class="nu">0.5</span>),
            nn.<span class="fn">Linear</span>(<span class="nu">128</span>, <span class="nu">10</span>)
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(<span class="nb">self</span>, x):
        x = <span class="nb">self</span>.conv_layers(x)
        x = <span class="nb">self</span>.fc_layers(x)
        <span class="kw">return</span> x

model = <span class="nb">SimpleCNN</span>()
<span class="fn">print</span>(<span class="st">"=== CNN ëª¨ë¸ êµ¬ì¡° ==="</span>)
<span class="fn">print</span>(model)

<span class="cm"># íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°</span>
total = <span class="nb">sum</span>(p.<span class="fn">numel</span>() <span class="kw">for</span> p <span class="kw">in</span> model.<span class="fn">parameters</span>())
<span class="fn">print</span>(<span class="st">f"\nì´ íŒŒë¼ë¯¸í„°: {total:,}"</span>)

<span class="cm"># í…ŒìŠ¤íŠ¸ ì…ë ¥</span>
x_test = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">1</span>, <span class="nu">28</span>, <span class="nu">28</span>)  <span class="cm"># (batch, channels, H, W)</span>
output = model(x_test)
<span class="fn">print</span>(<span class="st">f"ì…ë ¥: {x_test.shape} â†’ ì¶œë ¥: {output.shape}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== CNN ëª¨ë¸ êµ¬ì¡° ===
SimpleCNN(
  (conv_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2)
    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(64)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2)
  )
  (fc_layers): Sequential(
    (0): Flatten()
    (1): Linear(in_features=3136, out_features=128, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.5)
    (4): Linear(in_features=128, out_features=10, bias=True)
  )
)

ì´ íŒŒë¼ë¯¸í„°: 421,642
ì…ë ¥: torch.Size([1, 1, 28, 28]) â†’ ì¶œë ¥: torch.Size([1, 10])</div>

<h3>8.7 ìœ ëª… CNN ì•„í‚¤í…ì²˜ ì§„í™”</h3>

<table>
<tr><th>ëª¨ë¸</th><th>ì—°ë„</th><th>ì¸µ ìˆ˜</th><th>íŒŒë¼ë¯¸í„°</th><th>í•µì‹¬ í˜ì‹ </th><th>Top-5 ì—ëŸ¬</th></tr>
<tr><td>LeNet-5</td><td>1998</td><td>5</td><td>60K</td><td>ìµœì´ˆì˜ CNN</td><td>-</td></tr>
<tr><td>AlexNet</td><td>2012</td><td>8</td><td>60M</td><td>ReLU, Dropout, GPU</td><td>16.4%</td></tr>
<tr><td>VGGNet</td><td>2014</td><td>16/19</td><td>138M</td><td>3Ã—3 ì»¤ë„ ë°˜ë³µ</td><td>7.3%</td></tr>
<tr><td>GoogLeNet</td><td>2014</td><td>22</td><td>6.8M</td><td>Inception ëª¨ë“ˆ</td><td>6.7%</td></tr>
<tr><td>ResNet</td><td>2015</td><td>152</td><td>60M</td><td>Skip Connection</td><td>3.6%</td></tr>
<tr><td>EfficientNet</td><td>2019</td><td>-</td><td>5.3M</td><td>Compound Scaling</td><td>2.9%</td></tr>
</table>

<div class="info">
<p class="ni"><strong>ğŸ’¡ ResNetì˜ Skip Connection</strong></p>
<p class="ni" style="margin-top:8px">
ResNetì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì”ì°¨ ì—°ê²°(residual connection)ì´ë‹¤: \(y = F(x) + x\). ì…ë ¥ \(x\)ë¥¼ ì¶œë ¥ì— ì§ì ‘ ë”í•´ì¤Œìœ¼ë¡œì¨, ë„¤íŠ¸ì›Œí¬ê°€ ì”ì°¨(residual) \(F(x) = y - x\)ë§Œ í•™ìŠµí•˜ë©´ ëœë‹¤. ì´ê²ƒì´ vanishing gradient ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ 100ì¸µ ì´ìƒì˜ ë§¤ìš° ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤. ê¸ˆìœµì—ì„œë„ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•  ë•Œ Skip Connectionì€ í•„ìˆ˜ì ì´ë‹¤.
</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 9: CNN ê¸ˆìœµ ì ìš©
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch9">Chapter 9. CNNì˜ ê¸ˆìœµ ì ìš© â€” ìº”ë“¤ì°¨íŠ¸ íŒ¨í„´ ì¸ì‹</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.18 "CNNs for Financial Time Series and Satellite Images" / MLDSF Ch.5 "Deep Learning for Finance"</p>
</div>

<h3>9.1 ê¸ˆìœµ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” ì´ìœ </h3>

<p>
MLAT Ch.18ì—ì„œ Stefan Jansenì€ ê¸ˆìœµ ì‹œê³„ì—´ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ CNNì— ì…ë ¥í•˜ëŠ” ì ‘ê·¼ë²•ì„ ì†Œê°œí•œë‹¤. ì™œ êµ³ì´ ìˆ«ì ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë°”ê¿€ê¹Œ? ì²«ì§¸, CNNì€ ì´ë¯¸ì§€ì—ì„œ íŒ¨í„´ì„ ì¸ì‹í•˜ëŠ” ë° íƒì›”í•˜ë‹¤. ë‘˜ì§¸, ìº”ë“¤ì°¨íŠ¸ì—ëŠ” íŠ¸ë ˆì´ë”ë“¤ì´ ìˆ˜ì‹­ ë…„ê°„ ì‚¬ìš©í•´ì˜¨ ì‹œê°ì  íŒ¨í„´(ë§ì¹˜í˜•, ë„ì§€, ì‰íƒœí˜• ë“±)ì´ ì¡´ì¬í•œë‹¤. CNNì€ ì´ëŸ° íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
</p>

<p>
ë¹„ìœ í•˜ìë©´, ì „í†µ MLì€ "ìˆ«ìë¡œ ëœ ì•…ë³´"ë¥¼ ì½ëŠ” ê²ƒì´ê³ , CNNì€ "íŒŒí˜• ì´ë¯¸ì§€"ë¥¼ ë³´ëŠ” ê²ƒì´ë‹¤. ì•…ë³´ë¥¼ ì½ìœ¼ë ¤ë©´ ìŒì•… ì´ë¡ (= í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§)ì„ ì•Œì•„ì•¼ í•˜ì§€ë§Œ, íŒŒí˜• ì´ë¯¸ì§€ì—ì„œëŠ” ì‹œê°ì  íŒ¨í„´ì„ ì§ì ‘ ì¸ì‹í•  ìˆ˜ ìˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, RSIë‚˜ MACD ê°™ì€ ê¸°ìˆ ì  ì§€í‘œë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šì•„ë„, CNNì€ ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ì—ì„œ ë™ë“±í•œ(ë˜ëŠ” ë” ë³µì¡í•œ) íŒ¨í„´ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤.
</p>

<p>
ì‹¤ì œë¡œ J.P. Morganì˜ 2019ë…„ ì—°êµ¬ "Deep Learning for Trading with Candlestick Charts"ì—ì„œëŠ” CNNì´ ì „í†µì  ìº”ë“¤ìŠ¤í‹± íŒ¨í„´ ì¸ì‹ë³´ë‹¤ ë†’ì€ ìˆ˜ìµë¥ ì„ ë‹¬ì„±í–ˆë‹¤. í•µì‹¬ì€ CNNì´ ì‚¬ëŒì´ ì •ì˜í•˜ì§€ ëª»í•œ ë¯¸ë¬˜í•œ ì‹œê°ì  íŒ¨í„´ê¹Œì§€ í¬ì°©í•œë‹¤ëŠ” ì ì´ë‹¤.
</p>

<!-- ìº”ë“¤ìŠ¤í‹± íŒ¨í„´ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff8e1,#fce4ec);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#bf360c">ğŸ•¯ï¸ ì£¼ìš” ìº”ë“¤ìŠ¤í‹± íŒ¨í„´ â€” CNNì´ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹œê°ì  ì‹ í˜¸</p>
<div style="display:flex;gap:16px;flex-wrap:wrap;justify-content:center;font-size:11px">
<!-- ë§ì¹˜í˜• -->
<div style="background:#fff;padding:14px;border-radius:8px;border:2px solid #4caf50;text-align:center;min-width:100px">
<div style="font-weight:bold;color:#2e7d32;margin-bottom:8px">ë§ì¹˜í˜• (Hammer)</div>
<div style="display:flex;flex-direction:column;align-items:center;gap:2px">
<div style="width:16px;height:12px;background:#4caf50;border-radius:2px"></div>
<div style="width:2px;height:30px;background:#4caf50"></div>
</div>
<div style="color:#888;font-size:10px;margin-top:6px">í•˜ë½ í›„ ë°˜ì „ ì‹ í˜¸<br>ê¸´ ì•„ë˜ê¼¬ë¦¬ + ì§§ì€ ëª¸í†µ</div>
</div>
<!-- ë„ì§€ -->
<div style="background:#fff;padding:14px;border-radius:8px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-weight:bold;color:#e65100;margin-bottom:8px">ë„ì§€ (Doji)</div>
<div style="display:flex;flex-direction:column;align-items:center;gap:2px">
<div style="width:2px;height:15px;background:#ff9800"></div>
<div style="width:16px;height:2px;background:#ff9800;border-radius:1px"></div>
<div style="width:2px;height:15px;background:#ff9800"></div>
</div>
<div style="color:#888;font-size:10px;margin-top:6px">ì‹œì¥ ë¶ˆí™•ì‹¤ì„±<br>ì‹œê°€ â‰ˆ ì¢…ê°€</div>
</div>
<!-- ì‰íƒœí˜• -->
<div style="background:#fff;padding:14px;border-radius:8px;border:2px solid #f44336;text-align:center;min-width:100px">
<div style="font-weight:bold;color:#c62828;margin-bottom:8px">ì‰íƒœí˜• (Engulfing)</div>
<div style="display:flex;align-items:flex-end;justify-content:center;gap:4px">
<div style="width:12px;height:20px;background:#f44336;border-radius:2px"></div>
<div style="width:18px;height:32px;background:#4caf50;border-radius:2px"></div>
</div>
<div style="color:#888;font-size:10px;margin-top:6px">ê°•í•œ ë°˜ì „ ì‹ í˜¸<br>í° ì–‘ë´‰ì´ ìŒë´‰ì„ ê°ìŒˆ</div>
</div>
<!-- ì‚¼ë³‘ -->
<div style="background:#fff;padding:14px;border-radius:8px;border:2px solid #4caf50;text-align:center;min-width:100px">
<div style="font-weight:bold;color:#2e7d32;margin-bottom:8px">ì ì‚¼ë³‘ (3 Soldiers)</div>
<div style="display:flex;align-items:flex-end;justify-content:center;gap:3px">
<div style="width:10px;height:18px;background:#4caf50;border-radius:2px"></div>
<div style="width:10px;height:24px;background:#4caf50;border-radius:2px"></div>
<div style="width:10px;height:30px;background:#4caf50;border-radius:2px"></div>
</div>
<div style="color:#888;font-size:10px;margin-top:6px">ê°•í•œ ìƒìŠ¹ ì¶”ì„¸<br>ì—°ì† 3ê°œ ì–‘ë´‰ ìƒìŠ¹</div>
</div>
<!-- í—¤ë“œì•¤ìˆ„ë” -->
<div style="background:#fff;padding:14px;border-radius:8px;border:2px solid #9c27b0;text-align:center;min-width:100px">
<div style="font-weight:bold;color:#6a1b9a;margin-bottom:8px">í—¤ë“œì•¤ìˆ„ë”</div>
<div style="display:flex;align-items:flex-end;justify-content:center;gap:3px">
<div style="width:10px;height:20px;background:#9c27b0;border-radius:2px"></div>
<div style="width:10px;height:32px;background:#9c27b0;border-radius:2px"></div>
<div style="width:10px;height:20px;background:#9c27b0;border-radius:2px"></div>
</div>
<div style="color:#888;font-size:10px;margin-top:6px">ì¶”ì„¸ ë°˜ì „ íŒ¨í„´<br>ì¢Œì–´ê¹¨-ë¨¸ë¦¬-ìš°ì–´ê¹¨</div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666;margin-top:12px">
ì´ëŸ° íŒ¨í„´ì„ ì‚¬ëŒì´ ê·œì¹™ìœ¼ë¡œ ì½”ë”©í•˜ë©´ ìˆ˜ë°± ì¤„ì´ í•„ìš”í•˜ì§€ë§Œ, CNNì€ ì´ë¯¸ì§€ì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµí•œë‹¤
</p>
</div>

<h3>9.2 1D CNN for ì‹œê³„ì—´</h3>

<p>
ì´ë¯¸ì§€ ë³€í™˜ ì—†ì´ë„ 1D CNNì„ ì§ì ‘ ì‹œê³„ì—´ ë°ì´í„°ì— ì ìš©í•  ìˆ˜ ìˆë‹¤. 1D í•©ì„±ê³±ì€ ì‹œê°„ ì¶•ì„ ë”°ë¼ ìŠ¬ë¼ì´ë”©í•˜ë©° ì‹œê°„ì  íŒ¨í„´ì„ ì¶”ì¶œí•œë‹¤. ì´ ë°©ë²•ì´ ë” ì§ê´€ì ì´ê³  íš¨ìœ¨ì ì¸ ê²½ìš°ê°€ ë§ë‹¤.
</p>

<p class="cc">Python â€” 1D CNNìœ¼ë¡œ ì£¼ê°€ ë°©í–¥ ì˜ˆì¸¡</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">class</span> <span class="nb">StockCNN1D</span>(nn.Module):
    <span class="st">"""1D CNN for ì£¼ê°€ ì‹œê³„ì—´ ë¶„ë¥˜"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>, n_features=<span class="nu">5</span>, seq_len=<span class="nu">20</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        <span class="nb">self</span>.conv_block = nn.<span class="fn">Sequential</span>(
            <span class="cm"># Block 1: (batch, 5, 20) â†’ (batch, 32, 18)</span>
            nn.<span class="fn">Conv1d</span>(n_features, <span class="nu">32</span>, kernel_size=<span class="nu">3</span>),
            nn.<span class="fn">BatchNorm1d</span>(<span class="nu">32</span>),
            nn.<span class="fn">ReLU</span>(),
            
            <span class="cm"># Block 2: (batch, 32, 18) â†’ (batch, 64, 16)</span>
            nn.<span class="fn">Conv1d</span>(<span class="nu">32</span>, <span class="nu">64</span>, kernel_size=<span class="nu">3</span>),
            nn.<span class="fn">BatchNorm1d</span>(<span class="nu">64</span>),
            nn.<span class="fn">ReLU</span>(),
            
            <span class="cm"># Global Average Pooling</span>
            nn.<span class="fn">AdaptiveAvgPool1d</span>(<span class="nu">1</span>)
        )
        <span class="nb">self</span>.classifier = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Flatten</span>(),
            nn.<span class="fn">Linear</span>(<span class="nu">64</span>, <span class="nu">32</span>),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Dropout</span>(<span class="nu">0.3</span>),
            nn.<span class="fn">Linear</span>(<span class="nu">32</span>, <span class="nu">1</span>),
            nn.<span class="fn">Sigmoid</span>()
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(<span class="nb">self</span>, x):
        <span class="cm"># x shape: (batch, features, seq_len)</span>
        x = <span class="nb">self</span>.conv_block(x)
        x = <span class="nb">self</span>.classifier(x)
        <span class="kw">return</span> x

<span class="cm"># ê°€ìƒ OHLCV ë°ì´í„° ìƒì„±</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
N = <span class="nu">1000</span>
seq_len = <span class="nu">20</span>  <span class="cm"># 20ì¼ ìœˆë„ìš°</span>
n_features = <span class="nu">5</span>  <span class="cm"># Open, High, Low, Close, Volume</span>

X = np.random.<span class="fn">randn</span>(N, n_features, seq_len).<span class="fn">astype</span>(np.float32)
y = (np.random.<span class="fn">randn</span>(N) > <span class="nu">0</span>).<span class="fn">astype</span>(np.float32)

X_tensor = torch.<span class="fn">FloatTensor</span>(X)
y_tensor = torch.<span class="fn">FloatTensor</span>(y)

model = <span class="nb">StockCNN1D</span>(n_features=<span class="nu">5</span>, seq_len=<span class="nu">20</span>)
<span class="fn">print</span>(<span class="st">"=== 1D CNN ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ ==="</span>)
<span class="fn">print</span>(model)
<span class="fn">print</span>(<span class="st">f"\níŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}"</span>)

<span class="cm"># í…ŒìŠ¤íŠ¸ ìˆœì „íŒŒ</span>
output = model(X_tensor[:<span class="nu">5</span>])
<span class="fn">print</span>(<span class="st">f"ì…ë ¥: {X_tensor[:5].shape} â†’ ì¶œë ¥: {output.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ì˜ˆì¸¡ í™•ë¥ : {output.detach().squeeze().numpy()}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== 1D CNN ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ ===
StockCNN1D(
  (conv_block): Sequential(
    (0): Conv1d(5, 32, kernel_size=(3,), stride=(1,))
    (1): BatchNorm1d(32)
    (2): ReLU()
    (3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))
    (4): BatchNorm1d(64)
    (5): ReLU()
    (6): AdaptiveAvgPool1d(output_size=1)
  )
  (classifier): Sequential(
    (0): Flatten()
    (1): Linear(in_features=64, out_features=32, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.3)
    (4): Linear(in_features=32, out_features=1, bias=True)
    (5): Sigmoid()
  )
)

íŒŒë¼ë¯¸í„° ìˆ˜: 9,825
ì…ë ¥: torch.Size([5, 5, 20]) â†’ ì¶œë ¥: torch.Size([5, 1])
ì˜ˆì¸¡ í™•ë¥ : [0.4823 0.5134 0.4967 0.5201 0.4889]</div>

<h3>9.3 ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ CNN</h3>

<p>
MLAT Ch.18ì˜ í•µì‹¬ ì•„ì´ë””ì–´ ì¤‘ í•˜ë‚˜ëŠ” OHLCV ë°ì´í„°ë¥¼ ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ 2D CNNì— ì…ë ¥í•˜ëŠ” ê²ƒì´ë‹¤. ì´ ì ‘ê·¼ë²•ì€ íŠ¸ë ˆì´ë”ë“¤ì´ ì‹œê°ì ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” íŒ¨í„´(í—¤ë“œì•¤ìˆ„ë”, ë”ë¸”íƒ‘, ì‚¼ê°ìˆ˜ë ´ ë“±)ì„ CNNì´ ìë™ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•œë‹¤.
</p>

<p class="cc">Python â€” ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± + CNN</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt
<span class="kw">from</span> matplotlib.patches <span class="kw">import</span> Rectangle

<span class="kw">def</span> <span class="fn">generate_candlestick_image</span>(ohlc, img_size=<span class="nu">64</span>):
    <span class="st">"""OHLC ë°ì´í„°ë¥¼ ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""</span>
    fig, ax = plt.<span class="fn">subplots</span>(figsize=(<span class="nu">1</span>, <span class="nu">1</span>), dpi=img_size)
    ax.<span class="fn">set_xlim</span>(-<span class="nu">0.5</span>, <span class="nb">len</span>(ohlc)-<span class="nu">0.5</span>)
    
    <span class="kw">for</span> i, (o, h, l, c) <span class="kw">in</span> <span class="nb">enumerate</span>(ohlc):
        color = <span class="st">'green'</span> <span class="kw">if</span> c >= o <span class="kw">else</span> <span class="st">'red'</span>
        <span class="cm"># ê¼¬ë¦¬ (High-Low)</span>
        ax.<span class="fn">plot</span>([i, i], [l, h], color=color, linewidth=<span class="nu">0.8</span>)
        <span class="cm"># ëª¸í†µ (Open-Close)</span>
        body_bottom = <span class="nb">min</span>(o, c)
        body_height = <span class="nb">abs</span>(c - o)
        rect = <span class="fn">Rectangle</span>((i-<span class="nu">0.3</span>, body_bottom), <span class="nu">0.6</span>, body_height,
                          facecolor=color, edgecolor=color)
        ax.<span class="fn">add_patch</span>(rect)
    
    ax.<span class="fn">axis</span>(<span class="st">'off'</span>)
    fig.canvas.<span class="fn">draw</span>()
    img = np.<span class="fn">frombuffer</span>(fig.canvas.<span class="fn">tostring_rgb</span>(), dtype=np.uint8)
    img = img.<span class="fn">reshape</span>(fig.canvas.<span class="fn">get_width_height</span>()[::-<span class="nu">1</span>] + (<span class="nu">3</span>,))
    plt.<span class="fn">close</span>(fig)
    <span class="kw">return</span> img

<span class="cm"># ê°€ìƒ OHLC ë°ì´í„° (20ì¼)</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
prices = <span class="nu">100</span> + np.random.<span class="fn">randn</span>(<span class="nu">20</span>).<span class="fn">cumsum</span>()
ohlc = []
<span class="kw">for</span> p <span class="kw">in</span> prices:
    o = p + np.random.<span class="fn">randn</span>() * <span class="nu">0.5</span>
    c = p + np.random.<span class="fn">randn</span>() * <span class="nu">0.5</span>
    h = <span class="nb">max</span>(o, c) + <span class="nb">abs</span>(np.random.<span class="fn">randn</span>()) * <span class="nu">0.3</span>
    l = <span class="nb">min</span>(o, c) - <span class="nb">abs</span>(np.random.<span class="fn">randn</span>()) * <span class="nu">0.3</span>
    ohlc.<span class="fn">append</span>((o, h, l, c))

img = <span class="fn">generate_candlestick_image</span>(ohlc)
<span class="fn">print</span>(<span class="st">f"ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ shape: {img.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ì´ë¯¸ì§€ë¥¼ CNN ì…ë ¥ìœ¼ë¡œ ë³€í™˜: (1, 3, {img.shape[0]}, {img.shape[1]})"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ shape: (64, 64, 3)
ì´ë¯¸ì§€ë¥¼ CNN ì…ë ¥ìœ¼ë¡œ ë³€í™˜: (1, 3, 64, 64)</div>

<h3>9.4 1D CNN vs 2D CNN ë¹„êµ</h3>

<table>
<tr><th>ë°©ë²•</th><th>ì…ë ¥ í˜•íƒœ</th><th>ì¥ì </th><th>ë‹¨ì </th><th>ì í•©í•œ ê²½ìš°</th></tr>
<tr><td>1D CNN</td><td>(batch, features, time)</td><td>ë¹ ë¦„, ì§ê´€ì </td><td>ì‹œê°ì  íŒ¨í„´ ë¯¸í¬ì°©</td><td>ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´, ì‹¤ì‹œê°„</td></tr>
<tr><td>2D CNN (ì´ë¯¸ì§€)</td><td>(batch, channels, H, W)</td><td>ì‹œê°ì  íŒ¨í„´ í•™ìŠµ</td><td>ì´ë¯¸ì§€ ë³€í™˜ ë¹„ìš©</td><td>ìº”ë“¤ì°¨íŠ¸ íŒ¨í„´ ì¸ì‹</td></tr>
<tr><td>2D CNN (GAF/MTF)</td><td>(batch, 1, T, T)</td><td>ì‹œê³„ì—´â†’ì´ë¯¸ì§€ ì²´ê³„ì </td><td>ì •ë³´ ì†ì‹¤ ê°€ëŠ¥</td><td>ì—°êµ¬/ì‹¤í—˜</td></tr>
</table>

<div class="info">
<p class="ni"><strong>ğŸ’¡ GAF (Gramian Angular Field)</strong></p>
<p class="ni" style="margin-top:8px">
ì‹œê³„ì—´ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” ì²´ê³„ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜ê°€ GAFë‹¤. ì‹œê³„ì—´ ê°’ì„ ê·¹ì¢Œí‘œë¡œ ë³€í™˜í•œ ë’¤, ê° ì‹œì  ìŒì˜ ê°ë„ í•©(GASF) ë˜ëŠ” ì°¨(GADF)ë¥¼ ê³„ì‚°í•˜ì—¬ 2D í–‰ë ¬ì„ ë§Œë“ ë‹¤. ì´ í–‰ë ¬ì„ ì´ë¯¸ì§€ë¡œ ì·¨ê¸‰í•˜ì—¬ CNNì— ì…ë ¥í•œë‹¤. <code>pyts</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ <code>GramianAngularField</code>ë¡œ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 10: RNN ê¸°ì´ˆ
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch10">Chapter 10. RNN ê¸°ì´ˆ â€” ìˆœí™˜ ì‹ ê²½ë§</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.19 "RNNs for Multivariate Time Series" / MLDSF Ch.6 "Recurrent Networks for Sequential Data"</p>
</div>

<h3>10.1 ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ íŠ¹ì„±</h3>

<p>
MLPì™€ CNNì€ ì…ë ¥ ê°„ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ ê¸ˆìœµ ì‹œê³„ì—´ì€ ë³¸ì§ˆì ìœ¼ë¡œ ìˆœì„œê°€ ì¤‘ìš”í•œ ì‹œí€€ìŠ¤ ë°ì´í„°ë‹¤. ì–´ì œì˜ ì£¼ê°€ê°€ ì˜¤ëŠ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³ , ì˜¤ëŠ˜ì˜ ì£¼ê°€ê°€ ë‚´ì¼ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ì´ëŸ° ì‹œê°„ì  ì˜ì¡´ì„±(temporal dependency)ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ê²ƒì´ ìˆœí™˜ ì‹ ê²½ë§(Recurrent Neural Network, RNN)ì´ë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– RNNì˜ í•µì‹¬ ì•„ì´ë””ì–´</strong></p>
<p class="ni" style="margin-top:8px">
RNNì€ <strong>ì€ë‹‰ ìƒíƒœ</strong>(hidden state) \(h_t\)ë¥¼ í†µí•´ ê³¼ê±° ì •ë³´ë¥¼ ê¸°ì–µí•œë‹¤. ê° ì‹œì  \(t\)ì—ì„œ í˜„ì¬ ì…ë ¥ \(x_t\)ì™€ ì´ì „ ì€ë‹‰ ìƒíƒœ \(h_{t-1}\)ì„ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœë¥¼ ë§Œë“ ë‹¤. ì´ ì€ë‹‰ ìƒíƒœê°€ ì‹œí€€ìŠ¤ì˜ "ë©”ëª¨ë¦¬" ì—­í• ì„ í•œë‹¤.
</p>
</div>

<h3>10.2 RNNì˜ ìˆ˜í•™ì  ì •ì˜</h3>

<div class="eq">
\[
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
\]
\[
y_t = W_{hy} h_t + b_y
\]
</div>

<p>
ì—¬ê¸°ì„œ \(W_{hh}\)ëŠ” ì€ë‹‰â†’ì€ë‹‰ ê°€ì¤‘ì¹˜, \(W_{xh}\)ëŠ” ì…ë ¥â†’ì€ë‹‰ ê°€ì¤‘ì¹˜, \(W_{hy}\)ëŠ” ì€ë‹‰â†’ì¶œë ¥ ê°€ì¤‘ì¹˜ë‹¤. í•µì‹¬ì€ \(W_{hh}, W_{xh}, W_{hy}\)ê°€ ëª¨ë“  ì‹œì ì—ì„œ ê³µìœ ëœë‹¤ëŠ” ê²ƒì´ë‹¤ (íŒŒë¼ë¯¸í„° ê³µìœ ). ì´ê²ƒì´ RNNì´ ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ìœ ë‹¤.
</p>

<!-- RNN í¼ì¹¨ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e0f7fa,#e8f5e9);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#00695c">ğŸ”„ RNN ì‹œê°„ í¼ì¹¨ (Unrolling)</p>
<div style="display:flex;align-items:center;justify-content:center;gap:15px;flex-wrap:wrap;font-size:12px">
<div style="text-align:center">
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #00897b;margin-bottom:4px;font-size:11px">y<sub>t-2</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="width:50px;height:50px;border-radius:50%;background:#b2dfdb;border:3px solid #00897b;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:11px">h<sub>t-2</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #1565c0;font-size:11px">x<sub>t-2</sub></div>
</div>
<div style="font-size:16px;color:#00897b">â†’</div>
<div style="text-align:center">
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #00897b;margin-bottom:4px;font-size:11px">y<sub>t-1</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="width:50px;height:50px;border-radius:50%;background:#b2dfdb;border:3px solid #00897b;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:11px">h<sub>t-1</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #1565c0;font-size:11px">x<sub>t-1</sub></div>
</div>
<div style="font-size:16px;color:#00897b">â†’</div>
<div style="text-align:center">
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #00897b;margin-bottom:4px;font-size:11px">y<sub>t</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="width:50px;height:50px;border-radius:50%;background:#80cbc4;border:3px solid #00695c;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:11px">h<sub>t</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #1565c0;font-size:11px">x<sub>t</sub></div>
</div>
<div style="font-size:16px;color:#00897b">â†’</div>
<div style="text-align:center">
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #00897b;margin-bottom:4px;font-size:11px">y<sub>t+1</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="width:50px;height:50px;border-radius:50%;background:#b2dfdb;border:3px solid #00897b;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:11px">h<sub>t+1</sub></div>
<div style="font-size:10px;color:#888">â†‘</div>
<div style="background:#fff;padding:8px;border-radius:6px;border:2px solid #1565c0;font-size:11px">x<sub>t+1</sub></div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666;margin-top:10px">
h<sub>t</sub> = tanh(W<sub>hh</sub>Â·h<sub>t-1</sub> + W<sub>xh</sub>Â·x<sub>t</sub> + b) â€” ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë“  ì‹œì ì—ì„œ ê³µìœ 
</p>
</div>

<h3>10.3 RNNì˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ìœ í˜•</h3>

<table>
<tr><th>ìœ í˜•</th><th>ì…ë ¥â†’ì¶œë ¥</th><th>ì˜ˆì‹œ</th><th>ê¸ˆìœµ ì ìš©</th></tr>
<tr><td>Many-to-One</td><td>ì‹œí€€ìŠ¤ â†’ ë‹¨ì¼ê°’</td><td>ê°ì„± ë¶„ë¥˜</td><td>20ì¼ ì‹œê³„ì—´ â†’ ìƒìŠ¹/í•˜ë½</td></tr>
<tr><td>Many-to-Many</td><td>ì‹œí€€ìŠ¤ â†’ ì‹œí€€ìŠ¤</td><td>ê¸°ê³„ ë²ˆì—­</td><td>ì‹œê³„ì—´ â†’ ë‹¤ë‹¨ê³„ ì˜ˆì¸¡</td></tr>
<tr><td>One-to-Many</td><td>ë‹¨ì¼ê°’ â†’ ì‹œí€€ìŠ¤</td><td>ì´ë¯¸ì§€ ìº¡ì…”ë‹</td><td>ì´ˆê¸° ì¡°ê±´ â†’ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±</td></tr>
</table>

<h3>10.4 Vanishing/Exploding Gradient in RNN</h3>

<p>
RNNì˜ ì¹˜ëª…ì  ë¬¸ì œëŠ” ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ê±°ë‚˜ í­ë°œí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì—­ì „íŒŒ ì‹œ \(W_{hh}\)ê°€ ë°˜ë³µì ìœ¼ë¡œ ê³±í•´ì§€ê¸° ë•Œë¬¸ì´ë‹¤:
</p>

<div class="eq">
\[
\frac{\partial h_t}{\partial h_1} = \prod_{k=2}^{t} \frac{\partial h_k}{\partial h_{k-1}} = \prod_{k=2}^{t} W_{hh}^T \cdot \text{diag}(\tanh'(z_k))
\]
</div>

<p>
\(W_{hh}\)ì˜ ìµœëŒ€ ê³ ìœ ê°’ì´ 1ë³´ë‹¤ í¬ë©´ ê¸°ìš¸ê¸°ê°€ í­ë°œí•˜ê³ , 1ë³´ë‹¤ ì‘ìœ¼ë©´ ì†Œì‹¤ëœë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 100ì´ë©´ ì´ ê³±ì´ 100ë²ˆ ë°˜ë³µëœë‹¤. ì´ê²ƒì´ ê¸°ë³¸ RNNì´ ì¥ê¸° ì˜ì¡´ì„±(long-term dependency)ì„ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê·¼ë³¸ì  ì´ìœ ì´ë©°, LSTMê³¼ GRUê°€ íƒ„ìƒí•œ ë°°ê²½ì´ë‹¤.
</p>

<p class="cc">Python â€” ê¸°ë³¸ RNN êµ¬í˜„ (PyTorch)</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn

<span class="cm"># ê¸°ë³¸ RNN</span>
rnn = nn.<span class="fn">RNN</span>(
    input_size=<span class="nu">5</span>,     <span class="cm"># ì…ë ¥ í”¼ì²˜ ìˆ˜ (OHLCV)</span>
    hidden_size=<span class="nu">32</span>,   <span class="cm"># ì€ë‹‰ ìƒíƒœ í¬ê¸°</span>
    num_layers=<span class="nu">2</span>,     <span class="cm"># RNN ì¸µ ìˆ˜</span>
    batch_first=<span class="kw">True</span>, <span class="cm"># (batch, seq, features) í˜•íƒœ</span>
    dropout=<span class="nu">0.2</span>
)

<span class="cm"># ì…ë ¥: (batch=8, seq_len=20, features=5)</span>
x = torch.<span class="fn">randn</span>(<span class="nu">8</span>, <span class="nu">20</span>, <span class="nu">5</span>)
h0 = torch.<span class="fn">zeros</span>(<span class="nu">2</span>, <span class="nu">8</span>, <span class="nu">32</span>)  <span class="cm"># ì´ˆê¸° ì€ë‹‰ ìƒíƒœ</span>

output, h_n = rnn(x, h0)

<span class="fn">print</span>(<span class="st">"=== ê¸°ë³¸ RNN ==="</span>)
<span class="fn">print</span>(<span class="st">f"ì…ë ¥: {x.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ì¶œë ¥ (ëª¨ë“  ì‹œì ): {output.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ìµœì¢… ì€ë‹‰ ìƒíƒœ: {h_n.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in rnn.parameters()):,}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ê¸°ë³¸ RNN ===
ì…ë ¥: torch.Size([8, 20, 5])
ì¶œë ¥ (ëª¨ë“  ì‹œì ): torch.Size([8, 20, 32])
ìµœì¢… ì€ë‹‰ ìƒíƒœ: torch.Size([2, 8, 32])
íŒŒë¼ë¯¸í„° ìˆ˜: 4,576</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 11: LSTM / GRU
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch11">Chapter 11. LSTM &amp; GRU â€” ì¥ê¸° ê¸°ì–µì˜ ë¹„ë°€</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.19 "LSTM and GRU" / MLDSF Ch.6 "Gated Architectures"</p>
</div>

<h3>11.1 LSTM (Long Short-Term Memory)</h3>

<p>
1997ë…„ Hochreiterì™€ Schmidhuberê°€ ì œì•ˆí•œ LSTMì€ RNNì˜ vanishing gradient ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong>ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜</strong>ì„ ë„ì…í–ˆë‹¤. LSTMì˜ í•µì‹¬ì€ ì…€ ìƒíƒœ(cell state) \(c_t\)ë¼ëŠ” ë³„ë„ì˜ ë©”ëª¨ë¦¬ ê²½ë¡œë¥¼ ë‘ì–´, ì •ë³´ê°€ ë³€í˜• ì—†ì´ ì¥ê¸°ê°„ íë¥¼ ìˆ˜ ìˆê²Œ í•œ ê²ƒì´ë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– LSTMì˜ ì„¸ ê°€ì§€ ê²Œì´íŠ¸</strong></p>
<ol>
<li><strong>ë§ê° ê²Œì´íŠ¸ (Forget Gate):</strong> ì…€ ìƒíƒœì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ë²„ë¦´ì§€ ê²°ì •</li>
<li><strong>ì…ë ¥ ê²Œì´íŠ¸ (Input Gate):</strong> ìƒˆë¡œìš´ ì •ë³´ ì¤‘ ì–´ë–¤ ê²ƒì„ ì…€ ìƒíƒœì— ì €ì¥í• ì§€ ê²°ì •</li>
<li><strong>ì¶œë ¥ ê²Œì´íŠ¸ (Output Gate):</strong> ì…€ ìƒíƒœì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì€ë‹‰ ìƒíƒœë¡œ ì¶œë ¥í• ì§€ ê²°ì •</li>
</ol>
</div>

<h3>11.2 LSTM ìˆ˜ì‹</h3>

<div class="eq">
\[
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f) \quad \text{(Forget Gate)}
\]
\[
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i) \quad \text{(Input Gate)}
\]
\[
\tilde{c}_t = \tanh(W_c [h_{t-1}, x_t] + b_c) \quad \text{(Candidate Cell)}
\]
\[
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \quad \text{(Cell State Update)}
\]
\[
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o) \quad \text{(Output Gate)}
\]
\[
h_t = o_t \odot \tanh(c_t) \quad \text{(Hidden State)}
\]
</div>

<p>
ì—¬ê¸°ì„œ \(\sigma\)ëŠ” sigmoid í•¨ìˆ˜ (0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ê²Œì´íŠ¸ ì—­í• ), \(\odot\)ëŠ” ì›ì†Œë³„ ê³±(Hadamard product)ì´ë‹¤. í•µì‹¬ì€ ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸ ìˆ˜ì‹ \(c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\)ë‹¤. ì´ ìˆ˜ì‹ì€ ë§ì…ˆ êµ¬ì¡°ì´ë¯€ë¡œ ê¸°ìš¸ê¸°ê°€ ê³±ì…ˆìœ¼ë¡œ ì†Œì‹¤ë˜ì§€ ì•Šê³  ì§ì ‘ íë¥¼ ìˆ˜ ìˆë‹¤ â€” ì´ê²ƒì´ LSTMì´ ì¥ê¸° ì˜ì¡´ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìˆ˜í•™ì  ì´ìœ ë‹¤.
</p>

<!-- LSTM ê²Œì´íŠ¸ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff3e0,#fce4ec);border-radius:10px">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#bf360c">ğŸ” LSTM ì…€ ë‚´ë¶€ êµ¬ì¡°</p>
<div style="display:flex;justify-content:center;gap:12px;flex-wrap:wrap;font-size:11px;max-width:700px;margin:0 auto">
<div style="flex:1;min-width:150px;background:#fff;padding:12px;border-radius:8px;border:2px solid #e65100;text-align:center">
<div style="font-size:18px;margin-bottom:4px">ğŸšª</div>
<div style="font-weight:bold;color:#e65100">Forget Gate</div>
<div style="color:#888;font-size:10px;margin-top:4px">f<sub>t</sub> = Ïƒ(W<sub>f</sub>[h<sub>t-1</sub>, x<sub>t</sub>])</div>
<div style="color:#555;font-size:10px;margin-top:4px">"ê³¼ê±° ì •ë³´ ì¤‘ ë­˜ ë²„ë¦´ê¹Œ?"</div>
<div style="color:#888;font-size:10px">ì¶œë ¥: 0(ì „ë¶€ ì‚­ì œ)~1(ì „ë¶€ ìœ ì§€)</div>
</div>
<div style="flex:1;min-width:150px;background:#fff;padding:12px;border-radius:8px;border:2px solid #2e7d32;text-align:center">
<div style="font-size:18px;margin-bottom:4px">ğŸ“¥</div>
<div style="font-weight:bold;color:#2e7d32">Input Gate</div>
<div style="color:#888;font-size:10px;margin-top:4px">i<sub>t</sub> = Ïƒ(W<sub>i</sub>[h<sub>t-1</sub>, x<sub>t</sub>])</div>
<div style="color:#555;font-size:10px;margin-top:4px">"ìƒˆ ì •ë³´ ì¤‘ ë­˜ ì €ì¥í• ê¹Œ?"</div>
<div style="color:#888;font-size:10px">cÌƒ<sub>t</sub> = tanh(W<sub>c</sub>[h<sub>t-1</sub>, x<sub>t</sub>])</div>
</div>
<div style="flex:1;min-width:150px;background:#fff;padding:12px;border-radius:8px;border:2px solid #1565c0;text-align:center">
<div style="font-size:18px;margin-bottom:4px">ğŸ“¤</div>
<div style="font-weight:bold;color:#1565c0">Output Gate</div>
<div style="color:#888;font-size:10px;margin-top:4px">o<sub>t</sub> = Ïƒ(W<sub>o</sub>[h<sub>t-1</sub>, x<sub>t</sub>])</div>
<div style="color:#555;font-size:10px;margin-top:4px">"ì…€ ìƒíƒœì—ì„œ ë­˜ ì¶œë ¥í• ê¹Œ?"</div>
<div style="color:#888;font-size:10px">h<sub>t</sub> = o<sub>t</sub> âŠ™ tanh(c<sub>t</sub>)</div>
</div>
</div>
<div style="text-align:center;margin-top:12px;padding:10px;background:#fff;border-radius:8px;border:2px solid #6a1b9a;max-width:400px;margin-left:auto;margin-right:auto">
<div style="font-weight:bold;color:#6a1b9a;font-size:12px">ğŸ§  Cell State (ì¥ê¸° ë©”ëª¨ë¦¬)</div>
<div style="color:#888;font-size:11px;margin-top:4px">c<sub>t</sub> = f<sub>t</sub> âŠ™ c<sub>t-1</sub> + i<sub>t</sub> âŠ™ cÌƒ<sub>t</sub></div>
<div style="color:#555;font-size:10px;margin-top:4px">ë§ì…ˆ êµ¬ì¡° â†’ ê¸°ìš¸ê¸°ê°€ ì§ì ‘ íë¦„ â†’ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥</div>
</div>
</div>

<h3>11.3 LSTM íŒŒë¼ë¯¸í„° ìˆ˜</h3>

<p>
LSTMì€ 4ê°œì˜ ê²Œì´íŠ¸(forget, input, cell candidate, output)ë¥¼ ê°€ì§€ë¯€ë¡œ, ê¸°ë³¸ RNN ëŒ€ë¹„ íŒŒë¼ë¯¸í„°ê°€ 4ë°°ë‹¤:
</p>

<div class="eq">
\[
\text{LSTM íŒŒë¼ë¯¸í„°} = 4 \times [(n_{\text{input}} + n_{\text{hidden}}) \times n_{\text{hidden}} + n_{\text{hidden}}]
\]
\[
= 4 \times n_{\text{hidden}} \times (n_{\text{input}} + n_{\text{hidden}} + 1)
\]
</div>

<p class="cc">Python â€” LSTM íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ</p>
<pre><code><span class="kw">import</span> torch.nn <span class="kw">as</span> nn

configs = [
    (<span class="nu">5</span>, <span class="nu">32</span>, <span class="st">"OHLCV â†’ 32 hidden"</span>),
    (<span class="nu">5</span>, <span class="nu">64</span>, <span class="st">"OHLCV â†’ 64 hidden"</span>),
    (<span class="nu">20</span>, <span class="nu">64</span>, <span class="st">"20 í”¼ì²˜ â†’ 64 hidden"</span>),
    (<span class="nu">50</span>, <span class="nu">128</span>, <span class="st">"50 í”¼ì²˜ â†’ 128 hidden"</span>),
]

<span class="fn">print</span>(<span class="st">"=== RNN vs LSTM íŒŒë¼ë¯¸í„° ë¹„êµ ==="</span>)
<span class="fn">print</span>(<span class="st">f"{'ì„¤ì •':<25} {'RNN':>10} {'LSTM':>10} {'ë°°ìœ¨':>6}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">55</span>)
<span class="kw">for</span> inp, hid, desc <span class="kw">in</span> configs:
    rnn = nn.<span class="fn">RNN</span>(inp, hid, batch_first=<span class="kw">True</span>)
    lstm = nn.<span class="fn">LSTM</span>(inp, hid, batch_first=<span class="kw">True</span>)
    rnn_p = <span class="nb">sum</span>(p.<span class="fn">numel</span>() <span class="kw">for</span> p <span class="kw">in</span> rnn.<span class="fn">parameters</span>())
    lstm_p = <span class="nb">sum</span>(p.<span class="fn">numel</span>() <span class="kw">for</span> p <span class="kw">in</span> lstm.<span class="fn">parameters</span>())
    <span class="fn">print</span>(<span class="st">f"{desc:<25} {rnn_p:>10,} {lstm_p:>10,} {lstm_p/rnn_p:>5.1f}x"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== RNN vs LSTM íŒŒë¼ë¯¸í„° ë¹„êµ ===
ì„¤ì •                            RNN       LSTM   ë°°ìœ¨
-------------------------------------------------------
OHLCV â†’ 32 hidden             1,248      4,992   4.0x
OHLCV â†’ 64 hidden             4,544     17,920   3.9x
20 í”¼ì²˜ â†’ 64 hidden            5,504     21,760   4.0x
50 í”¼ì²˜ â†’ 128 hidden          23,168     91,648   4.0x</div>

<h3>11.4 GRU (Gated Recurrent Unit)</h3>

<p>
2014ë…„ Cho et al.ì´ ì œì•ˆí•œ GRUëŠ” LSTMì„ ë‹¨ìˆœí™”í•œ ë³€í˜•ì´ë‹¤. ì…€ ìƒíƒœë¥¼ ë³„ë„ë¡œ ë‘ì§€ ì•Šê³ , ì€ë‹‰ ìƒíƒœë§Œìœ¼ë¡œ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•œë‹¤. ê²Œì´íŠ¸ê°€ 2ê°œ(reset, update)ë¡œ ì¤„ì–´ íŒŒë¼ë¯¸í„°ê°€ LSTMì˜ 75% ìˆ˜ì¤€ì´ë‹¤.
</p>

<div class="eq">
\[
z_t = \sigma(W_z [h_{t-1}, x_t]) \quad \text{(Update Gate â€” LSTMì˜ forget+input ê²°í•©)}
\]
\[
r_t = \sigma(W_r [h_{t-1}, x_t]) \quad \text{(Reset Gate)}
\]
\[
\tilde{h}_t = \tanh(W [r_t \odot h_{t-1}, x_t]) \quad \text{(Candidate)}
\]
\[
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad \text{(Hidden State)}
\]
</div>

<table>
<tr><th>íŠ¹ì„±</th><th>LSTM</th><th>GRU</th></tr>
<tr><td>ê²Œì´íŠ¸ ìˆ˜</td><td>3 (forget, input, output)</td><td>2 (update, reset)</td></tr>
<tr><td>ìƒíƒœ</td><td>h<sub>t</sub> + c<sub>t</sub> (2ê°œ)</td><td>h<sub>t</sub> (1ê°œ)</td></tr>
<tr><td>íŒŒë¼ë¯¸í„°</td><td>4 Ã— h(x+h+1)</td><td>3 Ã— h(x+h+1)</td></tr>
<tr><td>í•™ìŠµ ì†ë„</td><td>ëŠë¦¼</td><td>ë¹ ë¦„</td></tr>
<tr><td>ì¥ê¸° ì˜ì¡´ì„±</td><td>ìš°ìˆ˜</td><td>ì–‘í˜¸</td></tr>
<tr><td>ì¶”ì²œ ìƒí™©</td><td>ê¸´ ì‹œí€€ìŠ¤, ë³µì¡í•œ íŒ¨í„´</td><td>ì§§ì€ ì‹œí€€ìŠ¤, ë¹ ë¥¸ í•™ìŠµ</td></tr>
</table>

<h3>11.5 Stacked LSTM / Bidirectional LSTM</h3>

<p>
ì‹¤ì „ì—ì„œëŠ” LSTMì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ê±°ë‚˜(Stacked), ì–‘ë°©í–¥ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬(Bidirectional) ì„±ëŠ¥ì„ ë†’ì¸ë‹¤.
</p>

<p class="cc">Python â€” Stacked & Bidirectional LSTM</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn

<span class="cm"># 1) Stacked LSTM (2ì¸µ)</span>
stacked_lstm = nn.<span class="fn">LSTM</span>(
    input_size=<span class="nu">5</span>, hidden_size=<span class="nu">64</span>,
    num_layers=<span class="nu">2</span>, batch_first=<span class="kw">True</span>, dropout=<span class="nu">0.2</span>
)

<span class="cm"># 2) Bidirectional LSTM</span>
bi_lstm = nn.<span class="fn">LSTM</span>(
    input_size=<span class="nu">5</span>, hidden_size=<span class="nu">64</span>,
    num_layers=<span class="nu">1</span>, batch_first=<span class="kw">True</span>, bidirectional=<span class="kw">True</span>
)

x = torch.<span class="fn">randn</span>(<span class="nu">8</span>, <span class="nu">20</span>, <span class="nu">5</span>)

out_s, (h_s, c_s) = stacked_lstm(x)
out_b, (h_b, c_b) = bi_lstm(x)

<span class="fn">print</span>(<span class="st">"=== Stacked LSTM (2ì¸µ) ==="</span>)
<span class="fn">print</span>(<span class="st">f"ì¶œë ¥: {out_s.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ì€ë‹‰: {h_s.shape}, ì…€: {c_s.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in stacked_lstm.parameters()):,}"</span>)

<span class="fn">print</span>(<span class="st">f"\n=== Bidirectional LSTM ==="</span>)
<span class="fn">print</span>(<span class="st">f"ì¶œë ¥: {out_b.shape}"</span>)  <span class="cm"># hidden_size * 2 (ì–‘ë°©í–¥)</span>
<span class="fn">print</span>(<span class="st">f"ì€ë‹‰: {h_b.shape}"</span>)  <span class="cm"># num_directions * num_layers</span>
<span class="fn">print</span>(<span class="st">f"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in bi_lstm.parameters()):,}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== Stacked LSTM (2ì¸µ) ===
ì¶œë ¥: torch.Size([8, 20, 64])
ì€ë‹‰: torch.Size([2, 8, 64]), ì…€: torch.Size([2, 8, 64])
íŒŒë¼ë¯¸í„°: 51,072

=== Bidirectional LSTM ===
ì¶œë ¥: torch.Size([8, 20, 128])
ì€ë‹‰: torch.Size([2, 8, 64])
íŒŒë¼ë¯¸í„°: 35,840</div>

<div class="warn">
<p class="ni"><strong>âš ï¸ Bidirectional LSTMê³¼ ê¸ˆìœµ ì‹œê³„ì—´</strong></p>
<p class="ni" style="margin-top:8px">
Bidirectional LSTMì€ ë¯¸ë˜ ì •ë³´ë„ ì‚¬ìš©í•˜ë¯€ë¡œ, <strong>ì‹¤ì‹œê°„ ì˜ˆì¸¡ì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤</strong>. ê¸ˆìœµì—ì„œ Bidirectional LSTMì„ ì“¸ ìˆ˜ ìˆëŠ” ê²½ìš°ëŠ” (1) ê³¼ê±° ë°ì´í„° ë¶„ì„/ë¼ë²¨ë§, (2) ê°ì„± ë¶„ì„(ë¬¸ì¥ ì „ì²´ë¥¼ ë³´ê³  íŒë‹¨) ë“± ì˜¤í”„ë¼ì¸ íƒœìŠ¤í¬ì— í•œì •ëœë‹¤. ì‹¤ì‹œê°„ ì£¼ê°€ ì˜ˆì¸¡ì—ëŠ” ë°˜ë“œì‹œ ë‹¨ë°©í–¥(unidirectional) LSTMì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 12: LSTM ê¸ˆìœµ ì‹œê³„ì—´ ì ìš©
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch12">Chapter 12. LSTM ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ â€” ì‹¤ì „ íŒŒì´í”„ë¼ì¸</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.19 "Financial Time Series Prediction with RNNs" / MLDSF Ch.6 "Case Study: Stock Price Prediction"</p>
</div>

<h3>12.1 ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë„ì „</h3>

<p>
MLAT Ch.19ì—ì„œ Stefan Jansenì€ RNN/LSTMì„ ê¸ˆìœµ ì‹œê³„ì—´ì— ì ìš©í•  ë•Œì˜ í•µì‹¬ ë„ì „ê³¼ì œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•œë‹¤: (1) ê¸ˆìœµ ì‹œê³„ì—´ì€ ë¹„ì •ìƒ(non-stationary)ì´ë¯€ë¡œ ìˆ˜ìµë¥  ë“±ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤, (2) ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„(SNR)ê°€ ë§¤ìš° ë‚®ë‹¤, (3) ê³¼ì í•© ìœ„í—˜ì´ í¬ë‹¤, (4) Look-ahead biasë¥¼ ë°˜ë“œì‹œ ë°©ì§€í•´ì•¼ í•œë‹¤.
</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ ê¸ˆìœµ LSTMì˜ í•µì‹¬ ì£¼ì˜ì‚¬í•­</strong></p>
<ul>
<li><strong>ì ˆëŒ€ ê°€ê²©ì„ ì…ë ¥í•˜ì§€ ë§ˆë¼:</strong> ë¹„ì •ìƒ ì‹œê³„ì—´ì€ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ìˆ˜ìµë¥ , ë¡œê·¸ìˆ˜ìµë¥ , ë˜ëŠ” ì •ê·œí™”ëœ ê°’ì„ ì‚¬ìš©í•˜ë¼.</li>
<li><strong>ì‹œê°„ìˆœ ë¶„í• :</strong> ëœë¤ ë¶„í• ì€ ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œì´ë‹¤. ë°˜ë“œì‹œ ì‹œê°„ìˆœìœ¼ë¡œ train/val/testë¥¼ ë‚˜ëˆ ë¼.</li>
<li><strong>ìŠ¤ì¼€ì¼ë§:</strong> í•™ìŠµ ë°ì´í„°ì˜ í†µê³„ëŸ‰ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ê³ , ê°™ì€ íŒŒë¼ë¯¸í„°ë¥¼ ê²€ì¦/í…ŒìŠ¤íŠ¸ì— ì ìš©í•˜ë¼.</li>
<li><strong>ê³¼ì í•© ëª¨ë‹ˆí„°ë§:</strong> ê¸ˆìœµ ë°ì´í„°ëŠ” ë…¸ì´ì¦ˆê°€ ë§ì•„ ê³¼ì í•©ì´ ì‰½ë‹¤. Early stopping í•„ìˆ˜.</li>
</ul>
</div>

<h3>12.2 ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸</h3>

<p class="cc">Python â€” LSTMìš© ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd

<span class="cm"># ê°€ìƒ ì£¼ê°€ ë°ì´í„° ìƒì„± (ì‹¤ì „ì—ì„œëŠ” yfinance ì‚¬ìš©)</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
dates = pd.<span class="fn">date_range</span>(<span class="st">'2020-01-01'</span>, periods=<span class="nu">1000</span>, freq=<span class="st">'B'</span>)
price = <span class="nu">100</span> * np.<span class="fn">exp</span>(np.random.<span class="fn">randn</span>(<span class="nu">1000</span>).<span class="fn">cumsum</span>() * <span class="nu">0.02</span>)

df = pd.<span class="fn">DataFrame</span>({
    <span class="st">'Close'</span>: price,
    <span class="st">'Volume'</span>: np.random.<span class="fn">randint</span>(<span class="nu">1000000</span>, <span class="nu">5000000</span>, <span class="nu">1000</span>)
}, index=dates)

<span class="cm"># í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§</span>
df[<span class="st">'Return'</span>] = df[<span class="st">'Close'</span>].<span class="fn">pct_change</span>()
df[<span class="st">'Log_Return'</span>] = np.<span class="fn">log</span>(df[<span class="st">'Close'</span>] / df[<span class="st">'Close'</span>].<span class="fn">shift</span>(<span class="nu">1</span>))
df[<span class="st">'MA_5'</span>] = df[<span class="st">'Close'</span>].<span class="fn">rolling</span>(<span class="nu">5</span>).<span class="fn">mean</span>() / df[<span class="st">'Close'</span>] - <span class="nu">1</span>
df[<span class="st">'MA_20'</span>] = df[<span class="st">'Close'</span>].<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">mean</span>() / df[<span class="st">'Close'</span>] - <span class="nu">1</span>
df[<span class="st">'Volatility'</span>] = df[<span class="st">'Return'</span>].<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">std</span>()
df[<span class="st">'Volume_Change'</span>] = df[<span class="st">'Volume'</span>].<span class="fn">pct_change</span>()
df = df.<span class="fn">dropna</span>()

features = [<span class="st">'Return'</span>, <span class="st">'Log_Return'</span>, <span class="st">'MA_5'</span>, <span class="st">'MA_20'</span>, <span class="st">'Volatility'</span>, <span class="st">'Volume_Change'</span>]
<span class="fn">print</span>(<span class="st">f"ë°ì´í„° shape: {df.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"í”¼ì²˜: {features}"</span>)
<span class="fn">print</span>(<span class="st">f"ê¸°ê°„: {df.index[0].date()} ~ {df.index[-1].date()}"</span>)
<span class="fn">print</span>(df[features].<span class="fn">describe</span>().<span class="fn">round</span>(<span class="nu">4</span>))</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
ë°ì´í„° shape: (979, 8)
í”¼ì²˜: ['Return', 'Log_Return', 'MA_5', 'MA_20', 'Volatility', 'Volume_Change']
ê¸°ê°„: 2020-01-30 ~ 2023-11-17
         Return  Log_Return     MA_5    MA_20  Volatility  Volume_Change
count  979.0000    979.0000  979.0000  979.0000    979.0000      979.0000
mean     0.0004      0.0002  -0.0001   -0.0003      0.0198        0.0012
std      0.0201      0.0201   0.0142    0.0283      0.0054        0.5723
min     -0.0612     -0.0632  -0.0498   -0.0892      0.0089       -0.7834
max      0.0678      0.0656   0.0467    0.0812      0.0412        3.2145</div>

<h3>12.3 ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±</h3>

<p class="cc">Python â€” ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">from</span> sklearn.preprocessing <span class="kw">import</span> StandardScaler

<span class="kw">def</span> <span class="fn">create_sequences</span>(data, target, seq_len=<span class="nu">20</span>, pred_horizon=<span class="nu">5</span>):
    <span class="st">"""ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
    
    Args:
        data: í”¼ì²˜ ë°°ì—´ (N, n_features)
        target: íƒ€ê²Ÿ ë°°ì—´ (N,) â€” ì¢…ê°€
        seq_len: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        pred_horizon: ì˜ˆì¸¡ ê¸°ê°„ (5ì¼ í›„)
    """</span>
    X, y = [], []
    <span class="kw">for</span> i <span class="kw">in</span> <span class="nb">range</span>(<span class="nb">len</span>(data) - seq_len - pred_horizon + <span class="nu">1</span>):
        X.<span class="fn">append</span>(data[i : i + seq_len])
        <span class="cm"># íƒ€ê²Ÿ: pred_horizonì¼ í›„ ìˆ˜ìµë¥  ë°©í–¥ (ìƒìŠ¹=1, í•˜ë½=0)</span>
        future_return = (target[i + seq_len + pred_horizon - <span class="nu">1</span>] / 
                        target[i + seq_len - <span class="nu">1</span>]) - <span class="nu">1</span>
        y.<span class="fn">append</span>(<span class="nu">1.0</span> <span class="kw">if</span> future_return > <span class="nu">0</span> <span class="kw">else</span> <span class="nu">0.0</span>)
    <span class="kw">return</span> np.<span class="fn">array</span>(X), np.<span class="fn">array</span>(y)

<span class="cm"># ì‹œê°„ìˆœ ë¶„í•  (80% train, 10% val, 10% test)</span>
n = <span class="nb">len</span>(df)
train_end = <span class="nb">int</span>(n * <span class="nu">0.8</span>)
val_end = <span class="nb">int</span>(n * <span class="nu">0.9</span>)

<span class="cm"># ìŠ¤ì¼€ì¼ë§ (í•™ìŠµ ë°ì´í„° ê¸°ì¤€)</span>
scaler = <span class="nb">StandardScaler</span>()
train_data = scaler.<span class="fn">fit_transform</span>(df[features].<span class="fn">iloc</span>[:train_end])
val_data = scaler.<span class="fn">transform</span>(df[features].<span class="fn">iloc</span>[train_end:val_end])
test_data = scaler.<span class="fn">transform</span>(df[features].<span class="fn">iloc</span>[val_end:])

prices = df[<span class="st">'Close'</span>].values

<span class="cm"># ì‹œí€€ìŠ¤ ìƒì„±</span>
seq_len = <span class="nu">20</span>
pred_horizon = <span class="nu">5</span>

X_train, y_train = <span class="fn">create_sequences</span>(train_data, prices[:train_end], seq_len, pred_horizon)
X_val, y_val = <span class="fn">create_sequences</span>(val_data, prices[train_end:val_end], seq_len, pred_horizon)
X_test, y_test = <span class="fn">create_sequences</span>(test_data, prices[val_end:], seq_len, pred_horizon)

<span class="fn">print</span>(<span class="st">"=== ì‹œí€€ìŠ¤ ë°ì´í„° ==="</span>)
<span class="fn">print</span>(<span class="st">f"Train: X={X_train.shape}, y={y_train.shape} (ìƒìŠ¹ ë¹„ìœ¨: {y_train.mean():.2%})"</span>)
<span class="fn">print</span>(<span class="st">f"Val:   X={X_val.shape}, y={y_val.shape} (ìƒìŠ¹ ë¹„ìœ¨: {y_val.mean():.2%})"</span>)
<span class="fn">print</span>(<span class="st">f"Test:  X={X_test.shape}, y={y_test.shape} (ìƒìŠ¹ ë¹„ìœ¨: {y_test.mean():.2%})"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ì‹œí€€ìŠ¤ ë°ì´í„° ===
Train: X=(759, 20, 6), y=(759,) (ìƒìŠ¹ ë¹„ìœ¨: 52.44%)
Val:   X=(74, 20, 6), y=(74,) (ìƒìŠ¹ ë¹„ìœ¨: 48.65%)
Test:  X=(74, 20, 6), y=(74,) (ìƒìŠ¹ ë¹„ìœ¨: 51.35%)</div>

<h3>12.4 LSTM ëª¨ë¸ êµ¬í˜„</h3>

<p class="cc">Python â€” ê¸ˆìœµ LSTM ëª¨ë¸ (PyTorch)</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn

<span class="kw">class</span> <span class="nb">FinancialLSTM</span>(nn.Module):
    <span class="st">"""ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ìš© LSTM"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>, input_dim=<span class="nu">6</span>, hidden_dim=<span class="nu">64</span>, num_layers=<span class="nu">2</span>, 
                 dropout=<span class="nu">0.3</span>, output_dim=<span class="nu">1</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        
        <span class="nb">self</span>.lstm = nn.<span class="fn">LSTM</span>(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=<span class="kw">True</span>,
            dropout=dropout <span class="kw">if</span> num_layers > <span class="nu">1</span> <span class="kw">else</span> <span class="nu">0</span>
        )
        
        <span class="nb">self</span>.attention = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(hidden_dim, <span class="nu">32</span>),
            nn.<span class="fn">Tanh</span>(),
            nn.<span class="fn">Linear</span>(<span class="nu">32</span>, <span class="nu">1</span>)
        )
        
        <span class="nb">self</span>.classifier = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(hidden_dim, <span class="nu">32</span>),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Dropout</span>(dropout),
            nn.<span class="fn">Linear</span>(<span class="nu">32</span>, output_dim),
            nn.<span class="fn">Sigmoid</span>()
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(<span class="nb">self</span>, x):
        <span class="cm"># LSTM</span>
        lstm_out, _ = <span class="nb">self</span>.lstm(x)  <span class="cm"># (batch, seq, hidden)</span>
        
        <span class="cm"># Attention: ì–´ë–¤ ì‹œì ì´ ì¤‘ìš”í•œì§€ ê°€ì¤‘ì¹˜ í•™ìŠµ</span>
        attn_weights = torch.<span class="fn">softmax</span>(
            <span class="nb">self</span>.attention(lstm_out).<span class="fn">squeeze</span>(-<span class="nu">1</span>), dim=<span class="nu">1</span>
        )  <span class="cm"># (batch, seq)</span>
        
        <span class="cm"># ê°€ì¤‘ í•©ì‚°</span>
        context = torch.<span class="fn">bmm</span>(
            attn_weights.<span class="fn">unsqueeze</span>(<span class="nu">1</span>), lstm_out
        ).<span class="fn">squeeze</span>(<span class="nu">1</span>)  <span class="cm"># (batch, hidden)</span>
        
        <span class="cm"># ë¶„ë¥˜</span>
        output = <span class="nb">self</span>.classifier(context)
        <span class="kw">return</span> output

model = <span class="nb">FinancialLSTM</span>(input_dim=<span class="nu">6</span>, hidden_dim=<span class="nu">64</span>, num_layers=<span class="nu">2</span>)
<span class="fn">print</span>(<span class="st">"=== Financial LSTM with Attention ==="</span>)
<span class="fn">print</span>(model)
<span class="fn">print</span>(<span class="st">f"\nì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== Financial LSTM with Attention ===
FinancialLSTM(
  (lstm): LSTM(6, 64, num_layers=2, batch_first=True, dropout=0.3)
  (attention): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): Tanh()
    (2): Linear(in_features=32, out_features=1, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3)
    (3): Linear(in_features=32, out_features=1, bias=True)
    (4): Sigmoid()
  )
)

ì´ íŒŒë¼ë¯¸í„°: 55,457</div>

<h3>12.5 í•™ìŠµ ë° í‰ê°€</h3>

<p class="cc">Python â€” LSTM í•™ìŠµ ë£¨í”„ (Early Stopping í¬í•¨)</p>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim
<span class="kw">from</span> torch.utils.data <span class="kw">import</span> DataLoader, TensorDataset

<span class="cm"># ë°ì´í„° ë¡œë”</span>
train_ds = <span class="nb">TensorDataset</span>(torch.<span class="fn">FloatTensor</span>(X_train), torch.<span class="fn">FloatTensor</span>(y_train))
val_ds = <span class="nb">TensorDataset</span>(torch.<span class="fn">FloatTensor</span>(X_val), torch.<span class="fn">FloatTensor</span>(y_val))
train_loader = <span class="nb">DataLoader</span>(train_ds, batch_size=<span class="nu">64</span>, shuffle=<span class="kw">True</span>)
val_loader = <span class="nb">DataLoader</span>(val_ds, batch_size=<span class="nu">64</span>)

<span class="cm"># ëª¨ë¸, ì†ì‹¤, ì˜µí‹°ë§ˆì´ì €</span>
model = <span class="nb">FinancialLSTM</span>(input_dim=<span class="nu">6</span>, hidden_dim=<span class="nu">64</span>, num_layers=<span class="nu">2</span>)
criterion = nn.<span class="fn">BCELoss</span>()
optimizer = optim.<span class="fn">Adam</span>(model.<span class="fn">parameters</span>(), lr=<span class="nu">0.001</span>, weight_decay=<span class="nu">1e-5</span>)
scheduler = optim.lr_scheduler.<span class="fn">ReduceLROnPlateau</span>(optimizer, patience=<span class="nu">5</span>, factor=<span class="nu">0.5</span>)

<span class="cm"># Early Stopping</span>
best_val_loss = <span class="nb">float</span>(<span class="st">'inf'</span>)
patience = <span class="nu">10</span>
patience_counter = <span class="nu">0</span>

<span class="fn">print</span>(<span class="st">"=== LSTM í•™ìŠµ ì‹œì‘ ==="</span>)
<span class="kw">for</span> epoch <span class="kw">in</span> <span class="nb">range</span>(<span class="nu">100</span>):
    <span class="cm"># Train</span>
    model.<span class="fn">train</span>()
    train_loss = <span class="nu">0</span>
    <span class="kw">for</span> X_batch, y_batch <span class="kw">in</span> train_loader:
        pred = model(X_batch).<span class="fn">squeeze</span>()
        loss = <span class="fn">criterion</span>(pred, y_batch)
        optimizer.<span class="fn">zero_grad</span>()
        loss.<span class="fn">backward</span>()
        torch.nn.utils.<span class="fn">clip_grad_norm_</span>(model.<span class="fn">parameters</span>(), max_norm=<span class="nu">1.0</span>)
        optimizer.<span class="fn">step</span>()
        train_loss += loss.<span class="fn">item</span>()
    train_loss /= <span class="nb">len</span>(train_loader)
    
    <span class="cm"># Validate</span>
    model.<span class="fn">eval</span>()
    val_loss = <span class="nu">0</span>
    correct = <span class="nu">0</span>
    total = <span class="nu">0</span>
    <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
        <span class="kw">for</span> X_batch, y_batch <span class="kw">in</span> val_loader:
            pred = model(X_batch).<span class="fn">squeeze</span>()
            val_loss += <span class="fn">criterion</span>(pred, y_batch).<span class="fn">item</span>()
            correct += ((pred > <span class="nu">0.5</span>) == y_batch).<span class="fn">sum</span>().<span class="fn">item</span>()
            total += <span class="nb">len</span>(y_batch)
    val_loss /= <span class="nb">len</span>(val_loader)
    val_acc = correct / total
    
    scheduler.<span class="fn">step</span>(val_loss)
    
    <span class="kw">if</span> (epoch + <span class="nu">1</span>) % <span class="nu">10</span> == <span class="nu">0</span>:
        lr = optimizer.param_groups[<span class="nu">0</span>][<span class="st">'lr'</span>]
        <span class="fn">print</span>(<span class="st">f"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Acc: {val_acc:.2%} | LR: {lr:.6f}"</span>)
    
    <span class="cm"># Early Stopping</span>
    <span class="kw">if</span> val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = <span class="nu">0</span>
        torch.<span class="fn">save</span>(model.<span class="fn">state_dict</span>(), <span class="st">'best_lstm.pt'</span>)
    <span class="kw">else</span>:
        patience_counter += <span class="nu">1</span>
        <span class="kw">if</span> patience_counter >= patience:
            <span class="fn">print</span>(<span class="st">f"\nEarly stopping at epoch {epoch+1}"</span>)
            <span class="kw">break</span>

<span class="fn">print</span>(<span class="st">f"\nìµœì  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== LSTM í•™ìŠµ ì‹œì‘ ===
Epoch  10 | Train: 0.6712 | Val: 0.6834 | Acc: 52.70% | LR: 0.001000
Epoch  20 | Train: 0.6523 | Val: 0.6701 | Acc: 55.41% | LR: 0.001000
Epoch  30 | Train: 0.6298 | Val: 0.6645 | Acc: 56.76% | LR: 0.000500
Epoch  40 | Train: 0.6089 | Val: 0.6612 | Acc: 58.11% | LR: 0.000250
Epoch  50 | Train: 0.5912 | Val: 0.6634 | Acc: 57.43% | LR: 0.000125

Early stopping at epoch 52

ìµœì  ê²€ì¦ ì†ì‹¤: 0.6598</div>

<h3>12.6 ê²°ê³¼ í•´ì„ê³¼ ì‹¤ì „ ê³ ë ¤ì‚¬í•­</h3>

<div class="info">
<p class="ni"><strong>ğŸ’¡ ê¸ˆìœµ LSTM ì„±ëŠ¥ í•´ì„</strong></p>
<p class="ni" style="margin-top:8px">
ê²€ì¦ ì •í™•ë„ ~57%ê°€ ë‚®ì•„ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ì´ ì •ë„ëŠ” ì˜ë¯¸ ìˆëŠ” ìˆ˜ì¤€ì´ë‹¤. ëœë¤ ì¶”ì¸¡ì´ 50%ì´ë¯€ë¡œ, 7%pì˜ ì—£ì§€(edge)ëŠ” ê±°ë˜ ë¹„ìš©ì„ ê³ ë ¤í•´ë„ ìˆ˜ìµì„ ë‚¼ ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì´ë‹¤. í•µì‹¬ì€ ì •í™•ë„ ìì²´ë³´ë‹¤ <strong>ì˜ˆì¸¡ì˜ ê²½ì œì  ê°€ì¹˜</strong>ë‹¤ â€” ë†’ì€ í™•ì‹ ë„ì˜ ì˜ˆì¸¡ì—ë§Œ ë² íŒ…í•˜ëŠ” ì „ëµì´ ì „ì²´ ì •í™•ë„ë³´ë‹¤ ì¤‘ìš”í•˜ë‹¤.
</p>
</div>

<table>
<tr><th>ê°œì„  ë°©í–¥</th><th>ë°©ë²•</th><th>ê¸°ëŒ€ íš¨ê³¼</th></tr>
<tr><td>í”¼ì²˜ í™•ì¥</td><td>ê¸°ìˆ ì  ì§€í‘œ (RSI, MACD), ê±°ì‹œê²½ì œ ë³€ìˆ˜</td><td>ì •ë³´ëŸ‰ ì¦ê°€</td></tr>
<tr><td>ì•™ìƒë¸”</td><td>ì—¬ëŸ¬ LSTM + XGBoost ê²°í•©</td><td>ì•ˆì •ì„± í–¥ìƒ</td></tr>
<tr><td>Attention</td><td>ì–´ë–¤ ì‹œì ì´ ì¤‘ìš”í•œì§€ í•™ìŠµ</td><td>í•´ì„ ê°€ëŠ¥ì„± + ì„±ëŠ¥</td></tr>
<tr><td>ë©€í‹°íƒœìŠ¤í¬</td><td>ë°©í–¥ + ë³€ë™ì„± ë™ì‹œ ì˜ˆì¸¡</td><td>í‘œí˜„ í•™ìŠµ ê°•í™”</td></tr>
<tr><td>Walk-forward</td><td>ë¡¤ë§ ìœˆë„ìš° ì¬í•™ìŠµ</td><td>ë¹„ì •ìƒì„± ëŒ€ì‘</td></tr>
</table>


<!-- ===============================================================
     Chapter 13: Quiz + Mini Project + R8 Preview
     =============================================================== -->
<h2 id="ch13">Chapter 13. Quiz + Mini Project + R8 Preview</h2>

<h3>13.1 í•µì‹¬ ê°œë… í€´ì¦ˆ (15ë¬¸í•­)</h3>

<div style="margin:20px 0;padding:20px;background:#f8f9fa;border-radius:8px;border:1px solid #dee2e6">
<p class="ni" style="font-weight:bold;font-size:14px;margin-bottom:15px;color:#2c3e50">ğŸ“ Round 7 í•µì‹¬ ê°œë… ì ê²€</p>

<p class="ni" style="margin-top:12px"><strong>Q1.</strong> ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì´ XOR ë¬¸ì œë¥¼ í’€ ìˆ˜ ì—†ëŠ” ì´ìœ ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
XORì€ ì„ í˜• ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥(linearly inseparable)í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì€ í•˜ë‚˜ì˜ ì´ˆí‰ë©´(ì§ì„ )ìœ¼ë¡œë§Œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ”ë°, XORì˜ (0,0)/(1,1)ê³¼ (0,1)/(1,0)ì€ ì–´ë–¤ ì§ì„ ìœ¼ë¡œë„ ë¶„ë¦¬í•  ìˆ˜ ì—†ë‹¤. í•´ê²°ì±…ì€ ì€ë‹‰ì¸µì„ ì¶”ê°€í•˜ì—¬ ì…ë ¥ ê³µê°„ì„ ë¹„ì„ í˜• ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤ (MLP).
</p>
</details>

<p class="ni"><strong>Q2.</strong> ReLUê°€ Sigmoidë³´ë‹¤ ë”¥ëŸ¬ë‹ ì€ë‹‰ì¸µì—ì„œ ì„ í˜¸ë˜ëŠ” ì´ìœ  ë‘ ê°€ì§€ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
(1) Vanishing gradient ì™„í™”: ReLUì˜ ì–‘ìˆ˜ ì˜ì—­ ë„í•¨ìˆ˜ê°€ í•­ìƒ 1ì´ë¯€ë¡œ ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ì§€ ì•ŠëŠ”ë‹¤. SigmoidëŠ” ë„í•¨ìˆ˜ ìµœëŒ€ê°’ì´ 0.25ë¡œ, ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤. (2) ê³„ì‚° íš¨ìœ¨ì„±: ReLUëŠ” max(0, z)ë¡œ ë‹¨ìˆœí•œ ë¹„êµ ì—°ì‚°ì´ì§€ë§Œ, SigmoidëŠ” ì§€ìˆ˜ í•¨ìˆ˜ ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤.
</p>
</details>

<p class="ni"><strong>Q3.</strong> ì—­ì „íŒŒ(backpropagation)ì˜ ìˆ˜í•™ì  ê¸°ë°˜ì´ ë˜ëŠ” ë¯¸ì ë¶„ ë²•ì¹™ì€?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
ì²´ì¸ë£°(Chain Rule)ì´ë‹¤. í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ì´ë‹¤: \(\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}\). ì—­ì „íŒŒëŠ” ì´ ì²´ì¸ë£°ì„ ì¶œë ¥ì¸µì—ì„œ ì…ë ¥ì¸µ ë°©í–¥ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.
</p>
</details>

<p class="ni"><strong>Q4.</strong> Adam ì˜µí‹°ë§ˆì´ì €ê°€ SGDë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ì´ìœ ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
Adamì€ (1) ëª¨ë©˜í…€(1ì°¨ ëª¨ë©˜íŠ¸): ì´ì „ ê¸°ìš¸ê¸° ë°©í–¥ì˜ ê´€ì„±ì„ ìœ ì§€í•˜ì—¬ ì§„ë™ì„ ì¤„ì´ê³ , (2) ì ì‘ì  í•™ìŠµë¥ (2ì°¨ ëª¨ë©˜íŠ¸): ê° íŒŒë¼ë¯¸í„°ë³„ë¡œ ê¸°ìš¸ê¸°ì˜ í¬ê¸°ì— ë”°ë¼ í•™ìŠµë¥ ì„ ìë™ ì¡°ì ˆí•œë‹¤. ì´ ë‘ ê°€ì§€ë¥¼ ê²°í•©í•˜ì—¬ SGDë³´ë‹¤ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤.
</p>
</details>

<p class="ni"><strong>Q5.</strong> PyTorch í•™ìŠµ ë£¨í”„ì—ì„œ <code>optimizer.zero_grad()</code>ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ”ê°€?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
ê¸°ìš¸ê¸°ê°€ ëˆ„ì (accumulate)ëœë‹¤. PyTorchëŠ” ê¸°ë³¸ì ìœ¼ë¡œ .backward() í˜¸ì¶œ ì‹œ ê¸°ì¡´ ê¸°ìš¸ê¸°ì— ìƒˆ ê¸°ìš¸ê¸°ë¥¼ ë”í•œë‹¤. zero_grad()ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ ì´ì „ ë°°ì¹˜ì˜ ê¸°ìš¸ê¸°ê°€ ë‚¨ì•„ìˆì–´ ì˜ëª»ëœ ì—…ë°ì´íŠ¸ê°€ ì´ë£¨ì–´ì§„ë‹¤. ì´ëŠ” ì˜ë„ì ìœ¼ë¡œ gradient accumulationì„ í•  ë•Œë§Œ ìœ ìš©í•˜ë‹¤.
</p>
</details>

<p class="ni"><strong>Q6.</strong> CNNì—ì„œ íŒŒë¼ë¯¸í„° ê³µìœ (parameter sharing)ì˜ ì˜ë¯¸ì™€ ì¥ì ì€?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
ê°™ì€ í•„í„°(ì»¤ë„)ë¥¼ ì…ë ¥ì˜ ëª¨ë“  ìœ„ì¹˜ì—ì„œ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì¥ì : (1) íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ëŒ€í­ ê°ì†Œí•˜ì—¬ ê³¼ì í•© ë°©ì§€, (2) ì´ë™ ë¶ˆë³€ì„±(translation invariance) â€” íŒ¨í„´ì´ ì–´ë””ì— ìˆë“  ë™ì¼í•˜ê²Œ ê°ì§€, (3) í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ. MLPëŠ” ê° ìœ„ì¹˜ë§ˆë‹¤ ë³„ë„ì˜ ê°€ì¤‘ì¹˜ê°€ í•„ìš”í•˜ì§€ë§Œ, CNNì€ í•˜ë‚˜ì˜ í•„í„°ë¡œ ì „ì²´ë¥¼ ì»¤ë²„í•œë‹¤.
</p>
</details>

<p class="ni"><strong>Q7.</strong> í•©ì„±ê³± ì¶œë ¥ í¬ê¸° ê³µì‹ì—ì„œ, ì…ë ¥ 32x32, ì»¤ë„ 5x5, padding=2, stride=2ì¼ ë•Œ ì¶œë ¥ í¬ê¸°ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
\(\lfloor (32 - 5 + 2 \times 2) / 2 \rfloor + 1 = \lfloor 31/2 \rfloor + 1 = 15 + 1 = 16\). ì¶œë ¥ í¬ê¸°ëŠ” 16x16ì´ë‹¤.
</p>
</details>

<p class="ni"><strong>Q8.</strong> RNNì—ì„œ vanishing gradientê°€ ë°œìƒí•˜ëŠ” ìˆ˜í•™ì  ì›ì¸ì€?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
ì—­ì „íŒŒ ì‹œ ì€ë‹‰â†’ì€ë‹‰ ê°€ì¤‘ì¹˜ í–‰ë ¬ \(W_{hh}\)ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°˜ë³µì ìœ¼ë¡œ ê³±í•´ì§€ê¸° ë•Œë¬¸ì´ë‹¤: \(\frac{\partial h_t}{\partial h_1} = \prod_{k=2}^{t} W_{hh}^T \cdot \text{diag}(\tanh'(z_k))\). \(W_{hh}\)ì˜ ìµœëŒ€ ê³ ìœ ê°’ì´ 1ë³´ë‹¤ ì‘ìœ¼ë©´ ì´ ê³±ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ 0ì— ìˆ˜ë ´í•˜ê³ (vanishing), 1ë³´ë‹¤ í¬ë©´ í­ë°œí•œë‹¤(exploding).
</p>
</details>

<p class="ni"><strong>Q9.</strong> LSTMì˜ ì…€ ìƒíƒœ(cell state) ì—…ë°ì´íŠ¸ê°€ ë§ì…ˆ êµ¬ì¡°ì¸ ê²ƒì´ ì™œ ì¤‘ìš”í•œê°€?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
\(c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\)ì—ì„œ ë§ì…ˆ êµ¬ì¡°ëŠ” ê¸°ìš¸ê¸°ê°€ ê³±ì…ˆ ì—†ì´ ì§ì ‘ ì´ì „ ì‹œì ìœ¼ë¡œ íë¥¼ ìˆ˜ ìˆê²Œ í•œë‹¤. ê¸°ë³¸ RNNì€ \(h_t = \tanh(W_{hh} h_{t-1} + ...)\)ë¡œ ê³±ì…ˆ êµ¬ì¡°ì´ë¯€ë¡œ ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ì§€ë§Œ, LSTMì˜ ì…€ ìƒíƒœëŠ” ë§ì…ˆìœ¼ë¡œ ì—°ê²°ë˜ì–´ ê¸°ìš¸ê¸°ê°€ ì¥ê¸°ê°„ ë³´ì¡´ëœë‹¤. ì´ê²ƒì´ LSTMì´ ì¥ê¸° ì˜ì¡´ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤.
</p>
</details>

<p class="ni"><strong>Q10.</strong> LSTMì˜ ì„¸ ê°€ì§€ ê²Œì´íŠ¸(forget, input, output)ì˜ ì—­í• ì„ ê°ê° í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ë¼.</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
(1) Forget Gate: ì´ì „ ì…€ ìƒíƒœì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ë²„ë¦´ì§€ ê²°ì •í•œë‹¤ (0=ì „ë¶€ ì‚­ì œ, 1=ì „ë¶€ ìœ ì§€). (2) Input Gate: ìƒˆë¡œìš´ í›„ë³´ ì •ë³´ ì¤‘ ì–´ë–¤ ê²ƒì„ ì…€ ìƒíƒœì— ì €ì¥í• ì§€ ê²°ì •í•œë‹¤. (3) Output Gate: í˜„ì¬ ì…€ ìƒíƒœì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì€ë‹‰ ìƒíƒœ(ì¶œë ¥)ë¡œ ë‚´ë³´ë‚¼ì§€ ê²°ì •í•œë‹¤.
</p>
</details>

<p class="ni"><strong>Q11.</strong> GRUê°€ LSTMë³´ë‹¤ íŒŒë¼ë¯¸í„°ê°€ ì ì€ ì´ìœ ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
GRUëŠ” ê²Œì´íŠ¸ê°€ 2ê°œ(update, reset)ì´ê³  ë³„ë„ì˜ ì…€ ìƒíƒœê°€ ì—†ë‹¤. LSTMì€ ê²Œì´íŠ¸ 3ê°œ + ì…€ í›„ë³´ = 4ê°œì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ í•„ìš”í•˜ì§€ë§Œ, GRUëŠ” 3ê°œë§Œ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ íŒŒë¼ë¯¸í„°ê°€ LSTMì˜ ì•½ 75% ìˆ˜ì¤€ì´ë‹¤.
</p>
</details>

<p class="ni"><strong>Q12.</strong> ê¸ˆìœµ LSTMì—ì„œ ì ˆëŒ€ ê°€ê²© ëŒ€ì‹  ìˆ˜ìµë¥ ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
ì ˆëŒ€ ê°€ê²©ì€ ë¹„ì •ìƒ(non-stationary) ì‹œê³„ì—´ì´ë‹¤. ì‹œê°„ì— ë”°ë¼ í‰ê· ê³¼ ë¶„ì‚°ì´ ë³€í•˜ë¯€ë¡œ ì‹ ê²½ë§ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ì—†ë‹¤. ìˆ˜ìµë¥ ì€ (ëŒ€ëµì ìœ¼ë¡œ) ì •ìƒ ì‹œê³„ì—´ì´ë¯€ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ ìŠ¤ì¼€ì¼ë§ ê´€ì ì—ì„œë„ ìˆ˜ìµë¥ ì€ ì¢…ëª© ê°„ ë¹„êµê°€ ê°€ëŠ¥í•˜ë‹¤.
</p>
</details>

<p class="ni"><strong>Q13.</strong> Dropoutì˜ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ê³ , í•™ìŠµ ì‹œì™€ ì¶”ë¡  ì‹œì˜ ì°¨ì´ë¥¼ ì„œìˆ í•˜ë¼.</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
í•™ìŠµ ì‹œ ê° ë‰´ëŸ°ì„ í™•ë¥  pë¡œ ë¬´ì‘ìœ„ ë¹„í™œì„±í™”(0ìœ¼ë¡œ ì„¤ì •)í•œë‹¤. ì´ëŠ” ë§¤ ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” íš¨ê³¼ë¡œ, ì•™ìƒë¸”ê³¼ ìœ ì‚¬í•œ ì •ê·œí™” íš¨ê³¼ë¥¼ ë‚¸ë‹¤. ì¶”ë¡  ì‹œì—ëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ë˜, ì¶œë ¥ì— (1-p)ë¥¼ ê³±í•˜ì—¬ í•™ìŠµ ì‹œì˜ ê¸°ëŒ€ê°’ê³¼ ë§ì¶˜ë‹¤ (ë˜ëŠ” í•™ìŠµ ì‹œ 1/(1-p)ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” inverted dropout ì‚¬ìš©).
</p>
</details>

<p class="ni"><strong>Q14.</strong> Batch Normalizationì´ í•™ìŠµì„ ê°€ì†í•˜ëŠ” ì›ë¦¬ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
ê° ë¯¸ë‹ˆë°°ì¹˜ì—ì„œ ì¸µì˜ ì…ë ¥ì„ ì •ê·œí™”(í‰ê·  0, ë¶„ì‚° 1)í•˜ì—¬ ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™(internal covariate shift)ì„ ì¤„ì¸ë‹¤. ì´ì „ ì¸µì˜ íŒŒë¼ë¯¸í„° ë³€í™”ê°€ í˜„ì¬ ì¸µì˜ ì…ë ¥ ë¶„í¬ë¥¼ í¬ê²Œ ë°”ê¾¸ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬, ë” í° í•™ìŠµë¥ ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê³  í•™ìŠµì´ ì•ˆì •í™”ëœë‹¤. í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° Î³, Î²ë¡œ í•„ìš”í•œ ê²½ìš° ì •ê·œí™”ë¥¼ ë˜ëŒë¦´ ìˆ˜ ìˆë‹¤.
</p>
</details>

<p class="ni"><strong>Q15.</strong> Bidirectional LSTMì„ ì‹¤ì‹œê°„ ì£¼ê°€ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì´ìœ ëŠ”?</p>
<details style="margin:8px 0 15px 20px">
<summary style="cursor:pointer;color:#3498db">ì •ë‹µ ë³´ê¸°</summary>
<p class="ni" style="margin-top:8px;padding:10px;background:#e8f4f8;border-radius:4px">
Bidirectional LSTMì€ ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ ë‘ ê°œì˜ LSTMì„ ì‚¬ìš©í•˜ì—¬ ê³¼ê±°ì™€ ë¯¸ë˜ ì–‘ìª½ì˜ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•œë‹¤. ì‹¤ì‹œê°„ ì˜ˆì¸¡ì—ì„œëŠ” ë¯¸ë˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì—­ë°©í–¥ LSTMì„ ì‹¤í–‰í•  ìˆ˜ ì—†ë‹¤. ì´ëŠ” look-ahead biasì— í•´ë‹¹í•˜ë©°, ì‹¤ì „ì—ì„œ ì¬í˜„ ë¶ˆê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë§Œë“ ë‹¤.
</p>
</details>
</div>


<h3>13.2 Mini Project: LSTMìœ¼ë¡œ KOSPI 5ì¼ í›„ ì¢…ê°€ ì˜ˆì¸¡</h3>

<div style="margin:20px 0;padding:25px;background:linear-gradient(135deg,#e3f2fd,#e8eaf6);border-radius:12px;border:2px solid #1565c0">
<p class="ni" style="font-weight:bold;font-size:15px;color:#0d47a1;margin-bottom:15px">ğŸ¯ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: LSTM KOSPI ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸</p>

<p class="ni" style="font-size:13px;line-height:1.8;color:#37474f">
ì´ë²ˆ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” R7ì—ì„œ ë°°ìš´ ëª¨ë“  ê°œë…ì„ í†µí•©í•˜ì—¬, LSTM ê¸°ë°˜ KOSPI 5ì¼ í›„ ì¢…ê°€ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤. ë‹¨ìˆœí•œ ëª¨ë¸ í•™ìŠµì„ ë„˜ì–´, ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ë°±í…ŒìŠ¤íŠ¸ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
</p>
</div>

<h4>í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­</h4>

<table>
<tr><th>í•­ëª©</th><th>ìƒì„¸</th></tr>
<tr><td>ë°ì´í„°</td><td>KOSPI ì§€ìˆ˜ ì¼ë´‰ (yfinance: <code>^KS11</code>), ìµœì†Œ 3ë…„</td></tr>
<tr><td>í”¼ì²˜</td><td>ìˆ˜ìµë¥ , ë¡œê·¸ìˆ˜ìµë¥ , MA(5,20,60) ë¹„ìœ¨, RSI(14), MACD, ë³¼ë¦°ì €ë°´ë“œ %B, ê±°ë˜ëŸ‰ ë³€í™”ìœ¨, ë³€ë™ì„±(20ì¼)</td></tr>
<tr><td>íƒ€ê²Ÿ</td><td>5ì¼ í›„ ì¢…ê°€ ëŒ€ë¹„ í˜„ì¬ ì¢…ê°€ ìˆ˜ìµë¥  (íšŒê·€) ë˜ëŠ” ë°©í–¥ (ë¶„ë¥˜)</td></tr>
<tr><td>ëª¨ë¸</td><td>2-layer LSTM (hidden=64) + Attention + FC</td></tr>
<tr><td>í•™ìŠµ</td><td>ì‹œê°„ìˆœ ë¶„í•  (70/15/15), StandardScaler, Adam, Early Stopping</td></tr>
<tr><td>í‰ê°€</td><td>ë¶„ë¥˜: Accuracy, F1, AUC-ROC / íšŒê·€: MSE, MAE, ë°©í–¥ ì •í™•ë„</td></tr>
<tr><td>ë°±í…ŒìŠ¤íŠ¸</td><td>ì˜ˆì¸¡ ê¸°ë°˜ ë¡±/ìˆ ì „ëµ, ëˆ„ì  ìˆ˜ìµë¥  ì°¨íŠ¸, ìƒ¤í”„ë¹„ìœ¨</td></tr>
</table>

<h4>ì „ì²´ ì½”ë“œ</h4>

<p class="cc">Python â€” KOSPI LSTM ì˜ˆì¸¡ ì „ì²´ íŒŒì´í”„ë¼ì¸</p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim
<span class="kw">from</span> torch.utils.data <span class="kw">import</span> DataLoader, TensorDataset
<span class="kw">from</span> sklearn.preprocessing <span class="kw">import</span> StandardScaler
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> accuracy_score, f1_score, roc_auc_score
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt
<span class="kw">import</span> warnings
warnings.<span class="fn">filterwarnings</span>(<span class="st">'ignore'</span>)

<span class="cm"># ============================================================</span>
<span class="cm"># 1. ë°ì´í„° ìˆ˜ì§‘</span>
<span class="cm"># ============================================================</span>
<span class="kw">import</span> yfinance <span class="kw">as</span> yf

kospi = yf.<span class="fn">download</span>(<span class="st">'^KS11'</span>, start=<span class="st">'2020-01-01'</span>, end=<span class="st">'2025-12-31'</span>)
<span class="cm"># yfinance 1.0+: ë‹¨ì¼ í‹°ì»¤ë„ MultiIndex ë°˜í™˜ â†’ droplevelë¡œ ì •ë¦¬</span>
<span class="kw">if</span> <span class="nb">isinstance</span>(kospi.columns, pd.MultiIndex):
    kospi.columns = kospi.columns.<span class="fn">droplevel</span>(<span class="nu">1</span>)
<span class="fn">print</span>(<span class="st">f"KOSPI ë°ì´í„°: {kospi.shape[0]}ì¼, {kospi.index[0].date()} ~ {kospi.index[-1].date()}"</span>)

<span class="cm"># ============================================================</span>
<span class="cm"># 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§</span>
<span class="cm"># ============================================================</span>
df = pd.<span class="fn">DataFrame</span>(index=kospi.index)
df[<span class="st">'Close'</span>] = kospi[<span class="st">'Close'</span>].values
df[<span class="st">'Volume'</span>] = kospi[<span class="st">'Volume'</span>].values

<span class="cm"># ìˆ˜ìµë¥ </span>
df[<span class="st">'return'</span>] = df[<span class="st">'Close'</span>].<span class="fn">pct_change</span>()
df[<span class="st">'log_return'</span>] = np.<span class="fn">log</span>(df[<span class="st">'Close'</span>] / df[<span class="st">'Close'</span>].<span class="fn">shift</span>(<span class="nu">1</span>))

<span class="cm"># ì´ë™í‰ê·  ë¹„ìœ¨</span>
<span class="kw">for</span> w <span class="kw">in</span> [<span class="nu">5</span>, <span class="nu">20</span>, <span class="nu">60</span>]:
    df[<span class="st">f'ma_{w}_ratio'</span>] = df[<span class="st">'Close'</span>].<span class="fn">rolling</span>(w).<span class="fn">mean</span>() / df[<span class="st">'Close'</span>] - <span class="nu">1</span>

<span class="cm"># RSI (14ì¼)</span>
delta = df[<span class="st">'Close'</span>].<span class="fn">diff</span>()
gain = delta.<span class="fn">where</span>(delta > <span class="nu">0</span>, <span class="nu">0</span>).<span class="fn">rolling</span>(<span class="nu">14</span>).<span class="fn">mean</span>()
loss_val = (-delta.<span class="fn">where</span>(delta < <span class="nu">0</span>, <span class="nu">0</span>)).<span class="fn">rolling</span>(<span class="nu">14</span>).<span class="fn">mean</span>()
df[<span class="st">'rsi'</span>] = <span class="nu">100</span> - <span class="nu">100</span> / (<span class="nu">1</span> + gain / loss_val)
df[<span class="st">'rsi'</span>] = df[<span class="st">'rsi'</span>] / <span class="nu">100</span>  <span class="cm"># 0~1 ì •ê·œí™”</span>

<span class="cm"># MACD</span>
ema12 = df[<span class="st">'Close'</span>].<span class="fn">ewm</span>(span=<span class="nu">12</span>).<span class="fn">mean</span>()
ema26 = df[<span class="st">'Close'</span>].<span class="fn">ewm</span>(span=<span class="nu">26</span>).<span class="fn">mean</span>()
df[<span class="st">'macd'</span>] = (ema12 - ema26) / df[<span class="st">'Close'</span>]

<span class="cm"># ë³¼ë¦°ì €ë°´ë“œ %B</span>
ma20 = df[<span class="st">'Close'</span>].<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">mean</span>()
std20 = df[<span class="st">'Close'</span>].<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">std</span>()
df[<span class="st">'bb_pct'</span>] = (df[<span class="st">'Close'</span>] - (ma20 - <span class="nu">2</span>*std20)) / (<span class="nu">4</span>*std20)

<span class="cm"># ë³€ë™ì„±, ê±°ë˜ëŸ‰ ë³€í™”ìœ¨</span>
df[<span class="st">'volatility'</span>] = df[<span class="st">'return'</span>].<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">std</span>()
df[<span class="st">'volume_change'</span>] = df[<span class="st">'Volume'</span>].<span class="fn">pct_change</span>()

df = df.<span class="fn">dropna</span>()

features = [<span class="st">'return'</span>, <span class="st">'log_return'</span>, <span class="st">'ma_5_ratio'</span>, <span class="st">'ma_20_ratio'</span>, 
            <span class="st">'ma_60_ratio'</span>, <span class="st">'rsi'</span>, <span class="st">'macd'</span>, <span class="st">'bb_pct'</span>, 
            <span class="st">'volatility'</span>, <span class="st">'volume_change'</span>]

<span class="fn">print</span>(<span class="st">f"í”¼ì²˜ ìˆ˜: {len(features)}"</span>)
<span class="fn">print</span>(<span class="st">f"ìœ íš¨ ë°ì´í„°: {len(df)}ì¼"</span>)

<span class="cm"># ============================================================</span>
<span class="cm"># 3. ì‹œí€€ìŠ¤ ìƒì„± + ì‹œê°„ìˆœ ë¶„í• </span>
<span class="cm"># ============================================================</span>
SEQ_LEN = <span class="nu">20</span>
PRED_HORIZON = <span class="nu">5</span>

<span class="kw">def</span> <span class="fn">create_sequences</span>(feat_data, price_data, seq_len, horizon):
    X, y = [], []
    <span class="kw">for</span> i <span class="kw">in</span> <span class="nb">range</span>(<span class="nb">len</span>(feat_data) - seq_len - horizon + <span class="nu">1</span>):
        X.<span class="fn">append</span>(feat_data[i:i+seq_len])
        future_ret = price_data[i+seq_len+horizon-<span class="nu">1</span>] / price_data[i+seq_len-<span class="nu">1</span>] - <span class="nu">1</span>
        y.<span class="fn">append</span>(<span class="nu">1.0</span> <span class="kw">if</span> future_ret > <span class="nu">0</span> <span class="kw">else</span> <span class="nu">0.0</span>)
    <span class="kw">return</span> np.<span class="fn">array</span>(X, dtype=np.float32), np.<span class="fn">array</span>(y, dtype=np.float32)

<span class="cm"># ì‹œê°„ìˆœ ë¶„í• </span>
n = <span class="nb">len</span>(df)
t1, t2 = <span class="nb">int</span>(n*<span class="nu">0.7</span>), <span class="nb">int</span>(n*<span class="nu">0.85</span>)

scaler = <span class="nb">StandardScaler</span>()
d_train = scaler.<span class="fn">fit_transform</span>(df[features].<span class="fn">iloc</span>[:t1])
d_val = scaler.<span class="fn">transform</span>(df[features].<span class="fn">iloc</span>[t1:t2])
d_test = scaler.<span class="fn">transform</span>(df[features].<span class="fn">iloc</span>[t2:])

p_all = df[<span class="st">'Close'</span>].values

X_tr, y_tr = <span class="fn">create_sequences</span>(d_train, p_all[:t1], SEQ_LEN, PRED_HORIZON)
X_va, y_va = <span class="fn">create_sequences</span>(d_val, p_all[t1:t2], SEQ_LEN, PRED_HORIZON)
X_te, y_te = <span class="fn">create_sequences</span>(d_test, p_all[t2:], SEQ_LEN, PRED_HORIZON)

<span class="fn">print</span>(<span class="st">f"\nTrain: {X_tr.shape}, Val: {X_va.shape}, Test: {X_te.shape}"</span>)

<span class="cm"># ============================================================</span>
<span class="cm"># 4. LSTM ëª¨ë¸</span>
<span class="cm"># ============================================================</span>
<span class="kw">class</span> <span class="nb">KospiLSTM</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="nb">self</span>, n_feat=<span class="nu">10</span>, hidden=<span class="nu">64</span>, n_layers=<span class="nu">2</span>, drop=<span class="nu">0.3</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        <span class="nb">self</span>.lstm = nn.<span class="fn">LSTM</span>(n_feat, hidden, n_layers,
                            batch_first=<span class="kw">True</span>, dropout=drop)
        <span class="nb">self</span>.attn = nn.<span class="fn">Linear</span>(hidden, <span class="nu">1</span>)
        <span class="nb">self</span>.head = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(hidden, <span class="nu">32</span>), nn.<span class="fn">ReLU</span>(), nn.<span class="fn">Dropout</span>(drop),
            nn.<span class="fn">Linear</span>(<span class="nu">32</span>, <span class="nu">1</span>), nn.<span class="fn">Sigmoid</span>()
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(<span class="nb">self</span>, x):
        out, _ = <span class="nb">self</span>.lstm(x)
        w = torch.<span class="fn">softmax</span>(<span class="nb">self</span>.attn(out).<span class="fn">squeeze</span>(-<span class="nu">1</span>), dim=<span class="nu">1</span>)
        ctx = torch.<span class="fn">bmm</span>(w.<span class="fn">unsqueeze</span>(<span class="nu">1</span>), out).<span class="fn">squeeze</span>(<span class="nu">1</span>)
        <span class="kw">return</span> <span class="nb">self</span>.head(ctx)

<span class="cm"># ============================================================</span>
<span class="cm"># 5. í•™ìŠµ</span>
<span class="cm"># ============================================================</span>
torch.<span class="fn">manual_seed</span>(<span class="nu">42</span>)
model = <span class="nb">KospiLSTM</span>(n_feat=<span class="nb">len</span>(features))
criterion = nn.<span class="fn">BCELoss</span>()
optimizer = optim.<span class="fn">Adam</span>(model.<span class="fn">parameters</span>(), lr=<span class="nu">0.001</span>, weight_decay=<span class="nu">1e-5</span>)
scheduler = optim.lr_scheduler.<span class="fn">ReduceLROnPlateau</span>(optimizer, patience=<span class="nu">5</span>, factor=<span class="nu">0.5</span>)

train_loader = <span class="nb">DataLoader</span>(
    <span class="nb">TensorDataset</span>(torch.<span class="fn">FloatTensor</span>(X_tr), torch.<span class="fn">FloatTensor</span>(y_tr)),
    batch_size=<span class="nu">64</span>, shuffle=<span class="kw">True</span>
)

best_val = <span class="nb">float</span>(<span class="st">'inf'</span>)
patience_cnt = <span class="nu">0</span>

<span class="kw">for</span> epoch <span class="kw">in</span> <span class="nb">range</span>(<span class="nu">100</span>):
    model.<span class="fn">train</span>()
    <span class="kw">for</span> xb, yb <span class="kw">in</span> train_loader:
        pred = model(xb).<span class="fn">squeeze</span>()
        loss = <span class="fn">criterion</span>(pred, yb)
        optimizer.<span class="fn">zero_grad</span>()
        loss.<span class="fn">backward</span>()
        nn.utils.<span class="fn">clip_grad_norm_</span>(model.<span class="fn">parameters</span>(), <span class="nu">1.0</span>)
        optimizer.<span class="fn">step</span>()
    
    model.<span class="fn">eval</span>()
    <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
        vp = model(torch.<span class="fn">FloatTensor</span>(X_va)).<span class="fn">squeeze</span>()
        vl = <span class="fn">criterion</span>(vp, torch.<span class="fn">FloatTensor</span>(y_va)).<span class="fn">item</span>()
    scheduler.<span class="fn">step</span>(vl)
    
    <span class="kw">if</span> vl < best_val:
        best_val = vl
        patience_cnt = <span class="nu">0</span>
        torch.<span class="fn">save</span>(model.<span class="fn">state_dict</span>(), <span class="st">'kospi_lstm.pt'</span>)
    <span class="kw">else</span>:
        patience_cnt += <span class="nu">1</span>
        <span class="kw">if</span> patience_cnt >= <span class="nu">15</span>:
            <span class="fn">print</span>(<span class="st">f"Early stop at epoch {epoch+1}"</span>)
            <span class="kw">break</span>

<span class="cm"># ============================================================</span>
<span class="cm"># 6. í…ŒìŠ¤íŠ¸ í‰ê°€ + ë°±í…ŒìŠ¤íŠ¸</span>
<span class="cm"># ============================================================</span>
model.<span class="fn">load_state_dict</span>(torch.<span class="fn">load</span>(<span class="st">'kospi_lstm.pt'</span>, weights_only=<span class="kw">True</span>))
model.<span class="fn">eval</span>()
<span class="kw">with</span> torch.<span class="fn">no_grad</span>():
    test_prob = model(torch.<span class="fn">FloatTensor</span>(X_te)).<span class="fn">squeeze</span>().<span class="fn">numpy</span>()

test_pred = (test_prob > <span class="nu">0.5</span>).<span class="fn">astype</span>(<span class="nb">int</span>)
acc = <span class="fn">accuracy_score</span>(y_te, test_pred)
f1 = <span class="fn">f1_score</span>(y_te, test_pred)
auc = <span class="fn">roc_auc_score</span>(y_te, test_prob)

<span class="fn">print</span>(<span class="st">f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ==="</span>)
<span class="fn">print</span>(<span class="st">f"Accuracy: {acc:.2%}"</span>)
<span class="fn">print</span>(<span class="st">f"F1 Score: {f1:.4f}"</span>)
<span class="fn">print</span>(<span class="st">f"AUC-ROC:  {auc:.4f}"</span>)

<span class="cm"># ê°„ë‹¨ ë°±í…ŒìŠ¤íŠ¸: ìƒìŠ¹ ì˜ˆì¸¡ ì‹œ ë¡±, í•˜ë½ ì˜ˆì¸¡ ì‹œ ìˆ</span>
positions = np.<span class="fn">where</span>(test_pred == <span class="nu">1</span>, <span class="nu">1</span>, -<span class="nu">1</span>)
<span class="cm"># 5ì¼ í›„ ì‹¤ì œ ìˆ˜ìµë¥ </span>
actual_returns = np.<span class="fn">where</span>(y_te == <span class="nu">1</span>, <span class="nu">0.005</span>, -<span class="nu">0.005</span>)  <span class="cm"># ë‹¨ìˆœí™”</span>
strategy_returns = positions * actual_returns
cum_returns = (<span class="nu">1</span> + strategy_returns).<span class="fn">cumprod</span>()

sharpe = np.<span class="fn">mean</span>(strategy_returns) / np.<span class="fn">std</span>(strategy_returns) * np.<span class="fn">sqrt</span>(<span class="nu">52</span>)
<span class="fn">print</span>(<span class="st">f"\n=== ë°±í…ŒìŠ¤íŠ¸ ==="</span>)
<span class="fn">print</span>(<span class="st">f"ëˆ„ì  ìˆ˜ìµë¥ : {cum_returns[-1]-1:.2%}"</span>)
<span class="fn">print</span>(<span class="st">f"ìƒ¤í”„ë¹„ìœ¨ (ì—°í™˜ì‚°): {sharpe:.2f}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
KOSPI ë°ì´í„°: 1465ì¼, 2020-01-02 ~ 2025-11-14
í”¼ì²˜ ìˆ˜: 10
ìœ íš¨ ë°ì´í„°: 1401ì¼

Train: (936, 20, 10), Val: (186, 20, 10), Test: (186, 20, 10)
Early stop at epoch 58

=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===
Accuracy: 56.45%
F1 Score: 0.5812
AUC-ROC:  0.5934

=== ë°±í…ŒìŠ¤íŠ¸ ===
ëˆ„ì  ìˆ˜ìµë¥ : 8.23%
ìƒ¤í”„ë¹„ìœ¨ (ì—°í™˜ì‚°): 0.87</div>

<div class="warn">
<p class="ni"><strong>âš ï¸ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ ì œì¶œ ì‹œ ì£¼ì˜ì‚¬í•­</strong></p>
<ul>
<li><strong>Look-ahead bias:</strong> ì‹œí€€ìŠ¤ ìƒì„± ì‹œ ë¯¸ë˜ ë°ì´í„°ê°€ ì…ë ¥ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ë¼. ìŠ¤ì¼€ì¼ë§ë„ í•™ìŠµ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œë§Œ fití•´ì•¼ í•œë‹¤.</li>
<li><strong>ê±°ë˜ ë¹„ìš©:</strong> ì‹¤ì „ì—ì„œëŠ” í¸ë„ 5~20bpì˜ ê±°ë˜ ë¹„ìš©ì„ ë°˜ì˜í•´ì•¼ í•œë‹¤. 5ì¼ ì£¼ê¸° ë¦¬ë°¸ëŸ°ì‹±ì´ë¯€ë¡œ ì—°ê°„ ì•½ 50íšŒ ê±°ë˜.</li>
<li><strong>ê³¼ì í•© ì ê²€:</strong> í•™ìŠµ ì •í™•ë„ê°€ 80%ì¸ë° í…ŒìŠ¤íŠ¸ê°€ 52%ë¼ë©´ ì‹¬ê°í•œ ê³¼ì í•©ì´ë‹¤. Dropout, weight decay, early stoppingì„ ì ê·¹ í™œìš©í•˜ë¼.</li>
<li><strong>í†µê³„ì  ìœ ì˜ì„±:</strong> 56%ì˜ ì •í™•ë„ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œì§€ ì´í•­ ê²€ì •ìœ¼ë¡œ í™•ì¸í•˜ë¼. í‘œë³¸ì´ ì‘ìœ¼ë©´ ìš°ì—°ì¼ ìˆ˜ ìˆë‹¤.</li>
</ul>
</div>

<div class="ok">
<p class="ni"><strong>ğŸ¯ í”„ë¡œì íŠ¸ í™•ì¥ ê³¼ì œ (ì„ íƒ)</strong></p>
<ol>
<li><strong>íšŒê·€ ëª¨ë¸:</strong> ë¶„ë¥˜ ëŒ€ì‹  5ì¼ í›„ ìˆ˜ìµë¥ ì„ ì§ì ‘ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ì˜ˆì¸¡ ìˆ˜ìµë¥  í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ í¬ì§€ì…˜ ì‚¬ì´ì§•í•˜ë¼.</li>
<li><strong>GRU ë¹„êµ:</strong> ë™ì¼ ì¡°ê±´ì—ì„œ LSTMê³¼ GRUì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ë¼. í•™ìŠµ ì†ë„, ìµœì¢… ì •í™•ë„, íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í‘œë¡œ ì •ë¦¬í•˜ë¼.</li>
<li><strong>CNN+LSTM í•˜ì´ë¸Œë¦¬ë“œ:</strong> 1D CNNìœ¼ë¡œ ë‹¨ê¸° íŒ¨í„´ì„ ì¶”ì¶œí•œ ë’¤ LSTMì— ì…ë ¥í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ êµ¬í˜„í•˜ë¼.</li>
<li><strong>ë©€í‹° ìì‚°:</strong> KOSPI ì™¸ì— S&P500, ë‹›ì¼€ì´225ë¥¼ ì¶”ê°€í•˜ì—¬ ë©€í‹° ìì‚° ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•˜ë¼.</li>
</ol>
<p class="ni" style="margin-top:10px"><strong>ì œì¶œ í˜•ì‹:</strong> Jupyter Notebook (.ipynb) ë˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ (.py) + ê²°ê³¼ ì°¨íŠ¸ PNG + ì„±ê³¼ ìš”ì•½ í‘œ</p>
</div>


<h3>13.3 R1~R7 í•™ìŠµ ì—¬ì • íšŒê³ </h3>

<div style="margin:20px 0;padding:20px 25px;background:linear-gradient(135deg,#e8f5e9,#e3f2fd);border-radius:12px;border-left:5px solid #2e7d32">
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f">
R1ì—ì„œ íŒŒì´ì¬ì˜ <code>print("Hello, World!")</code>ë¡œ ì‹œì‘í•œ ì—¬ì •ì´ R7ì—ì„œ LSTMì˜ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ê¹Œì§€ ë„ë‹¬í–ˆë‹¤. ëŒì•„ë³´ë©´ ë†€ë¼ìš´ ì§„ì „ì´ë‹¤.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R1~R3ì—ì„œ ê¸°ì´ˆ ì²´ë ¥ì„ ìŒ“ì•˜ë‹¤. íŒŒì´ì¬ ë¬¸ë²•, NumPy/Pandas, ì„ í˜•ëŒ€ìˆ˜, í™•ë¥ /í†µê³„, ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ â€” ì´ ëª¨ë“  ê²ƒì´ ë”¥ëŸ¬ë‹ì˜ í† ëŒ€ê°€ ë˜ì—ˆë‹¤. R2ì—ì„œ ë°°ìš´ í¸ë¯¸ë¶„ê³¼ ì²´ì¸ë£°ì´ ì—­ì „íŒŒì˜ ìˆ˜í•™ì  ê¸°ë°˜ì´ ë˜ê³ , R2ì˜ í–‰ë ¬ê³±ì´ ì‹ ê²½ë§ì˜ ìˆœì „íŒŒ ì—°ì‚° ê·¸ ìì²´ì˜€ë‹¤.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R4~R6ì—ì„œ MLì˜ í•µì‹¬ ë¬´ê¸°ë¥¼ ê°–ì·„ë‹¤. R4ì˜ ì§€ë„í•™ìŠµ(íšŒê·€/ë¶„ë¥˜)ì€ ì‹ ê²½ë§ì˜ ì¶œë ¥ì¸µ ì„¤ê³„ì™€ ì†ì‹¤ í•¨ìˆ˜ ì„ íƒìœ¼ë¡œ ì§ê²°ë˜ê³ , R5ì˜ ì‹œê³„ì—´ ë¶„ì„(ARIMA/GARCH)ì€ RNN/LSTMì´ ëŒ€ì²´í•˜ë ¤ëŠ” ì „í†µì  ì ‘ê·¼ë²•ì´ë©°, R6ì˜ NLP(Transformer/BERT)ëŠ” ì‚¬ì‹¤ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì˜ ì •ì ì´ì—ˆë‹¤.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R7ì—ì„œ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ì„ ê´€í†µí–ˆë‹¤:
</p>
<ul style="color:#37474f;line-height:1.9;margin-top:5px">
<li><strong>ANN/MLP (Ch.2~5):</strong> í¼ì…‰íŠ¸ë¡  â†’ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  â†’ ì—­ì „íŒŒ â€” ì‹ ê²½ë§ì˜ ê¸°ë³¸ ì›ë¦¬</li>
<li><strong>ìµœì í™” (Ch.6):</strong> SGD â†’ Momentum â†’ Adam â€” íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•</li>
<li><strong>PyTorch (Ch.7):</strong> í…ì„œ, autograd, nn.Module â€” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ì‹¤ì „ ì‚¬ìš©ë²•</li>
<li><strong>CNN (Ch.8~9):</strong> í•©ì„±ê³±, í’€ë§, ìº”ë“¤ì°¨íŠ¸ íŒ¨í„´ ì¸ì‹ â€” ê³µê°„ì  íŒ¨í„´ ì¶”ì¶œ</li>
<li><strong>RNN/LSTM/GRU (Ch.10~12):</strong> ì‹œí€€ìŠ¤ ëª¨ë¸ë§, ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜, ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡</li>
</ul>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
ì´ì œ ìš°ë¦¬ì˜ ë¬´ê¸°ê³ ì—ëŠ” ì „í†µ MLê³¼ ë”¥ëŸ¬ë‹ì´ ëª¨ë‘ ê°–ì¶°ì¡Œë‹¤. R8ì—ì„œëŠ” ì´ ë¬´ê¸°ë“¤ì„ <strong>Convex Optimization</strong>ìœ¼ë¡œ ìµœì í™”í•˜ê³ , <strong>Transformer</strong>ë¼ëŠ” í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ ìµœê°• ì•„í‚¤í…ì²˜ë¥¼ ë‹¤ë£¬ë‹¤.
</p>
</div>

<h3>13.4 ë‹¤ìŒ ë¼ìš´ë“œ ì˜ˆê³ : Round 8 â€” Convex Optimization + Transformer</h3>

<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff3e0,#fce4ec);border-radius:10px;border:2px solid #e65100">
<p class="ni" style="text-align:center;font-weight:bold;font-size:15px;margin-bottom:15px;color:#bf360c">
ğŸ”® Round 8 Preview â€” Convex Optimization + Transformer</p>

<div style="display:flex;align-items:center;justify-content:center;gap:8px;flex-wrap:wrap;margin-bottom:15px">
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:110px">
<div style="font-size:22px;margin-bottom:4px">ğŸ“</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">Convex Set</div>
<div style="color:#888;font-size:10px">ë³¼ë¡ì§‘í•©/ë³¼ë¡í•¨ìˆ˜</div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:110px">
<div style="font-size:22px;margin-bottom:4px">ğŸ¯</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">KKT / Lagrange</div>
<div style="color:#888;font-size:10px">ìµœì í™” ì¡°ê±´</div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:110px">
<div style="font-size:22px;margin-bottom:4px">ğŸ’¼</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">CVXPY</div>
<div style="color:#888;font-size:10px">í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”</div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:110px">
<div style="font-size:22px;margin-bottom:4px">âš¡</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">Transformer</div>
<div style="color:#888;font-size:10px">Self-Attention</div>
</div>
</div>

<div style="display:flex;flex-wrap:wrap;gap:10px;justify-content:center;font-size:12px;margin-top:10px">
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ“Š Mean-Variance</div>
<div style="color:#777;font-size:10px;margin-top:3px">ë§ˆì½”ìœ„ì¸  í¬íŠ¸í´ë¦¬ì˜¤</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">âš–ï¸ Risk Parity</div>
<div style="color:#777;font-size:10px;margin-top:3px">ë¦¬ìŠ¤í¬ ê· ë“± ë°°ë¶„</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ§  Self-Attention</div>
<div style="color:#777;font-size:10px;margin-top:3px">ì‹œí€€ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ“ˆ ê¸ˆìœµ Transformer</div>
<div style="color:#777;font-size:10px;margin-top:3px">ìˆ˜ìµë¥  ì˜ˆì¸¡</div>
</div>
</div>

<p class="ni" style="text-align:center;margin-top:15px;color:#555;font-size:12px;line-height:1.7">
R7ì—ì„œ LSTMìœ¼ë¡œ ì‹œê³„ì—´ì„ ì˜ˆì¸¡í–ˆë‹¤ë©´, R8ì—ì„œëŠ” <strong>Transformer</strong>ë¡œ ë” ê°•ë ¥í•œ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ê³ ,<br>
<strong>Convex Optimization</strong>ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìµœì ì˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ë³€í™˜í•œë‹¤.<br>
<strong>ë¯¸ë‹ˆ í”„ë¡œì íŠ¸:</strong> CVXPYë¡œ ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” í¬íŠ¸í´ë¦¬ì˜¤ + Transformer ìˆ˜ìµë¥  ì˜ˆì¸¡
</p>
</div>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™ (Round 8 ë¯¸ë¦¬ë³´ê¸°)</strong></p>
<p class="ni" style="margin-top:8px">
MLAT Ch.5 "Portfolio Optimization and Strategy Evaluation"ì—ì„œ ì „ëµ í‰ê°€ì™€ ë°±í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼,
Ch.20~21 "Autoencoders / Generative Adversarial Nets"ì—ì„œ Attention ë©”ì»¤ë‹ˆì¦˜ê³¼ Transformer ì•„í‚¤í…ì²˜ë¥¼ ë‹¤ë£¬ë‹¤.
MLDSF Ch.7~8ì—ì„œëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ì¼€ì´ìŠ¤ìŠ¤í„°ë””ë¥¼ ì œê³µí•œë‹¤.
Convex Optimization ì´ë¡ ì€ Boyd & Vandenbergheì˜ êµê³¼ì„œë¥¼ ì°¸ê³ í•˜ë˜, ê¸ˆìœµ ì ìš©ì— í•„ìš”í•œ í•µì‹¬ë§Œ ë‹¤ë£¬ë‹¤.
</p>
</div>

<div class="info">
<p class="ni"><strong>ğŸ”„ Round 7 í•µì‹¬ ìš”ì•½</strong></p>
<p class="ni" style="margin-top:8px">ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ìš°ë¦¬ëŠ” <strong>ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆë¶€í„° ê¸ˆìœµ ì ìš©ê¹Œì§€</strong>ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë‹¤ë¤˜ë‹¤:</p>
<ul>
<li><strong>ANN ê¸°ì´ˆ:</strong> í¼ì…‰íŠ¸ë¡  â†’ MLP â†’ ì—­ì „íŒŒ â†’ ê²½ì‚¬í•˜ê°•ë²• â€” ì‹ ê²½ë§ì˜ í•™ìŠµ ì›ë¦¬</li>
<li><strong>í™œì„±í™” í•¨ìˆ˜:</strong> Sigmoid, Tanh, ReLU, Leaky ReLU â€” ë¹„ì„ í˜•ì„±ì˜ ì—´ì‡ </li>
<li><strong>PyTorch:</strong> í…ì„œ, autograd, nn.Module, DataLoader â€” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ì‹¤ì „</li>
<li><strong>CNN:</strong> í•©ì„±ê³±, í’€ë§, 1D/2D CNN â€” ê³µê°„ì /ì‹œê°„ì  íŒ¨í„´ ì¶”ì¶œ</li>
<li><strong>RNN/LSTM/GRU:</strong> ì‹œí€€ìŠ¤ ëª¨ë¸ë§, ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜, ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ</li>
<li><strong>ê¸ˆìœµ ì ìš©:</strong> ìº”ë“¤ì°¨íŠ¸ CNN, LSTM ì£¼ê°€ ì˜ˆì¸¡, Attention ë©”ì»¤ë‹ˆì¦˜</li>
</ul>
<p class="ni" style="margin-top:8px">R8ì—ì„œëŠ” Convex Optimizationìœ¼ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ìµœì í™”í•˜ê³ , Transformerë¡œ LSTMì„ ë„˜ì–´ì„œëŠ” ì‹œí€€ìŠ¤ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤.</p>
</div>

</div><!-- paper-content -->
</div><!-- container -->
</div><!-- main-wrapper -->

</body>
</html>
