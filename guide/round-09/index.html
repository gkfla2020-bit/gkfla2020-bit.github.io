<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Round 9 - HFT System Design + Reinforcement Learning</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300;400;500&family=Space+Mono:wght@400&family=Inter:wght@300;400&display=swap" rel="stylesheet">
<script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:'Inter',sans-serif;background:#fafaf8;color:#1a1a1a;line-height:1.7;overflow-x:hidden}
.sidebar{position:fixed;left:0;top:0;width:260px;height:100vh;background:rgba(255,255,255,.97);border-right:1px solid rgba(0,0,0,.06);padding:32px 24px;z-index:100;overflow-y:auto;display:flex;flex-direction:column}
.sidebar-profile{text-align:center;margin-bottom:28px;padding-bottom:24px;border-bottom:1px solid rgba(0,0,0,.08)}
.profile-icon{font-size:48px;margin-bottom:8px}
.profile-name{font-family:'Cormorant Garamond',serif;font-size:1.3rem;font-weight:500;margin-bottom:4px}
.profile-title{font-size:.68rem;color:#888;letter-spacing:.08em;text-transform:uppercase;margin-bottom:8px}
.profile-bio{font-size:.78rem;color:#666;line-height:1.5}
.sidebar-nav{flex:1;margin-top:16px}
.nav-section{margin-bottom:20px}
.nav-section-title{font-size:.6rem;font-weight:600;color:#aaa;letter-spacing:.15em;text-transform:uppercase;margin-bottom:10px}
.nav-list{list-style:none}
.nav-list li{margin-bottom:5px}
.nav-list a{font-size:.78rem;color:#555;text-decoration:none;transition:all .2s;display:block;padding:3px 0}
.nav-list a:hover{color:#0080c6;padding-left:4px}
.nav-list a.active{color:#0080c6;font-weight:500}
.nav-list a.done{color:#28a745}
.badge{display:inline-block;font-size:.5rem;background:#0080c6;color:#fff;padding:1px 5px;border-radius:8px;margin-left:3px;vertical-align:middle}
.badge-done{background:#28a745}
.sidebar-footer{padding-top:16px;border-top:1px solid rgba(0,0,0,.06);font-size:.65rem;color:#aaa;text-align:center}
.main-wrapper{margin-left:260px;min-height:100vh}
.container{max-width:1100px;margin:0 auto;padding:50px 40px 80px}
.paper-content{font-family:'Times New Roman','Nanum Myeongjo',serif;line-height:1.8;background:#fff;padding:40px;border-radius:8px;box-shadow:0 2px 20px rgba(0,0,0,.05)}
.paper-header{text-align:center;margin-bottom:40px;padding-bottom:30px;border-bottom:2px solid #333}
.paper-category{font-size:14px;color:#666;margin-bottom:10px}
.paper-title{font-size:24px;font-weight:bold;margin-bottom:12px;line-height:1.4}
.paper-subtitle{font-size:14px;color:#555;margin-bottom:8px}
.paper-team{font-size:13px;color:#444}
.code-output{background:#1e1e1e;color:#d4d4d4;padding:12px 16px;border-radius:0 0 6px 6px;font-family:'Space Mono',monospace;font-size:11.5px;line-height:1.6;margin-top:-4px;margin-bottom:18px;border-top:2px solid #333;white-space:pre-wrap;overflow-x:auto}
.code-output .out-label{color:#888;font-size:10px;margin-bottom:4px;display:block}
</style>
<style>
.abstract{background:#f8f9fa;padding:25px;margin:30px 0;border-left:4px solid #2c3e50}
.abstract-title{font-weight:bold;font-size:16px;margin-bottom:15px}
h2{font-size:18px;margin:35px 0 20px;padding-bottom:8px;border-bottom:1px solid #ddd;color:#2c3e50}
h3{font-size:15px;margin:25px 0 15px;color:#34495e}
h4{font-size:14px;margin:20px 0 12px;color:#34495e}
p{text-align:justify;margin-bottom:15px;text-indent:2em}
p.ni{text-indent:0}
table{width:100%;border-collapse:collapse;margin:20px 0;font-size:12px}
th,td{border:1px solid #ddd;padding:10px 8px;text-align:center}
th{background:#2c3e50;color:white;font-weight:bold}
tr:nth-child(even){background:#f8f9fa}
tr:hover{background:#e8f4f8}
.tc{font-size:13px;font-weight:bold;margin:15px 0 10px;text-align:center}
.eq{text-align:center;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:4px;overflow-x:auto}
ul,ol{margin-left:2em;margin-bottom:15px}
li{margin-bottom:6px}
.def{background:#fff9e6;border:1px solid #ffc107;border-radius:4px;padding:20px;margin:20px 0}
.info{background:#e8f4f8;border-left:4px solid #3498db;padding:20px;margin:20px 0}
.warn{background:#fff3cd;border-left:4px solid #f39c12;padding:20px;margin:20px 0}
.ok{background:#d4edda;border-left:4px solid #28a745;padding:20px;margin:20px 0}
pre{background:#1e1e1e;color:#d4d4d4;padding:20px;border-radius:6px;overflow-x:auto;margin:20px 0;font-family:'Space Mono','Consolas',monospace;font-size:13px;line-height:1.6}
code{font-family:'Space Mono','Consolas',monospace;font-size:13px}
p code,li code,td code{background:#f0f0f0;padding:2px 6px;border-radius:3px;color:#c7254e;font-size:12px}
.cc{font-size:12px;font-weight:bold;color:#2c3e50;margin-top:15px;margin-bottom:4px}
.cm{color:#6a9955}.kw{color:#569cd6}.st{color:#ce9178}.fn{color:#dcdcaa}.nb{color:#4ec9b0}.nu{color:#b5cea8}
.progress-bar{width:100%;height:6px;background:#e0e0e0;border-radius:3px;margin-top:16px}
.progress-fill{height:100%;background:linear-gradient(90deg,#0080c6,#00b894);border-radius:3px;width:90%}
.progress-label{font-size:11px;color:#888;margin-top:4px;text-align:center}
@media(max-width:1024px){
.sidebar{width:100%;height:auto;position:relative;border-right:none;border-bottom:1px solid rgba(0,0,0,.08);padding:16px}
.sidebar-profile{margin-bottom:10px;padding-bottom:10px;display:flex;align-items:center;gap:12px;text-align:left}
.profile-icon{font-size:32px;margin-bottom:0}.profile-bio{display:none}
.nav-section{display:inline-block;margin-right:16px;margin-bottom:8px}
.nav-list{display:flex;gap:10px;flex-wrap:wrap}.nav-list li{margin-bottom:0}
.sidebar-footer{display:none}
.main-wrapper{margin-left:0}
.container{padding:0}.paper-content{padding:20px 16px;border-radius:0;box-shadow:none}
.paper-title{font-size:18px}p{font-size:14px;text-indent:1.5em;text-align:left}
pre{font-size:11px;padding:14px}table{font-size:10px;display:block;overflow-x:auto}
}
</style>
</head>
<body>

<div class="sidebar">
<div class="sidebar-profile">
<div class="profile-icon">&#x26A1;</div>
<div class="profile-name">HFT ML Master Plan</div>
<div class="profile-title">Convex Opt + DL + HFT</div>
<div class="profile-bio">10 Rounds: Zero to HFT System Trading</div>
</div>
<div class="sidebar-nav">
<div class="nav-section">
<div class="nav-section-title">Curriculum</div>
<ul class="nav-list">
<li><a class="done" href="../round-01/">R1. Python + Finance <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-02/">R2. Linear Algebra + Stats <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-03/">R3. Data / Feature Eng. <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-04/">R4. Supervised Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-05/">R5. Unsupervised + TS <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-06/">R6. NLP + Sentiment <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-07/">R7. Deep Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-08/">R8. Convex Opt + Transformer <span class="badge badge-done">DONE</span></a></li>
<li><a class="active" href="#">R9. HFT + RL <span class="badge">NOW</span></a></li>
<li><a href="../round-10/">R10. Final Project</a></li>
</ul>
</div>
<div class="nav-section">
<div class="nav-section-title">This Lecture</div>
<ul class="nav-list">
<li><a href="#ch1">1. ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜</a></li>
<li><a href="#ch2">2. ì˜¤ë”ë¶ ì—­í•™</a></li>
<li><a href="#ch3">3. HFT ì „ëµ ìœ í˜•</a></li>
<li><a href="#ch4">4. ì €ì§€ì—° ì•„í‚¤í…ì²˜</a></li>
<li><a href="#ch5">5. ê°•í™”í•™ìŠµ ê¸°ì´ˆ (MDP)</a></li>
<li><a href="#ch6">6. Q-Learning</a></li>
<li><a href="#ch7">7. DQN</a></li>
<li><a href="#ch8">8. Policy Gradient</a></li>
<li><a href="#ch9">9. Actor-Critic &amp; PPO</a></li>
<li><a href="#ch10">10. íŠ¸ë ˆì´ë”© í™˜ê²½ ì„¤ê³„</a></li>
<li><a href="#ch11">11. DQN íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸</a></li>
<li><a href="#ch12">12. ì‹¤ì „ íŒŒì´í”„ë¼ì¸</a></li>
<li><a href="#ch13">13. í”¼ë“œë°± + Quiz</a></li>
</ul>
</div>
</div>
<div class="sidebar-footer">Round 9 of 10 Â· âš¡ HFT + RL</div>
</div>

<div class="main-wrapper">
<div class="container">
<div class="paper-content">

<div class="paper-header">
<div class="paper-category">Round 9 / 10 Â· HFT ì‹œìŠ¤í…œ + ê°•í™”í•™ìŠµ</div>
<h1 class="paper-title">HFT System Design &amp; Reinforcement Learning<br>for Algorithmic Trading</h1>
<div class="paper-subtitle">ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ë¥¼ ì´í•´í•˜ê³ , ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œ ë§¤ë§¤ë¥¼ í•™ìŠµí•˜ê²Œ í•œë‹¤</div>
<div class="paper-team">Textbooks: MLAT Ch.22~23 / MLDSF Ch.13~14</div>
<div class="progress-bar"><div class="progress-fill"></div></div>
<div class="progress-label">Overall Progress: 90%</div>
</div>

<div class="abstract">
<div class="abstract-title">Abstract</div>
<p class="ni">
R1~R8ê¹Œì§€ ìš°ë¦¬ëŠ” ë°ì´í„° ìˆ˜ì§‘(R1,R3), ìˆ˜í•™ì  ê¸°ë°˜(R2), ì „í†µ ML(R4~R5), NLP(R6), ë”¥ëŸ¬ë‹(R7), í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì™€ Transformer(R8)ë¥¼ ëª¨ë‘ ìµí˜”ë‹¤. ì´ì œ ë§ˆì§€ë§‰ í¼ì¦ ë‘ ì¡°ê°ì„ ë§ì¶˜ë‹¤. ì²«ì§¸, <strong>ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜</strong> â€” ì£¼ë¬¸ì´ ì–´ë–»ê²Œ ì²´ê²°ë˜ëŠ”ì§€, ì˜¤ë”ë¶ì´ ì–´ë–»ê²Œ ì›€ì§ì´ëŠ”ì§€, HFT ì‹œìŠ¤í…œì´ ì–´ë–¤ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ëŠ”ì§€ë¥¼ ì´í•´í•œë‹¤. ë‘˜ì§¸, <strong>ê°•í™”í•™ìŠµ(Reinforcement Learning)</strong> â€” ì—ì´ì „íŠ¸ê°€ ì‹œì¥ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ ë§¤ë§¤ ì „ëµì„ í•™ìŠµí•˜ëŠ” íŒ¨ëŸ¬ë‹¤ì„ì´ë‹¤.
</p>
<p class="ni" style="margin-top:10px">
R4~R7ì˜ ì§€ë„í•™ìŠµ/ë”¥ëŸ¬ë‹ì€ "ê³¼ê±° ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµ"í•˜ëŠ” ë°©ì‹ì´ì—ˆë‹¤. ê°•í™”í•™ìŠµì€ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥´ë‹¤ â€” ì—ì´ì „íŠ¸ê°€ <strong>í–‰ë™(action)ì„ ì·¨í•˜ê³ , ë³´ìƒ(reward)ì„ ë°›ìœ¼ë©°, ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ì „ëµì„ ê°œì„ </strong>í•œë‹¤. ì´ê²ƒì€ ì‹¤ì œ íŠ¸ë ˆì´ë”ê°€ ì‹œì¥ì—ì„œ ê²½í—˜ì„ ìŒ“ì•„ê°€ëŠ” ê³¼ì •ê³¼ ë³¸ì§ˆì ìœ¼ë¡œ ë™ì¼í•˜ë‹¤. Q-Learningì—ì„œ DQN, Policy Gradient, PPOê¹Œì§€ â€” ê°•í™”í•™ìŠµì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ë°°ìš°ê³ , ì´ë¥¼ íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ë¡œ êµ¬í˜„í•œë‹¤.
</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 1: ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch1">Chapter 1. ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ â€” ì£¼ë¬¸ì´ ì²´ê²°ë˜ëŠ” ì„¸ê³„</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Market Microstructure" ë„ì…ë¶€ / ë‘ì‡ì•Œê³  Ch.4 í(Queue) ìë£Œêµ¬ì¡° â€” ì˜¤ë”ë¶ì€ ë³¸ì§ˆì ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ íë‹¤</p>
</div>

<h3>1.1 ì™œ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ë¥¼ ì•Œì•„ì•¼ í•˜ëŠ”ê°€</h3>

<p>
R1~R8ì—ì„œ ìš°ë¦¬ëŠ” "ì¢…ê°€(Close)"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì „ëµì„ ë§Œë“¤ì—ˆë‹¤. í•˜ë£¨ì— í•˜ë‚˜ì˜ ê°€ê²©. í•˜ì§€ë§Œ ì‹¤ì œ ì‹œì¥ì—ì„œëŠ” 1ì´ˆì— ìˆ˜ì²œ ê±´ì˜ ì£¼ë¬¸ì´ ì˜¤ê°€ê³ , ê°€ê²©ì€ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë³€í•œë‹¤. "100ë‹¬ëŸ¬ì— AAPLì„ ì‚¬ê² ë‹¤"ëŠ” ì£¼ë¬¸ì´ ì‹¤ì œë¡œ 100ë‹¬ëŸ¬ì— ì²´ê²°ë ê¹Œ? ì•„ë‹ ìˆ˜ë„ ìˆë‹¤. ì£¼ë¬¸ì„ ë„£ëŠ” ìˆœê°„ ê°€ê²©ì´ ì›€ì§ì¼ ìˆ˜ ìˆê³ (ìŠ¬ë¦¬í”¼ì§€), ì›í•˜ëŠ” ìˆ˜ëŸ‰ë§Œí¼ ì²´ê²°ë˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤(ë¶€ë¶„ ì²´ê²°). ì´ëŸ° í˜„ì‹¤ì„ ì´í•´í•˜ì§€ ëª»í•˜ë©´, ë°±í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ìˆ˜ìµì´ ë‚˜ì§€ë§Œ ì‹¤ì „ì—ì„œëŠ” ì†ì‹¤ì´ ë‚˜ëŠ” "ë°±í…ŒìŠ¤íŠ¸ í™˜ìƒ"ì— ë¹ ì§„ë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ (Market Microstructure) ì •ì˜</strong></p>
<p class="ni" style="margin-top:8px">
ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ëŠ” ê¸ˆìœµ ìì‚°ì˜ ê±°ë˜ê°€ ì´ë£¨ì–´ì§€ëŠ” ê³¼ì •ê³¼ ë©”ì»¤ë‹ˆì¦˜ì„ ì—°êµ¬í•˜ëŠ” ë¶„ì•¼ë‹¤. ì£¼ë¬¸ì˜ ìœ í˜•, ê°€ê²© í˜•ì„± ê³¼ì •, ì •ë³´ì˜ ë¹„ëŒ€ì¹­, ê±°ë˜ ë¹„ìš©, ì‹œì¥ ì°¸ì—¬ìì˜ í–‰ë™ ë“±ì„ ë‹¤ë£¬ë‹¤. HFT(High-Frequency Trading)ëŠ” ì´ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ì˜ ë¯¸ì„¸í•œ ë¹„íš¨ìœ¨ì„±ì„ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ í¬ì°©í•˜ì—¬ ìˆ˜ìµì„ ì¶”êµ¬í•˜ëŠ” ì „ëµì´ë‹¤.
</p>
</div>

<h3>1.2 ì£¼ë¬¸ ìœ í˜•: ì‹œì¥ê°€ vs ì§€ì •ê°€</h3>

<p>
ëª¨ë“  ê±°ë˜ëŠ” ì£¼ë¬¸(order)ì—ì„œ ì‹œì‘ëœë‹¤. ì£¼ë¬¸ì—ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ìœ í˜•ì´ ìˆë‹¤:
</p>

<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #e74c3c">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#e74c3c;margin-bottom:10px">ğŸ”´ ì‹œì¥ê°€ ì£¼ë¬¸ (Market Order)</p>
<p class="ni" style="font-size:12px;margin-bottom:6px">ğŸ“‹ "ì§€ê¸ˆ ë‹¹ì¥, ìµœì„ ì˜ ê°€ê²©ì— ì²´ê²°í•´ì¤˜"</p>
<p class="ni" style="font-size:12px;margin-bottom:6px">âš¡ ì¦‰ì‹œ ì²´ê²° ë³´ì¥, ê°€ê²© ë¶ˆí™•ì‹¤</p>
<p class="ni" style="font-size:12px;margin-bottom:6px">ğŸ’° ìœ ë™ì„±ì„ ì†Œë¹„(take)í•œë‹¤ â†’ Taker</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:8px;border-top:1px solid #eee;padding-top:8px">ì˜ˆ: "AAPL 100ì£¼ë¥¼ ì§€ê¸ˆ ë°”ë¡œ ì‚¬!" â†’ í˜„ì¬ ìµœìš°ì„  ë§¤ë„í˜¸ê°€ì— ì²´ê²°</p>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #27ae60">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#27ae60;margin-bottom:10px">ğŸŸ¢ ì§€ì •ê°€ ì£¼ë¬¸ (Limit Order)</p>
<p class="ni" style="font-size:12px;margin-bottom:6px">ğŸ“‹ "ì´ ê°€ê²© ì´í•˜(ë§¤ìˆ˜)/ì´ìƒ(ë§¤ë„)ì—ì„œë§Œ ì²´ê²°í•´ì¤˜"</p>
<p class="ni" style="font-size:12px;margin-bottom:6px">âš¡ ê°€ê²© ë³´ì¥, ì²´ê²° ë¶ˆí™•ì‹¤</p>
<p class="ni" style="font-size:12px;margin-bottom:6px">ğŸ’° ìœ ë™ì„±ì„ ê³µê¸‰(make)í•œë‹¤ â†’ Maker</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:8px;border-top:1px solid #eee;padding-top:8px">ì˜ˆ: "AAPLì„ $149.50 ì´í•˜ì—ì„œ 100ì£¼ ì‚¬!" â†’ ì˜¤ë”ë¶ì— ëŒ€ê¸°</p>
</div>
</div>

<h3>1.3 ì˜¤ë”ë¶ (Order Book) êµ¬ì¡°</h3>

<p>
ì˜¤ë”ë¶ì€ íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ëª¨ë“  ë¯¸ì²´ê²° ì§€ì •ê°€ ì£¼ë¬¸ì„ ê°€ê²©ë³„ë¡œ ì •ë ¬í•œ ì¥ë¶€ë‹¤. ë§¤ìˆ˜ ì£¼ë¬¸(bid)ì€ ë†’ì€ ê°€ê²©ë¶€í„°, ë§¤ë„ ì£¼ë¬¸(ask)ì€ ë‚®ì€ ê°€ê²©ë¶€í„° ì •ë ¬ëœë‹¤. ìµœìš°ì„  ë§¤ìˆ˜í˜¸ê°€(best bid)ì™€ ìµœìš°ì„  ë§¤ë„í˜¸ê°€(best ask)ì˜ ì°¨ì´ê°€ <strong>ìŠ¤í”„ë ˆë“œ(spread)</strong>ë‹¤.
</p>

<div class="eq">
\[ \text{Spread} = P_{\text{ask}}^{(1)} - P_{\text{bid}}^{(1)}, \quad \text{Mid Price} = \frac{P_{\text{ask}}^{(1)} + P_{\text{bid}}^{(1)}}{2} \]
</div>

<!-- â˜… Plotly: Interactive Order Book -->
<h3>1.x ğŸ“Š Interactive: ì‹¤ì‹œê°„ ì˜¤ë”ë¶ ì‹œê°í™”</h3>
<div id="plot-ch1-orderbook" style="width:100%;height:450px;margin:20px 0"></div>
<script>
(function(){
  var bidPrices=[149.50,149.45,149.40,149.35,149.30,149.25,149.20,149.15,149.10,149.05];
  var bidSizes=[200,350,500,180,420,600,250,380,700,150];
  var askPrices=[149.55,149.60,149.65,149.70,149.75,149.80,149.85,149.90,149.95,150.00];
  var askSizes=[150,280,450,320,550,200,380,620,180,400];
  // Cumulative
  var bidCum=[],askCum=[],bs=0,as=0;
  for(var i=0;i<10;i++){bs+=bidSizes[i];bidCum.push(bs);as+=askSizes[i];askCum.push(as);}
  Plotly.newPlot('plot-ch1-orderbook',[
    {x:bidPrices,y:bidSizes,type:'bar',name:'Bid (ë§¤ìˆ˜)',marker:{color:'rgba(46,204,113,0.7)'},width:0.04},
    {x:askPrices,y:askSizes,type:'bar',name:'Ask (ë§¤ë„)',marker:{color:'rgba(231,76,60,0.7)'},width:0.04},
    {x:bidPrices,y:bidCum,type:'scatter',mode:'lines+markers',name:'Bid ëˆ„ì ',line:{color:'#27ae60',width:2},marker:{size:5},yaxis:'y2'},
    {x:askPrices,y:askCum,type:'scatter',mode:'lines+markers',name:'Ask ëˆ„ì ',line:{color:'#e74c3c',width:2},marker:{size:5},yaxis:'y2'}
  ],{
    title:{text:'ğŸ“Š AAPL ì˜¤ë”ë¶ (Level 2): Bid/Ask í˜¸ê°€ë³„ ìˆ˜ëŸ‰ + ëˆ„ì  ê¹Šì´',font:{size:14}},
    xaxis:{title:'ê°€ê²© ($)',dtick:0.05,range:[149.0,150.05]},
    yaxis:{title:'ìˆ˜ëŸ‰ (ì£¼)',side:'left'},
    yaxis2:{title:'ëˆ„ì  ìˆ˜ëŸ‰',overlaying:'y',side:'right'},
    legend:{x:0.01,y:0.98,bgcolor:'rgba(255,255,255,0.85)'},
    shapes:[{type:'rect',x0:149.50,x1:149.55,y0:0,y1:750,fillcolor:'rgba(52,152,219,0.1)',line:{width:2,color:'#3498db',dash:'dash'}}],
    annotations:[{x:149.525,y:720,text:'Spread: $0.05',showarrow:false,font:{size:11,color:'#3498db',weight:'bold'}},
                 {x:149.50,y:250,text:'Best Bid',showarrow:true,arrowhead:2,ax:30,ay:-30,font:{size:10,color:'#27ae60'}},
                 {x:149.55,y:200,text:'Best Ask',showarrow:true,arrowhead:2,ax:-30,ay:-30,font:{size:10,color:'#e74c3c'}}],
    margin:{t:50,b:60},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff'
  },{responsive:true});
})();
</script>
<p class="ni">ğŸ–±ï¸ ì´ˆë¡ ë§‰ëŒ€=ë§¤ìˆ˜ ëŒ€ê¸° ì£¼ë¬¸, ë¹¨ê°„ ë§‰ëŒ€=ë§¤ë„ ëŒ€ê¸° ì£¼ë¬¸. ì„  ê·¸ë˜í”„ëŠ” ëˆ„ì  ê¹Šì´(depth)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŒŒë€ ì ì„  ì˜ì—­ì´ ìŠ¤í”„ë ˆë“œ($0.05)ì…ë‹ˆë‹¤. ì‹œì¥ê°€ ë§¤ìˆ˜ ì£¼ë¬¸ì„ ë„£ìœ¼ë©´ Best Ask($149.55)ì—ì„œ ì²´ê²°ë©ë‹ˆë‹¤.</p>

<h3>1.4 í•µì‹¬ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì§€í‘œ</h3>

<table>
<tr><th>ì§€í‘œ</th><th>ìˆ˜ì‹</th><th>ì˜ë¯¸</th><th>HFT í™œìš©</th></tr>
<tr><td>Bid-Ask Spread</td><td>\(P_{ask}^{(1)} - P_{bid}^{(1)}\)</td><td>ê±°ë˜ ë¹„ìš©ì˜ ì§ì ‘ì  ì¸¡ì •</td><td>ìŠ¤í”„ë ˆë“œê°€ ë„“ìœ¼ë©´ ë§ˆì¼“ë©”ì´í‚¹ ìˆ˜ìµ ê¸°íšŒ</td></tr>
<tr><td>Mid Price</td><td>\(\frac{P_{ask}^{(1)} + P_{bid}^{(1)}}{2}\)</td><td>"ê³µì • ê°€ê²©"ì˜ ì¶”ì •ì¹˜</td><td>ì‹œê·¸ë„ ìƒì„±ì˜ ê¸°ì¤€ì </td></tr>
<tr><td>Order Imbalance</td><td>\(\frac{V_{bid} - V_{ask}}{V_{bid} + V_{ask}}\)</td><td>ë§¤ìˆ˜/ë§¤ë„ ì••ë ¥ì˜ ë¶ˆê· í˜•</td><td>ë‹¨ê¸° ê°€ê²© ë°©í–¥ ì˜ˆì¸¡</td></tr>
<tr><td>VWAP</td><td>\(\frac{\sum p_i \cdot v_i}{\sum v_i}\)</td><td>ê±°ë˜ëŸ‰ ê°€ì¤‘ í‰ê·  ê°€ê²©</td><td>ëŒ€ëŸ‰ ì£¼ë¬¸ ì‹¤í–‰ ë²¤ì¹˜ë§ˆí¬</td></tr>
<tr><td>Kyle's Lambda</td><td>\(\lambda = \frac{\Delta p}{\Delta v}\)</td><td>ê°€ê²© ì¶©ê²© (Price Impact)</td><td>ì£¼ë¬¸ í¬ê¸° ìµœì í™”</td></tr>
</table>

<div class="warn">
<p class="ni"><strong>âš ï¸ ìŠ¬ë¦¬í”¼ì§€ (Slippage) â€” ë°±í…ŒìŠ¤íŠ¸ì˜ ìµœëŒ€ ì </strong></p>
<p class="ni" style="margin-top:8px">
ìŠ¬ë¦¬í”¼ì§€ëŠ” ì˜ˆìƒ ì²´ê²° ê°€ê²©ê³¼ ì‹¤ì œ ì²´ê²° ê°€ê²©ì˜ ì°¨ì´ë‹¤. ëŒ€ëŸ‰ ì‹œì¥ê°€ ì£¼ë¬¸ì„ ë„£ìœ¼ë©´ ì˜¤ë”ë¶ì˜ ì—¬ëŸ¬ í˜¸ê°€ë¥¼ ì†Œì§„í•˜ë©´ì„œ ê°€ê²©ì´ ë¶ˆë¦¬í•˜ê²Œ ì›€ì§ì¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 1,000ì£¼ë¥¼ ì‹œì¥ê°€ë¡œ ë§¤ìˆ˜í•˜ë©´, ì²˜ìŒ 200ì£¼ëŠ” $149.55ì—, ë‹¤ìŒ 280ì£¼ëŠ” $149.60ì—, ë‚˜ë¨¸ì§€ëŠ” ë” ë†’ì€ ê°€ê²©ì— ì²´ê²°ë  ìˆ˜ ìˆë‹¤. R10ì˜ ë°±í…ŒìŠ¤íŠ¸ì—ì„œ ìŠ¬ë¦¬í”¼ì§€ë¥¼ ë°˜ë“œì‹œ ëª¨ë¸ë§í•´ì•¼ í•œë‹¤.
</p>
</div>

<h3>1.5 ì •ë³´ì˜ ë¹„ëŒ€ì¹­ê³¼ ì—­ì„ íƒ</h3>

<p>
ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ì˜ í•µì‹¬ ì´ë¡  ì¤‘ í•˜ë‚˜ê°€ <strong>ì—­ì„ íƒ(adverse selection)</strong>ì´ë‹¤. ë§ˆì¼“ë©”ì´ì»¤ê°€ ì§€ì •ê°€ ì£¼ë¬¸ì„ ê±¸ì–´ë†“ìœ¼ë©´, ì •ë³´ë¥¼ ê°€ì§„ íŠ¸ë ˆì´ë”(informed trader)ê°€ ê·¸ ì£¼ë¬¸ì„ "ë”°ë¨¹ì„" ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë‹ ì„œí”„ë¼ì´ì¦ˆ ì§ì „ì— ë‚´ë¶€ ì •ë³´ë¥¼ ê°€ì§„ íŠ¸ë ˆì´ë”ê°€ ë§ˆì¼“ë©”ì´ì»¤ì˜ ë§¤ë„ ì£¼ë¬¸ì„ ëŒ€ëŸ‰ìœ¼ë¡œ ì²´ê²°ì‹œí‚¤ë©´, ë§ˆì¼“ë©”ì´ì»¤ëŠ” ì†ì‹¤ì„ ë³¸ë‹¤. ì´ê²ƒì´ ìŠ¤í”„ë ˆë“œê°€ ì¡´ì¬í•˜ëŠ” ê·¼ë³¸ì  ì´ìœ ë‹¤ â€” ë§ˆì¼“ë©”ì´ì»¤ëŠ” ì—­ì„ íƒ ìœ„í—˜ì— ëŒ€í•œ ë³´ìƒìœ¼ë¡œ ìŠ¤í”„ë ˆë“œë¥¼ ìš”êµ¬í•œë‹¤.
</p>

<div class="eq">
\[ \text{Spread} = \underbrace{2 \cdot \alpha \cdot \sigma}_{\text{ì—­ì„ íƒ ë¹„ìš©}} + \underbrace{c_{\text{inventory}}}_{\text{ì¬ê³  ë¹„ìš©}} + \underbrace{c_{\text{order}}}_{\text{ì£¼ë¬¸ ì²˜ë¦¬ ë¹„ìš©}} \]
</div>

<p>
ì—¬ê¸°ì„œ \(\alpha\)ëŠ” ì •ë³´ ë¹„ëŒ€ì¹­ì˜ ì •ë„, \(\sigma\)ëŠ” ìì‚°ì˜ ë³€ë™ì„±ì´ë‹¤. ë³€ë™ì„±ì´ ë†’ê±°ë‚˜ ì •ë³´ ë¹„ëŒ€ì¹­ì´ í´ìˆ˜ë¡ ìŠ¤í”„ë ˆë“œê°€ ë„“ì–´ì§„ë‹¤. ì´ê²ƒì´ ì–´ë‹ ë°œí‘œ ì§ì „ì— ìŠ¤í”„ë ˆë“œê°€ ê¸‰ê²©íˆ ë„“ì–´ì§€ëŠ” ì´ìœ ë‹¤.
</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 2: ì˜¤ë”ë¶ ì—­í•™
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch2">Chapter 2. ì˜¤ë”ë¶ ì—­í•™ â€” í˜¸ê°€ì°½ ë°ì´í„° ë¶„ì„</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Order Book Dynamics" / ë‘ì‡ì•Œê³  Ch.4 ìš°ì„ ìˆœìœ„ í â€” ì˜¤ë”ë¶ì€ ê°€ê²© ìš°ì„ , ì‹œê°„ ìš°ì„  ì›ì¹™ìœ¼ë¡œ ì •ë ¬ëœ ì´ì¤‘ ìš°ì„ ìˆœìœ„ íë‹¤</p>
</div>

<h3>2.1 ì˜¤ë”ë¶ì˜ ë™ì  ë³€í™”</h3>

<p>
ì˜¤ë”ë¶ì€ ì •ì ì¸ ìŠ¤ëƒ…ìƒ·ì´ ì•„ë‹ˆë¼, ë§¤ ë°€ë¦¬ì´ˆë§ˆë‹¤ ë³€í•˜ëŠ” ë™ì  ì‹œìŠ¤í…œì´ë‹¤. ìƒˆë¡œìš´ ì§€ì •ê°€ ì£¼ë¬¸ì´ ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ í˜¸ê°€ì— ìˆ˜ëŸ‰ì´ ì¶”ê°€ë˜ê³ , ì‹œì¥ê°€ ì£¼ë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ìµœìš°ì„  í˜¸ê°€ì˜ ìˆ˜ëŸ‰ì´ ì†Œì§„ëœë‹¤. ì£¼ë¬¸ ì·¨ì†Œ(cancel)ë„ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•œë‹¤. HFT ì‹œìŠ¤í…œì€ ì´ ë³€í™”ì˜ íŒ¨í„´ì—ì„œ ë‹¨ê¸° ê°€ê²© ë°©í–¥ì„ ì˜ˆì¸¡í•œë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– ì˜¤ë”ë¶ ì´ë²¤íŠ¸ 3ê°€ì§€</strong></p>
<p class="ni" style="margin-top:8px">
(1) <strong>Add</strong> â€” ìƒˆë¡œìš´ ì§€ì •ê°€ ì£¼ë¬¸ì´ ì˜¤ë”ë¶ì— ì¶”ê°€ë¨<br>
(2) <strong>Cancel</strong> â€” ê¸°ì¡´ ì§€ì •ê°€ ì£¼ë¬¸ì´ ì·¨ì†Œë¨<br>
(3) <strong>Execute (Trade)</strong> â€” ì‹œì¥ê°€ ì£¼ë¬¸ì´ ì§€ì •ê°€ ì£¼ë¬¸ê³¼ ë§¤ì¹­ë˜ì–´ ì²´ê²°ë¨
</p>
</div>

<h3>2.2 Order Imbalanceì™€ ê°€ê²© ì˜ˆì¸¡</h3>

<p>
ì˜¤ë”ë¶ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ë‹¨ê¸° ì˜ˆì¸¡ ì‹œê·¸ë„ ì¤‘ í•˜ë‚˜ê°€ <strong>ì£¼ë¬¸ ë¶ˆê· í˜•(Order Imbalance)</strong>ì´ë‹¤. ë§¤ìˆ˜ ìª½ ìˆ˜ëŸ‰ì´ ë§¤ë„ ìª½ë³´ë‹¤ ë§ìœ¼ë©´ ê°€ê²©ì´ ì˜¤ë¥¼ ê°€ëŠ¥ì„±ì´ ë†’ê³ , ë°˜ëŒ€ë©´ ë‚´ë¦´ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ì´ê²ƒì€ ì§ê´€ì ìœ¼ë¡œë„ ì´í•´ëœë‹¤ â€” ì‚¬ë ¤ëŠ” ì‚¬ëŒì´ ë§ìœ¼ë©´ ê°€ê²©ì´ ì˜¤ë¥¸ë‹¤.
</p>

<div class="eq">
\[ OI = \frac{V_{\text{bid}}^{(1)} - V_{\text{ask}}^{(1)}}{V_{\text{bid}}^{(1)} + V_{\text{ask}}^{(1)}} \in [-1, +1] \]
</div>

<p>
\(OI > 0\)ì´ë©´ ë§¤ìˆ˜ ì••ë ¥ ìš°ì„¸, \(OI < 0\)ì´ë©´ ë§¤ë„ ì••ë ¥ ìš°ì„¸ë‹¤. ì—¬ëŸ¬ í˜¸ê°€ ë ˆë²¨ì„ ê°€ì¤‘ í•©ì‚°í•˜ë©´ ë” ì•ˆì •ì ì¸ ì‹œê·¸ë„ì„ ì–»ì„ ìˆ˜ ìˆë‹¤:
</p>

<div class="eq">
\[ WOI = \frac{\sum_{k=1}^{L} w_k \cdot V_{\text{bid}}^{(k)} - \sum_{k=1}^{L} w_k \cdot V_{\text{ask}}^{(k)}}{\sum_{k=1}^{L} w_k \cdot (V_{\text{bid}}^{(k)} + V_{\text{ask}}^{(k)})}, \quad w_k = \frac{1}{k} \]
</div>

<p>
ê°€ì¤‘ì¹˜ \(w_k = 1/k\)ëŠ” ìµœìš°ì„  í˜¸ê°€ì— ê°€ì¥ í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤. ìµœìš°ì„  í˜¸ê°€ê°€ ê°€ê²© ë³€ë™ì— ê°€ì¥ ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì´ë‹¤.
</p>

<p class="cc">â–¼ Order Imbalance ê³„ì‚° ì˜ˆì œ</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ì˜¤ë”ë¶ ìŠ¤ëƒ…ìƒ· (5 ë ˆë²¨)</span>
bid_sizes = np.<span class="fn">array</span>([<span class="nu">200</span>, <span class="nu">350</span>, <span class="nu">500</span>, <span class="nu">180</span>, <span class="nu">420</span>])
ask_sizes = np.<span class="fn">array</span>([<span class="nu">150</span>, <span class="nu">280</span>, <span class="nu">450</span>, <span class="nu">320</span>, <span class="nu">550</span>])

<span class="cm"># ë‹¨ìˆœ OI (Level 1ë§Œ)</span>
oi_l1 = (bid_sizes[<span class="nu">0</span>] - ask_sizes[<span class="nu">0</span>]) / (bid_sizes[<span class="nu">0</span>] + ask_sizes[<span class="nu">0</span>])
<span class="fn">print</span>(<span class="st">f"Level-1 OI: {oi_l1:.4f}"</span>)  <span class="cm"># ì–‘ìˆ˜ â†’ ë§¤ìˆ˜ ì••ë ¥ ìš°ì„¸</span>

<span class="cm"># ê°€ì¤‘ OI (5 ë ˆë²¨)</span>
weights = <span class="nu">1.0</span> / np.<span class="fn">arange</span>(<span class="nu">1</span>, <span class="nu">6</span>)
woi = (np.<span class="fn">sum</span>(weights * bid_sizes) - np.<span class="fn">sum</span>(weights * ask_sizes)) \
    / (np.<span class="fn">sum</span>(weights * bid_sizes) + np.<span class="fn">sum</span>(weights * ask_sizes))
<span class="fn">print</span>(<span class="st">f"Weighted OI (5-level): {woi:.4f}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output:</span>
Level-1 OI: 0.1429
Weighted OI (5-level): 0.0312</div>

<h3>2.3 Price Impact â€” ì£¼ë¬¸ì´ ê°€ê²©ì„ ì›€ì§ì´ëŠ” ë©”ì»¤ë‹ˆì¦˜</h3>

<p>
ëŒ€ëŸ‰ ì£¼ë¬¸ì€ ì˜¤ë”ë¶ì˜ ì—¬ëŸ¬ í˜¸ê°€ë¥¼ ì†Œì§„í•˜ë©´ì„œ ê°€ê²©ì„ ì›€ì§ì¸ë‹¤. ì´ê²ƒì„ <strong>ê°€ê²© ì¶©ê²©(Price Impact)</strong>ì´ë¼ í•œë‹¤. Kyle(1985)ì˜ ëª¨ë¸ì—ì„œ ê°€ê²© ì¶©ê²©ì€ ì£¼ë¬¸ í¬ê¸°ì— ë¹„ë¡€í•œë‹¤:
</p>

<div class="eq">
\[ \Delta P = \lambda \cdot \Delta V, \quad \lambda = \frac{\sigma}{\sqrt{V_{\text{daily}}}} \]
</div>

<p>
ì—¬ê¸°ì„œ \(\lambda\)ëŠ” Kyle's Lambda(ì‹œì¥ ê¹Šì´ì˜ ì—­ìˆ˜), \(\sigma\)ëŠ” ë³€ë™ì„±, \(V_{\text{daily}}\)ëŠ” ì¼ê°„ ê±°ë˜ëŸ‰ì´ë‹¤. ìœ ë™ì„±ì´ í’ë¶€í•œ ì¢…ëª©(AAPL ë“±)ì€ \(\lambda\)ê°€ ì‘ê³ , ì†Œí˜•ì£¼ëŠ” \(\lambda\)ê°€ í¬ë‹¤.
</p>

<!-- â˜… Plotly: Order Imbalance vs Price Change -->
<div id="plot-ch2-oi" style="width:100%;height:420px;margin:20px 0"></div>
<script>
(function(){
  // Seeded PRNG
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(42);
  var N=200, oi=[], dp=[];
  for(var i=0;i<N;i++){
    var o=(rng()-0.5)*2; // OI in [-1,1]
    var noise=(rng()-0.5)*0.3;
    var d=o*0.05+noise*0.02; // price change ~ linear in OI + noise
    oi.push(o); dp.push(d);
  }
  // Color by quadrant
  var colors=oi.map(function(o,i){return (o>0&&dp[i]>0)||(o<0&&dp[i]<0)?'rgba(46,204,113,0.6)':'rgba(231,76,60,0.4)';});
  Plotly.newPlot('plot-ch2-oi',[
    {x:oi,y:dp,mode:'markers',type:'scatter',marker:{size:5,color:colors},name:'OI vs Î”P',
     hovertemplate:'OI: %{x:.3f}<br>Î”P: %{y:.4f}%'},
    {x:[-1,1],y:[-0.05,0.05],mode:'lines',line:{color:'#e67e22',width:2,dash:'dash'},name:'ì„ í˜• íšŒê·€ (ì´ë¡ )'}
  ],{
    title:{text:'ğŸ“Š Order Imbalance vs ê°€ê²© ë³€í™” (200 ìŠ¤ëƒ…ìƒ·)',font:{size:13}},
    xaxis:{title:'Order Imbalance',range:[-1.1,1.1]},
    yaxis:{title:'ê°€ê²© ë³€í™” (%)',zeroline:true},
    legend:{x:0.01,y:0.99,bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:45,b:50},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ OIê°€ ì–‘ìˆ˜ì¼ ë•Œ ê°€ê²©ì´ ì˜¤ë¥´ëŠ” ê²½í–¥(ì´ˆë¡), ìŒìˆ˜ì¼ ë•Œ ë‚´ë¦¬ëŠ” ê²½í–¥(ë¹¨ê°•). ì ì„ ì€ ì´ë¡ ì  ì„ í˜• ê´€ê³„.</p>

<h3>2.4 Trade Flow Toxicity â€” VPIN</h3>

<p>
VPIN(Volume-Synchronized Probability of Informed Trading)ì€ ê±°ë˜ íë¦„ì˜ "ë…ì„±"ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œë‹¤. ì •ë³´ë¥¼ ê°€ì§„ íŠ¸ë ˆì´ë”ì˜ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ VPINì´ ë†’ì•„ì§€ê³ , ì´ëŠ” ë§ˆì¼“ë©”ì´ì»¤ì—ê²Œ ìœ„í—˜ ì‹ í˜¸ë‹¤. 2010ë…„ Flash Crash ì§ì „ì— VPINì´ ê¸‰ë“±í•œ ê²ƒìœ¼ë¡œ ìœ ëª…í•˜ë‹¤.
</p>

<div class="eq">
\[ VPIN = \frac{\sum_{i=1}^{n} |V_i^{buy} - V_i^{sell}|}{n \cdot V_{\text{bucket}}} \]
</div>

<p>
ì—¬ê¸°ì„œ \(V_{\text{bucket}}\)ì€ ê³ ì • ê±°ë˜ëŸ‰ ë‹¨ìœ„(volume bucket)ì´ê³ , \(V_i^{buy}, V_i^{sell}\)ì€ ê° ë²„í‚· ë‚´ ë§¤ìˆ˜/ë§¤ë„ ê±°ë˜ëŸ‰ì´ë‹¤. VPINì´ ë†’ìœ¼ë©´ ì •ë³´ ë¹„ëŒ€ì¹­ì´ í¬ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ, ë§ˆì¼“ë©”ì´ì»¤ëŠ” ìŠ¤í”„ë ˆë“œë¥¼ ë„“íˆê±°ë‚˜ ì£¼ë¬¸ì„ ì² íšŒí•´ì•¼ í•œë‹¤.
</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ ì‹¤ì „ ì£¼ì˜:</strong> VPIN ê³„ì‚°ì—ì„œ ë§¤ìˆ˜/ë§¤ë„ ë¶„ë¥˜(trade classification)ëŠ” Lee-Ready ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ Tick Ruleì„ ì‚¬ìš©í•œë‹¤. ì™„ë²½í•œ ë¶„ë¥˜ëŠ” ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ VPINì—ëŠ” í•­ìƒ ë…¸ì´ì¦ˆê°€ í¬í•¨ëœë‹¤.</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 3: HFT ì „ëµ ìœ í˜•
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch3">Chapter 3. HFT ì „ëµ ìœ í˜• â€” ë§ˆì¼“ë©”ì´í‚¹, ì°¨ìµê±°ë˜, ëª¨ë©˜í…€</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLDSF Ch.13 "High-Frequency Trading Strategies" / MLAT Ch.22 "Trading Strategies"</p>
</div>

<h3>3.1 ë§ˆì¼“ë©”ì´í‚¹ (Market Making)</h3>

<p>
ë§ˆì¼“ë©”ì´í‚¹ì€ HFTì˜ ê°€ì¥ ëŒ€í‘œì ì¸ ì „ëµì´ë‹¤. ë§ˆì¼“ë©”ì´ì»¤ëŠ” ì˜¤ë”ë¶ì˜ ì–‘ìª½(bid/ask)ì— ì§€ì •ê°€ ì£¼ë¬¸ì„ ë™ì‹œì— ê±¸ì–´ë†“ê³ , ìŠ¤í”„ë ˆë“œë¥¼ ìˆ˜ìµìœ¼ë¡œ ì·¨í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $149.50ì— ë§¤ìˆ˜ ì£¼ë¬¸, $149.55ì— ë§¤ë„ ì£¼ë¬¸ì„ ê±¸ë©´, ì–‘ìª½ ëª¨ë‘ ì²´ê²°ë  ë•Œ $0.05ì˜ ìˆ˜ìµì„ ì–»ëŠ”ë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– Avellaneda-Stoikov ëª¨ë¸ (2008)</strong></p>
<p class="ni" style="margin-top:8px">
ìµœì  í˜¸ê°€ ì„¤ì •ì˜ ê³ ì „ì  ëª¨ë¸ì´ë‹¤. ë§ˆì¼“ë©”ì´ì»¤ì˜ ìµœì  bid/ask ê°€ê²©ì€:
</p>
</div>

<div class="eq">
\[ \delta^{bid} = \delta^{ask} = \frac{\gamma \sigma^2 (T-t)}{2} + \frac{1}{\gamma} \ln\left(1 + \frac{\gamma}{\kappa}\right) \]
</div>

<p>
ì—¬ê¸°ì„œ \(\gamma\)ëŠ” ìœ„í—˜ íšŒí”¼ ê³„ìˆ˜, \(\sigma\)ëŠ” ë³€ë™ì„±, \(T-t\)ëŠ” ì”ì—¬ ì‹œê°„, \(\kappa\)ëŠ” ì£¼ë¬¸ ë„ì°©ë¥ ì´ë‹¤. í•µì‹¬ ì¸ì‚¬ì´íŠ¸: (1) ë³€ë™ì„±ì´ ë†’ìœ¼ë©´ ìŠ¤í”„ë ˆë“œë¥¼ ë„“í˜€ì•¼ í•˜ê³ , (2) ì”ì—¬ ì‹œê°„ì´ ì ìœ¼ë©´ ì¬ê³  ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìŠ¤í”„ë ˆë“œë¥¼ ì¢í˜€ì•¼ í•œë‹¤.
</p>

<p class="cc">â–¼ Avellaneda-Stoikov ë§ˆì¼“ë©”ì´í‚¹ ì‹œë®¬ë ˆì´ì…˜</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">def</span> <span class="fn">avellaneda_stoikov</span>(S, sigma, gamma, kappa, T, t, q):
    <span class="st">"""
    S: í˜„ì¬ mid price
    sigma: ë³€ë™ì„±
    gamma: ìœ„í—˜ íšŒí”¼ ê³„ìˆ˜
    kappa: ì£¼ë¬¸ ë„ì°©ë¥ 
    T: ë§ˆê° ì‹œê°„, t: í˜„ì¬ ì‹œê°„
    q: í˜„ì¬ ì¬ê³  (ì–‘ìˆ˜=ë¡±, ìŒìˆ˜=ìˆ)
    """</span>
    <span class="cm"># ì˜ˆì•½ ê°€ê²© (ì¬ê³  ì¡°ì •)</span>
    reservation = S - q * gamma * sigma**<span class="nu">2</span> * (T - t)

    <span class="cm"># ìµœì  ìŠ¤í”„ë ˆë“œ</span>
    spread = gamma * sigma**<span class="nu">2</span> * (T - t) + (<span class="nu">2</span>/gamma) * np.<span class="fn">log</span>(<span class="nu">1</span> + gamma/kappa)

    bid = reservation - spread / <span class="nu">2</span>
    ask = reservation + spread / <span class="nu">2</span>
    <span class="kw">return</span> bid, ask, reservation, spread

<span class="cm"># íŒŒë¼ë¯¸í„°</span>
S = <span class="nu">100.0</span>    <span class="cm"># mid price</span>
sigma = <span class="nu">0.02</span>  <span class="cm"># ì¼ê°„ ë³€ë™ì„± 2%</span>
gamma = <span class="nu">0.1</span>   <span class="cm"># ìœ„í—˜ íšŒí”¼</span>
kappa = <span class="nu">1.5</span>   <span class="cm"># ì£¼ë¬¸ ë„ì°©ë¥ </span>
T = <span class="nu">1.0</span>       <span class="cm"># 1ì¼</span>

<span class="cm"># ì¬ê³ ë³„ ìµœì  í˜¸ê°€</span>
<span class="kw">for</span> q <span class="kw">in</span> [<span class="nu">-5</span>, <span class="nu">0</span>, <span class="nu">5</span>]:
    bid, ask, res, spr = <span class="fn">avellaneda_stoikov</span>(S, sigma, gamma, kappa, T, <span class="nu">0</span>, q)
    <span class="fn">print</span>(<span class="st">f"ì¬ê³  q={q:+d}: Bid={bid:.4f}, Ask={ask:.4f}, "</span>
          <span class="st">f"Spread={spr:.4f}, Reservation={res:.4f}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output:</span>
ì¬ê³  q=-5: Bid=99.9982, Ask=100.0118, Spread=0.0136, Reservation=100.0050
ì¬ê³  q=+0: Bid=99.9932, Ask=100.0068, Spread=0.0136, Reservation=100.0000
ì¬ê³  q=+5: Bid=99.9882, Ask=100.0018, Spread=0.0136, Reservation=99.9950</div>

<p>
ì¬ê³ ê°€ ì–‘ìˆ˜(ë¡±)ì´ë©´ ì˜ˆì•½ ê°€ê²©ì´ ë‚´ë ¤ê°€ì„œ ë§¤ë„ë¥¼ ìœ ë„í•˜ê³ , ìŒìˆ˜(ìˆ)ì´ë©´ ì˜¬ë¼ê°€ì„œ ë§¤ìˆ˜ë¥¼ ìœ ë„í•œë‹¤. ì´ê²ƒì´ ì¬ê³  ê´€ë¦¬(inventory management)ì˜ í•µì‹¬ì´ë‹¤.
</p>

<h3>3.2 í†µê³„ì  ì°¨ìµê±°ë˜ (Statistical Arbitrage)</h3>

<p>
í†µê³„ì  ì°¨ìµê±°ë˜ëŠ” ë‘ ê°œ ì´ìƒì˜ ê´€ë ¨ ìì‚° ê°„ ê°€ê²© ê´´ë¦¬ë¥¼ í¬ì°©í•˜ì—¬ ìˆ˜ìµì„ ì¶”êµ¬í•œë‹¤. R5ì—ì„œ ë°°ìš´ ê³µì ë¶„(cointegration)ì´ í•µì‹¬ ë„êµ¬ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì½”ì¹´ì½œë¼(KO)ì™€ í©ì‹œ(PEP)ì˜ ê°€ê²© ë¹„ìœ¨ì´ ì—­ì‚¬ì  í‰ê· ì—ì„œ ë²—ì–´ë‚˜ë©´, ë¹„ì‹¼ ìª½ì„ ë§¤ë„í•˜ê³  ì‹¼ ìª½ì„ ë§¤ìˆ˜í•œë‹¤.
</p>

<div class="eq">
\[ z_t = \frac{P_t^A - \beta \cdot P_t^B - \mu}{\sigma}, \quad \text{Signal: } |z_t| > \tau \text{ì´ë©´ ì§„ì…} \]
</div>

<h3>3.3 ëª¨ë©˜í…€ ì´ê·¸ë‹ˆì…˜ (Momentum Ignition)</h3>

<p>
ëª¨ë©˜í…€ ì´ê·¸ë‹ˆì…˜ì€ ë…¼ë€ì´ ë§ì€ ì „ëµì´ë‹¤. ëŒ€ëŸ‰ ì£¼ë¬¸ì„ ë¹ ë¥´ê²Œ ë„£ì–´ ê°€ê²©ì„ í•œ ë°©í–¥ìœ¼ë¡œ ë°€ê³ , ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì´ ë”°ë¼ì˜¤ë©´(ëª¨ë©˜í…€) ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ í¬ì§€ì…˜ì„ ì²­ì‚°í•œë‹¤. ë§ì€ ê±°ë˜ì†Œì—ì„œ ì´ë¥¼ ì‹œì¥ ì¡°ì‘ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ê·œì œí•˜ê³  ìˆë‹¤.
</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ ê·œì œ ì£¼ì˜:</strong> ëª¨ë©˜í…€ ì´ê·¸ë‹ˆì…˜, ìŠ¤í‘¸í•‘(spoofing), ë ˆì´ì–´ë§(layering) ë“±ì€ ëŒ€ë¶€ë¶„ì˜ ì„ ì§„êµ­ ì‹œì¥ì—ì„œ ë¶ˆë²•ì´ë‹¤. í•™ìŠµ ëª©ì ìœ¼ë¡œë§Œ ì´í•´í•˜ê³ , ì‹¤ì „ì—ì„œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.</p>
</div>

<h3>3.4 HFT ì „ëµ ë¹„êµ</h3>

<table>
<tr><th>ì „ëµ</th><th>ìˆ˜ìµ ì›ì²œ</th><th>ë³´ìœ  ê¸°ê°„</th><th>ë¦¬ìŠ¤í¬</th><th>ì¸í”„ë¼ ìš”êµ¬</th></tr>
<tr><td>ë§ˆì¼“ë©”ì´í‚¹</td><td>Bid-Ask ìŠ¤í”„ë ˆë“œ</td><td>ì´ˆ~ë¶„</td><td>ì¬ê³  ë¦¬ìŠ¤í¬, ì—­ì„ íƒ</td><td>ë§¤ìš° ë†’ìŒ (co-location)</td></tr>
<tr><td>í†µê³„ì  ì°¨ìµê±°ë˜</td><td>ê°€ê²© ê´´ë¦¬ ìˆ˜ë ´</td><td>ë¶„~ì‹œê°„</td><td>ê´´ë¦¬ í™•ëŒ€ ë¦¬ìŠ¤í¬</td><td>ë†’ìŒ</td></tr>
<tr><td>ëª¨ë©˜í…€</td><td>ë‹¨ê¸° ì¶”ì„¸ ì¶”ì¢…</td><td>ì´ˆ~ë¶„</td><td>ë°˜ì „ ë¦¬ìŠ¤í¬</td><td>ë†’ìŒ</td></tr>
<tr><td>ë ˆì´í„´ì‹œ ì°¨ìµê±°ë˜</td><td>ê±°ë˜ì†Œ ê°„ ê°€ê²© ì°¨ì´</td><td>ë°€ë¦¬ì´ˆ</td><td>ê¸°ìˆ  ë¦¬ìŠ¤í¬</td><td>ê·¹ë„ë¡œ ë†’ìŒ</td></tr>
</table>

<!-- â˜… Plotly: Market Making P&L Simulation -->
<div id="plot-ch3-mm" style="width:100%;height:450px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(123);
  var N=500, pnl=[0], inventory=[0], midPrices=[100];
  var cumPnl=0, inv=0, mid=100;
  for(var i=1;i<=N;i++){
    mid+=((rng()-0.5)*0.1);
    midPrices.push(mid);
    // Random trade: buy or sell with some probability
    var r=rng();
    if(r<0.3){ // buy filled
      cumPnl-=mid+0.025; inv+=1;
    } else if(r<0.6){ // sell filled
      cumPnl+=mid-0.025; inv-=1;
    }
    // Mark-to-market P&L
    pnl.push(cumPnl+inv*mid);
    inventory.push(inv);
  }
  var steps=Array.from({length:N+1},function(_,i){return i;});
  Plotly.newPlot('plot-ch3-mm',[
    {x:steps,y:pnl,type:'scatter',mode:'lines',name:'ëˆ„ì  P&L ($)',line:{color:'#2ecc71',width:2}},
    {x:steps,y:inventory,type:'scatter',mode:'lines',name:'ì¬ê³  (ì£¼)',line:{color:'#e74c3c',width:1.5},yaxis:'y2'}
  ],{
    title:{text:'ğŸ“Š ë§ˆì¼“ë©”ì´í‚¹ ì‹œë®¬ë ˆì´ì…˜: P&L vs ì¬ê³  (500 ê±°ë˜)',font:{size:13}},
    xaxis:{title:'ê±°ë˜ ë²ˆí˜¸'},
    yaxis:{title:'ëˆ„ì  P&L ($)',side:'left',titlefont:{color:'#2ecc71'}},
    yaxis2:{title:'ì¬ê³  (ì£¼)',overlaying:'y',side:'right',titlefont:{color:'#e74c3c'},zeroline:true},
    legend:{x:0.01,y:0.99,bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:45,b:50},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ ì´ˆë¡ì„ =ëˆ„ì  P&L, ë¹¨ê°„ì„ =ì¬ê³ . ë§ˆì¼“ë©”ì´ì»¤ëŠ” ì¬ê³ ë¥¼ 0 ê·¼ì²˜ë¡œ ìœ ì§€í•˜ë©´ì„œ ìŠ¤í”„ë ˆë“œ ìˆ˜ìµì„ ì¶•ì í•œë‹¤.</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 4: ì €ì§€ì—° ì•„í‚¤í…ì²˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch4">Chapter 4. ì €ì§€ì—° ì•„í‚¤í…ì²˜ â€” ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì‹œìŠ¤í…œ</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLDSF Ch.14 "System Architecture for Trading" / ë‘ì‡ì•Œê³  Ch.8 í•´ì‹œ í…Œì´ë¸” â€” O(1) ì¡°íšŒê°€ HFTì—ì„œ ì¤‘ìš”í•œ ì´ìœ </p>
</div>

<h3>4.1 ì™œ ì†ë„ê°€ ì¤‘ìš”í•œê°€</h3>

<p>
HFTì—ì„œ 1ë°€ë¦¬ì´ˆì˜ ì°¨ì´ê°€ ìˆ˜ìµê³¼ ì†ì‹¤ì„ ê°€ë¥¸ë‹¤. ì‹œì¥ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ë¶„ì„í•˜ê³ , ì£¼ë¬¸ì„ ìƒì„±í•˜ì—¬ ê±°ë˜ì†Œì— ì „ì†¡í•˜ëŠ” ì „ì²´ ê³¼ì •ì´ ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„ë¡œ ì´ë£¨ì–´ì ¸ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ HFT ì‹œìŠ¤í…œì€ ì¼ë°˜ì ì¸ ì›¹ ì„œë²„ì™€ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œë‹¤.
</p>

<h3>4.2 ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜</h3>

<p>
ì „í†µì ì¸ "í´ë§(polling)" ë°©ì‹ â€” ì£¼ê¸°ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í™•ì¸í•˜ëŠ” ë°©ì‹ â€” ì€ HFTì— ë¶€ì í•©í•˜ë‹¤. ëŒ€ì‹  <strong>ì´ë²¤íŠ¸ ë“œë¦¬ë¸(Event-Driven)</strong> ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œë‹¤. ì‹œì¥ ë°ì´í„°ê°€ ë„ì°©í•˜ë©´ ì¦‰ì‹œ ì½œë°± í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ê³ , ì‹œê·¸ë„ì´ ë°œìƒí•˜ë©´ ì¦‰ì‹œ ì£¼ë¬¸ì´ ìƒì„±ëœë‹¤.
</p>

<p class="cc">â–¼ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ íŠ¸ë ˆì´ë”© ì—”ì§„ (Python í”„ë¡œí† íƒ€ì…)</p>
<pre>
<span class="kw">import</span> asyncio
<span class="kw">from</span> collections <span class="kw">import</span> defaultdict
<span class="kw">from</span> dataclasses <span class="kw">import</span> dataclass
<span class="kw">from</span> enum <span class="kw">import</span> Enum
<span class="kw">from</span> typing <span class="kw">import</span> Callable, List

<span class="kw">class</span> <span class="nb">EventType</span>(Enum):
    MARKET_DATA = <span class="st">"market_data"</span>
    SIGNAL = <span class="st">"signal"</span>
    ORDER = <span class="st">"order"</span>
    FILL = <span class="st">"fill"</span>

@<span class="fn">dataclass</span>
<span class="kw">class</span> <span class="nb">Event</span>:
    event_type: EventType
    data: <span class="nb">dict</span>
    timestamp: <span class="nb">float</span>

<span class="kw">class</span> <span class="nb">EventBus</span>:
    <span class="st">"""ì´ë²¤íŠ¸ ë²„ìŠ¤: ë°œí–‰-êµ¬ë… íŒ¨í„´"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.handlers = <span class="fn">defaultdict</span>(<span class="nb">list</span>)

    <span class="kw">def</span> <span class="fn">subscribe</span>(self, event_type: EventType, handler: Callable):
        self.handlers[event_type].<span class="fn">append</span>(handler)

    <span class="kw">async</span> <span class="kw">def</span> <span class="fn">publish</span>(self, event: Event):
        <span class="kw">for</span> handler <span class="kw">in</span> self.handlers[event.event_type]:
            <span class="kw">await</span> <span class="fn">handler</span>(event)

<span class="kw">class</span> <span class="nb">TradingEngine</span>:
    <span class="st">"""ì´ë²¤íŠ¸ ë“œë¦¬ë¸ íŠ¸ë ˆì´ë”© ì—”ì§„"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.bus = <span class="fn">EventBus</span>()
        self.bus.<span class="fn">subscribe</span>(EventType.MARKET_DATA, self.on_market_data)
        self.bus.<span class="fn">subscribe</span>(EventType.SIGNAL, self.on_signal)
        self.position = <span class="nu">0</span>

    <span class="kw">async</span> <span class="kw">def</span> <span class="fn">on_market_data</span>(self, event):
        <span class="st">"""ì‹œì¥ ë°ì´í„° â†’ ì‹œê·¸ë„ ìƒì„±"""</span>
        oi = event.data.<span class="fn">get</span>(<span class="st">'order_imbalance'</span>, <span class="nu">0</span>)
        <span class="kw">if</span> <span class="fn">abs</span>(oi) > <span class="nu">0.3</span>:  <span class="cm"># ì„ê³„ê°’ ì´ˆê³¼</span>
            signal = <span class="fn">Event</span>(
                event_type=EventType.SIGNAL,
                data={<span class="st">'direction'</span>: <span class="nu">1</span> <span class="kw">if</span> oi > <span class="nu">0</span> <span class="kw">else</span> -<span class="nu">1</span>,
                      <span class="st">'strength'</span>: <span class="fn">abs</span>(oi)},
                timestamp=event.timestamp
            )
            <span class="kw">await</span> self.bus.<span class="fn">publish</span>(signal)

    <span class="kw">async</span> <span class="kw">def</span> <span class="fn">on_signal</span>(self, event):
        <span class="st">"""ì‹œê·¸ë„ â†’ ì£¼ë¬¸ ìƒì„±"""</span>
        direction = event.data[<span class="st">'direction'</span>]
        <span class="fn">print</span>(<span class="st">f"[SIGNAL] dir={direction:+d}, "</span>
              <span class="st">f"strength={event.data['strength']:.2f}"</span>)
        <span class="cm"># í¬ì§€ì…˜ ê´€ë¦¬ ë¡œì§</span>
        <span class="kw">if</span> self.position * direction <= <span class="nu">0</span>:
            self.position += direction * <span class="nu">100</span>
            <span class="fn">print</span>(<span class="st">f"[ORDER] {'BUY' if direction > 0 else 'SELL'} "</span>
                  <span class="st">f"100 shares, pos={self.position}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output (ì‹œë®¬ë ˆì´ì…˜):</span>
[SIGNAL] dir=+1, strength=0.45
[ORDER] BUY 100 shares, pos=100
[SIGNAL] dir=-1, strength=0.38
[ORDER] SELL 100 shares, pos=0</div>

<h3>4.3 ì €ì§€ì—° ìµœì í™” ê¸°ë²•</h3>

<table>
<tr><th>ê¸°ë²•</th><th>ì„¤ëª…</th><th>íš¨ê³¼</th></tr>
<tr><td>Co-location</td><td>ì„œë²„ë¥¼ ê±°ë˜ì†Œ ë°ì´í„°ì„¼í„°ì— ë°°ì¹˜</td><td>ë„¤íŠ¸ì›Œí¬ ì§€ì—° ìµœì†Œí™” (~Î¼s)</td></tr>
<tr><td>Kernel Bypass</td><td>OS ì»¤ë„ì„ ìš°íšŒí•˜ì—¬ NIC ì§ì ‘ ì ‘ê·¼</td><td>ì‹œìŠ¤í…œ ì½œ ì˜¤ë²„í—¤ë“œ ì œê±°</td></tr>
<tr><td>Lock-free Queue</td><td>ë®¤í…ìŠ¤ ì—†ëŠ” í ìë£Œêµ¬ì¡°</td><td>ìŠ¤ë ˆë“œ ê²½í•© ì œê±°</td></tr>
<tr><td>Memory Pool</td><td>ì‚¬ì „ í• ë‹¹ëœ ë©”ëª¨ë¦¬ í’€ ì‚¬ìš©</td><td>GC/malloc ì§€ì—° ì œê±°</td></tr>
<tr><td>FPGA</td><td>í•˜ë“œì›¨ì–´ ìˆ˜ì¤€ ë¡œì§ êµ¬í˜„</td><td>ë‚˜ë…¸ì´ˆ ë‹¨ìœ„ ì²˜ë¦¬</td></tr>
</table>

<!-- â˜… Plotly: Latency Comparison -->
<div id="plot-ch4-latency" style="width:100%;height:400px;margin:20px 0"></div>
<script>
(function(){
  var categories=['Python\n(ì¼ë°˜)','Python\n(ìµœì í™”)','C++\n(ì¼ë°˜)','C++\n(ìµœì í™”)','FPGA'];
  var latencies=[5000,500,50,5,0.5]; // microseconds
  var colors=['#e74c3c','#e67e22','#f1c40f','#2ecc71','#3498db'];
  Plotly.newPlot('plot-ch4-latency',[{
    x:categories,y:latencies,type:'bar',
    marker:{color:colors,line:{width:1,color:'#333'}},
    text:latencies.map(function(v){return v>=1?v+'Î¼s':v*1000+'ns';}),
    textposition:'outside',
    hovertemplate:'%{x}: %{y}Î¼s'
  }],{
    title:{text:'ğŸ“Š íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ ë ˆì´í„´ì‹œ ë¹„êµ (ë¡œê·¸ ìŠ¤ì¼€ì¼)',font:{size:13}},
    yaxis:{title:'ë ˆì´í„´ì‹œ (Î¼s)',type:'log',range:[-1,4]},
    xaxis:{title:''},
    margin:{t:45,b:80},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff',
    annotations:[{x:'Python\n(ì¼ë°˜)',y:Math.log10(5000),text:'5ms â€” ë°±í…ŒìŠ¤íŠ¸ìš©',showarrow:true,arrowhead:2,ax:60,ay:-20,font:{size:10,color:'#e74c3c'}},
                 {x:'FPGA',y:Math.log10(0.5),text:'500ns â€” ì‹¤ì „ HFT',showarrow:true,arrowhead:2,ax:-60,ay:-20,font:{size:10,color:'#3498db'}}]
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ Pythonì€ í”„ë¡œí† íƒ€ì´í•‘/ë°±í…ŒìŠ¤íŠ¸ì— ì í•©í•˜ê³ , ì‹¤ì „ HFTëŠ” C++/FPGAê°€ í•„ìˆ˜. ìš°ë¦¬ëŠ” Pythonìœ¼ë¡œ ì „ëµ ë¡œì§ì„ ê²€ì¦í•œ í›„ C++ë¡œ í¬íŒ…í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ë”°ë¥¸ë‹¤.</p>

<div class="ok">
<p class="ni"><strong>ğŸ’¡ í˜„ì‹¤ì  ì¡°ì–¸:</strong> ê°œì¸ íŠ¸ë ˆì´ë”ê°€ ë‚˜ë…¸ì´ˆ ë‹¨ìœ„ì˜ HFTë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì ì´ë‹¤. í•˜ì§€ë§Œ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜, íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë¡œì§ì€ ì–´ë–¤ ê·œëª¨ì˜ ì‹œìŠ¤í…œì—ì„œë“  í•µì‹¬ì´ë‹¤. ìš°ë¦¬ì˜ ëª©í‘œëŠ” "HFTì˜ ì‚¬ê³ ë°©ì‹"ì„ ë°°ìš°ëŠ” ê²ƒì´ë‹¤.</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 5: ê°•í™”í•™ìŠµ ê¸°ì´ˆ â€” MDP, ë²¨ë§Œ ë°©ì •ì‹
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch5">Chapter 5. ê°•í™”í•™ìŠµ ê¸°ì´ˆ â€” MDPì™€ ë²¨ë§Œ ë°©ì •ì‹</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Reinforcement Learning Fundamentals" / í˜¼ê³µíŒŒ Ch.8 í´ë˜ìŠ¤ â€” RL ì—ì´ì „íŠ¸ë¥¼ í´ë˜ìŠ¤ë¡œ êµ¬í˜„</p>
</div>

<h3>5.1 ê°•í™”í•™ìŠµì´ë€ ë¬´ì—‡ì¸ê°€</h3>

<p>
R4~R7ì—ì„œ ë°°ìš´ ì§€ë„í•™ìŠµ(Supervised Learning)ì€ "ì •ë‹µì´ ìˆëŠ” ë°ì´í„°"ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí–ˆë‹¤. ê°•í™”í•™ìŠµ(Reinforcement Learning, RL)ì€ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥´ë‹¤. ì—ì´ì „íŠ¸(agent)ê°€ í™˜ê²½(environment)ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©°, ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ìµœì ì˜ í–‰ë™ ì „ëµ(policy)ì„ í•™ìŠµí•œë‹¤. ì •ë‹µì´ ì£¼ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ â€” ëŒ€ì‹  í–‰ë™ì˜ ê²°ê³¼ë¡œ ë³´ìƒ(reward)ì„ ë°›ê³ , ëˆ„ì  ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
</p>

<div style="margin:25px 0;display:flex;gap:15px;flex-wrap:wrap;justify-content:center;align-items:center">
<div style="background:#e8f4f8;padding:15px 20px;border-radius:10px;text-align:center;min-width:120px">
<p class="ni" style="font-size:28px">ğŸ¤–</p>
<p class="ni" style="font-weight:bold;font-size:13px">ì—ì´ì „íŠ¸</p>
<p class="ni" style="font-size:11px;color:#666">íŠ¸ë ˆì´ë”© ë´‡</p>
</div>
<div style="text-align:center;min-width:80px">
<p class="ni" style="font-size:12px;color:#e74c3c">í–‰ë™ (a<sub>t</sub>) â†’</p>
<p class="ni" style="font-size:12px;color:#27ae60">â† ë³´ìƒ (r<sub>t</sub>)</p>
<p class="ni" style="font-size:12px;color:#3498db">â† ìƒíƒœ (s<sub>t+1</sub>)</p>
</div>
<div style="background:#fff3cd;padding:15px 20px;border-radius:10px;text-align:center;min-width:120px">
<p class="ni" style="font-size:28px">ğŸ“ˆ</p>
<p class="ni" style="font-weight:bold;font-size:13px">í™˜ê²½</p>
<p class="ni" style="font-size:11px;color:#666">ê¸ˆìœµ ì‹œì¥</p>
</div>
</div>

<h3>5.2 ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì • (MDP)</h3>

<p>
ê°•í™”í•™ìŠµì˜ ìˆ˜í•™ì  í”„ë ˆì„ì›Œí¬ê°€ <strong>ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(Markov Decision Process, MDP)</strong>ì´ë‹¤. MDPëŠ” 5-íŠœí”Œ \((S, A, P, R, \gamma)\)ë¡œ ì •ì˜ëœë‹¤:
</p>

<table>
<tr><th>ìš”ì†Œ</th><th>ê¸°í˜¸</th><th>íŠ¸ë ˆì´ë”© ì˜ˆì‹œ</th></tr>
<tr><td>ìƒíƒœ ê³µê°„</td><td>\(S\)</td><td>ê°€ê²©, í¬ì§€ì…˜, ê¸°ìˆ ì  ì§€í‘œ, ì˜¤ë”ë¶ ìƒíƒœ</td></tr>
<tr><td>í–‰ë™ ê³µê°„</td><td>\(A\)</td><td>{ë§¤ìˆ˜, ë§¤ë„, í™€ë“œ}</td></tr>
<tr><td>ì „ì´ í™•ë¥ </td><td>\(P(s'|s,a)\)</td><td>í–‰ë™ í›„ ì‹œì¥ ìƒíƒœ ë³€í™” í™•ë¥ </td></tr>
<tr><td>ë³´ìƒ í•¨ìˆ˜</td><td>\(R(s,a,s')\)</td><td>ì‹¤í˜„ ìˆ˜ìµ, ìƒ¤í”„ë¹„ìœ¨, ë¦¬ìŠ¤í¬ ì¡°ì • ìˆ˜ìµ</td></tr>
<tr><td>í• ì¸ ì¸ì</td><td>\(\gamma \in [0,1]\)</td><td>ë¯¸ë˜ ë³´ìƒì˜ í˜„ì¬ ê°€ì¹˜ (ë³´í†µ 0.99)</td></tr>
</table>

<div class="def">
<p class="ni"><strong>ğŸ“– ë§ˆë¥´ì½”í”„ ì„±ì§ˆ (Markov Property)</strong></p>
<p class="ni" style="margin-top:8px">
ë¯¸ë˜ ìƒíƒœëŠ” í˜„ì¬ ìƒíƒœì—ë§Œ ì˜ì¡´í•˜ê³ , ê³¼ê±° ì´ë ¥ì—ëŠ” ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤: \(P(s_{t+1}|s_t, a_t) = P(s_{t+1}|s_1, a_1, \ldots, s_t, a_t)\). ê¸ˆìœµ ì‹œì¥ì€ ì—„ë°€íˆ ë§ˆë¥´ì½”í”„ê°€ ì•„ë‹ˆì§€ë§Œ, ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìƒíƒœì— í¬í•¨ì‹œí‚¤ë©´ ê·¼ì‚¬ì ìœ¼ë¡œ ë§ˆë¥´ì½”í”„ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆë‹¤ (ì˜ˆ: ê³¼ê±° Nì¼ ìˆ˜ìµë¥ ì„ ìƒíƒœì— í¬í•¨).
</p>
</div>

<h3>5.3 ê°€ì¹˜ í•¨ìˆ˜ì™€ ë²¨ë§Œ ë°©ì •ì‹</h3>

<p>
ì •ì±…(policy) \(\pi(a|s)\)ëŠ” ìƒíƒœ \(s\)ì—ì„œ í–‰ë™ \(a\)ë¥¼ ì„ íƒí•  í™•ë¥ ì´ë‹¤. ì •ì±…ì˜ "ì¢‹ìŒ"ì„ ì¸¡ì •í•˜ëŠ” ê²ƒì´ <strong>ê°€ì¹˜ í•¨ìˆ˜(Value Function)</strong>ë‹¤:
</p>

<div class="eq">
\[ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \,\middle|\, s_0 = s \right] \]
</div>

<p>
ìƒíƒœ-í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜(Q-function)ëŠ” ìƒíƒœ \(s\)ì—ì„œ í–‰ë™ \(a\)ë¥¼ ì·¨í•œ í›„ì˜ ê¸°ëŒ€ ëˆ„ì  ë³´ìƒì´ë‹¤:
</p>

<div class="eq">
\[ Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \,\middle|\, s_0 = s, a_0 = a \right] \]
</div>

<p>
<strong>ë²¨ë§Œ ë°©ì •ì‹(Bellman Equation)</strong>ì€ ê°€ì¹˜ í•¨ìˆ˜ì˜ ì¬ê·€ì  ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤:
</p>

<div class="eq">
\[ V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right] \]
</div>

<p>
ìµœì  ê°€ì¹˜ í•¨ìˆ˜ \(V^*(s)\)ì™€ ìµœì  Q-í•¨ìˆ˜ \(Q^*(s,a)\)ì— ëŒ€í•œ <strong>ë²¨ë§Œ ìµœì  ë°©ì •ì‹</strong>:
</p>

<div class="eq">
\[ Q^*(s, a) = \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma \max_{a'} Q^*(s', a') \right] \]
</div>

<p>
ì´ ë°©ì •ì‹ì´ Q-Learningì˜ ì´ë¡ ì  ê¸°ë°˜ì´ë‹¤. ìµœì  Q-í•¨ìˆ˜ë¥¼ ì•Œë©´, ìµœì  ì •ì±…ì€ ë‹¨ìˆœíˆ \(\pi^*(s) = \arg\max_a Q^*(s,a)\)ì´ë‹¤.
</p>

<!-- â˜… Plotly: MDP Grid World Value Function -->
<div id="plot-ch5-mdp" style="width:100%;height:450px;margin:20px 0"></div>
<script>
(function(){
  // Simple 5x5 grid world value function (computed via value iteration)
  var V=[
    [0.59,0.66,0.73,0.81,0.90],
    [0.53,0.59,0.66,0.73,0.81],
    [0.48,0.53,0.59,0.66,0.73],
    [0.43,0.48,0.53,0.59,0.66],
    [0.39,0.43,0.48,0.53,0.59]
  ];
  // Reverse for proper orientation
  V.reverse();
  var annotations=[];
  var arrows=[['â†—','â†’','â†’','â†’','â˜…'],['â†‘','â†—','â†’','â†’','â†‘'],['â†‘','â†‘','â†—','â†’','â†‘'],['â†‘','â†‘','â†‘','â†—','â†‘'],['â†‘','â†‘','â†‘','â†‘','â†—']];
  arrows.reverse();
  for(var i=0;i<5;i++){
    for(var j=0;j<5;j++){
      annotations.push({x:j,y:i,text:V[i][j].toFixed(2)+'<br>'+arrows[i][j],showarrow:false,font:{size:11,color:V[i][j]>0.7?'white':'black'}});
    }
  }
  Plotly.newPlot('plot-ch5-mdp',[{
    z:V,type:'heatmap',colorscale:'Viridis',
    hovertemplate:'(%{x},%{y})<br>V=%{z:.2f}',
    colorbar:{title:'V(s)',titleside:'right'}
  }],{
    title:{text:'ğŸ“Š 5Ã—5 Grid World: ê°€ì¹˜ í•¨ìˆ˜ V(s) + ìµœì  ì •ì±… (Î³=0.9)',font:{size:13}},
    xaxis:{title:'x',dtick:1,range:[-0.5,4.5]},
    yaxis:{title:'y',dtick:1,range:[-0.5,4.5]},
    annotations:annotations,
    margin:{t:45,b:50,l:50,r:80},paper_bgcolor:'#fafaf8'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ 5Ã—5 ê·¸ë¦¬ë“œ ì›”ë“œì—ì„œ ëª©í‘œ(â˜…, ìš°ìƒë‹¨)ê¹Œì§€ì˜ ê°€ì¹˜ í•¨ìˆ˜. ë°ì„ìˆ˜ë¡ ê°€ì¹˜ê°€ ë†’ë‹¤. í™”ì‚´í‘œëŠ” ìµœì  ì •ì±…(ì´ë™ ë°©í–¥).</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 6: Q-Learning
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch6">Chapter 6. Q-Learning â€” í…Œì´ë¸” ê¸°ë°˜ ê°•í™”í•™ìŠµ</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Q-Learning" / í˜¼ê³µíŒŒ Ch.5 ë”•ì…”ë„ˆë¦¬ â€” Q-í…Œì´ë¸”ì€ ë³¸ì§ˆì ìœ¼ë¡œ (ìƒíƒœ, í–‰ë™) â†’ ê°€ì¹˜ì˜ ë”•ì…”ë„ˆë¦¬ë‹¤</p>
</div>

<h3>6.1 Q-Learning ì•Œê³ ë¦¬ì¦˜</h3>

<p>
Q-Learningì€ ëª¨ë¸-í”„ë¦¬(model-free) ê°•í™”í•™ìŠµì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. í™˜ê²½ì˜ ì „ì´ í™•ë¥  \(P(s'|s,a)\)ë¥¼ ëª¨ë¥´ë”ë¼ë„, ê²½í—˜(experience)ë§Œìœ¼ë¡œ ìµœì  Q-í•¨ìˆ˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. í•µì‹¬ ì—…ë°ì´íŠ¸ ê·œì¹™:
</p>

<div class="eq">
\[ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right] \]
</div>

<p>
ì—¬ê¸°ì„œ \(\alpha\)ëŠ” í•™ìŠµë¥ , \(r_t + \gamma \max_{a'} Q(s_{t+1}, a')\)ëŠ” TD íƒ€ê²Ÿ(Temporal Difference target), ê·¸ë¦¬ê³  ëŒ€ê´„í˜¸ ì•ˆì˜ ì „ì²´ê°€ TD ì—ëŸ¬ë‹¤. TD ì—ëŸ¬ê°€ 0ì´ ë˜ë©´ ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì´ ë§Œì¡±ë˜ë¯€ë¡œ, Q-í…Œì´ë¸”ì´ ìˆ˜ë ´í•œ ê²ƒì´ë‹¤.
</p>

<h3>6.2 Îµ-Greedy íƒìƒ‰</h3>

<p>
ê°•í™”í•™ìŠµì˜ í•µì‹¬ ë”œë ˆë§ˆê°€ <strong>íƒìƒ‰-í™œìš© íŠ¸ë ˆì´ë“œì˜¤í”„(Exploration-Exploitation Trade-off)</strong>ë‹¤. í˜„ì¬ ìµœì„ ì´ë¼ê³  ìƒê°í•˜ëŠ” í–‰ë™ë§Œ í•˜ë©´(í™œìš©) ë” ì¢‹ì€ í–‰ë™ì„ ë°œê²¬í•˜ì§€ ëª»í•˜ê³ , ë¬´ì‘ìœ„ í–‰ë™ë§Œ í•˜ë©´(íƒìƒ‰) í•™ìŠµí•œ ê²ƒì„ í™œìš©í•˜ì§€ ëª»í•œë‹¤. Îµ-Greedy ì •ì±…ì€ ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ í•´ê²°í•œë‹¤:
</p>

<div class="eq">
\[ a_t = \begin{cases} \arg\max_a Q(s_t, a) & \text{í™•ë¥  } 1-\varepsilon \text{ (í™œìš©)} \\ \text{random action} & \text{í™•ë¥  } \varepsilon \text{ (íƒìƒ‰)} \end{cases} \]
</div>

<p>
ë³´í†µ \(\varepsilon\)ì„ 1.0ì—ì„œ ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¨ë‹¤(Îµ-decay). ì´ˆê¸°ì—ëŠ” ë§ì´ íƒìƒ‰í•˜ê³ , í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ í™œìš© ë¹„ì¤‘ì„ ë†’ì¸ë‹¤.
</p>

<h3>6.3 ê°„ë‹¨í•œ íŠ¸ë ˆì´ë”© Q-Learning</h3>

<p class="cc">â–¼ Q-Learning íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ (ì´ì‚° ìƒíƒœ)</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">class</span> <span class="nb">SimpleQLearner</span>:
    <span class="st">"""
    ìƒíƒœ: ê°€ê²© ë³€í™” ë°©í–¥ (up/down/flat) Ã— í¬ì§€ì…˜ (long/flat/short)
    í–‰ë™: buy / sell / hold
    """</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, n_states=<span class="nu">9</span>, n_actions=<span class="nu">3</span>,
                 alpha=<span class="nu">0.1</span>, gamma=<span class="nu">0.99</span>, epsilon=<span class="nu">1.0</span>):
        self.q_table = np.<span class="fn">zeros</span>((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = <span class="nu">0.01</span>
        self.epsilon_decay = <span class="nu">0.995</span>
        self.n_actions = n_actions

    <span class="kw">def</span> <span class="fn">get_state</span>(self, price_change, position):
        <span class="st">"""ê°€ê²© ë³€í™” + í¬ì§€ì…˜ â†’ ì´ì‚° ìƒíƒœ ì¸ë±ìŠ¤"""</span>
        <span class="kw">if</span> price_change > <span class="nu">0.001</span>:
            pc = <span class="nu">0</span>  <span class="cm"># up</span>
        <span class="kw">elif</span> price_change < -<span class="nu">0.001</span>:
            pc = <span class="nu">1</span>  <span class="cm"># down</span>
        <span class="kw">else</span>:
            pc = <span class="nu">2</span>  <span class="cm"># flat</span>
        pos = {<span class="nu">1</span>: <span class="nu">0</span>, <span class="nu">0</span>: <span class="nu">1</span>, -<span class="nu">1</span>: <span class="nu">2</span>}[position]
        <span class="kw">return</span> pc * <span class="nu">3</span> + pos

    <span class="kw">def</span> <span class="fn">choose_action</span>(self, state):
        <span class="kw">if</span> np.random.<span class="fn">random</span>() < self.epsilon:
            <span class="kw">return</span> np.random.<span class="fn">randint</span>(self.n_actions)
        <span class="kw">return</span> np.<span class="fn">argmax</span>(self.q_table[state])

    <span class="kw">def</span> <span class="fn">update</span>(self, state, action, reward, next_state):
        td_target = reward + self.gamma * np.<span class="fn">max</span>(self.q_table[next_state])
        td_error = td_target - self.q_table[state, action]
        self.q_table[state, action] += self.alpha * td_error
        <span class="cm"># Îµ ê°ì†Œ</span>
        self.epsilon = <span class="fn">max</span>(self.epsilon_min,
                            self.epsilon * self.epsilon_decay)

<span class="cm"># â”€â”€ í•™ìŠµ ë£¨í”„ â”€â”€</span>
agent = <span class="fn">SimpleQLearner</span>()
prices = np.<span class="fn">cumsum</span>(np.random.<span class="fn">randn</span>(<span class="nu">10000</span>) * <span class="nu">0.01</span>) + <span class="nu">100</span>
returns = np.<span class="fn">diff</span>(prices) / prices[:-<span class="nu">1</span>]

position = <span class="nu">0</span>  <span class="cm"># -1, 0, 1</span>
total_reward = <span class="nu">0</span>
rewards_history = []

<span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(returns) - <span class="nu">1</span>):
    state = agent.<span class="fn">get_state</span>(returns[t], position)
    action = agent.<span class="fn">choose_action</span>(state)

    <span class="cm"># í–‰ë™ ì‹¤í–‰: 0=buy, 1=sell, 2=hold</span>
    <span class="kw">if</span> action == <span class="nu">0</span> <span class="kw">and</span> position <= <span class="nu">0</span>:
        position = <span class="nu">1</span>
    <span class="kw">elif</span> action == <span class="nu">1</span> <span class="kw">and</span> position >= <span class="nu">0</span>:
        position = -<span class="nu">1</span>

    <span class="cm"># ë³´ìƒ: í¬ì§€ì…˜ Ã— ë‹¤ìŒ ìˆ˜ìµë¥ </span>
    reward = position * returns[t + <span class="nu">1</span>]
    total_reward += reward

    next_state = agent.<span class="fn">get_state</span>(returns[t + <span class="nu">1</span>], position)
    agent.<span class="fn">update</span>(state, action, reward, next_state)

    <span class="kw">if</span> (t + <span class="nu">1</span>) % <span class="nu">2000</span> == <span class="nu">0</span>:
        rewards_history.<span class="fn">append</span>(total_reward)
        <span class="fn">print</span>(<span class="st">f"Step {t+1:5d} | Cum Reward: {total_reward:.4f} | "</span>
              <span class="st">f"Îµ: {agent.epsilon:.3f}"</span>)

<span class="fn">print</span>(<span class="st">f"\nìµœì¢… Q-í…Œì´ë¸”:\n{agent.q_table.round(4)}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output (ì˜ˆì‹œ):</span>
Step  2000 | Cum Reward: 0.0312 | Îµ: 0.045
Step  4000 | Cum Reward: 0.0891 | Îµ: 0.010
Step  6000 | Cum Reward: 0.1523 | Îµ: 0.010
Step  8000 | Cum Reward: 0.2187 | Îµ: 0.010

ìµœì¢… Q-í…Œì´ë¸”:
[[ 0.0023  -0.0015  0.0008]   â† up + long
 [ 0.0031  -0.0042  0.0012]   â† up + flat
 ...
 [-0.0018  0.0027  0.0005]]   â† flat + short</div>

<!-- â˜… Plotly: Q-Table Heatmap -->
<div id="plot-ch6-qtable" style="width:100%;height:420px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(77);
  // 9 states x 3 actions Q-table
  var states=['Up+Long','Up+Flat','Up+Short','Down+Long','Down+Flat','Down+Short','Flat+Long','Flat+Flat','Flat+Short'];
  var actions=['Buy','Sell','Hold'];
  var z=[];
  for(var i=0;i<9;i++){
    var row=[];
    for(var j=0;j<3;j++){
      // Simulate learned Q-values with some structure
      var base=(rng()-0.5)*0.05;
      if(i<3&&j==0) base+=0.02; // up â†’ buy has higher Q
      if(i>=3&&i<6&&j==1) base+=0.02; // down â†’ sell has higher Q
      row.push(parseFloat(base.toFixed(4)));
    }
    z.push(row);
  }
  Plotly.newPlot('plot-ch6-qtable',[{
    z:z,x:actions,y:states,type:'heatmap',colorscale:'RdYlGn',
    hovertemplate:'ìƒíƒœ: %{y}<br>í–‰ë™: %{x}<br>Qê°’: %{z:.4f}',
    colorbar:{title:'Q(s,a)'}
  }],{
    title:{text:'ğŸ“Š Q-í…Œì´ë¸” íˆíŠ¸ë§µ: ìƒíƒœ Ã— í–‰ë™ â†’ Qê°’',font:{size:13}},
    xaxis:{title:'í–‰ë™ (Action)',side:'bottom'},
    yaxis:{title:'',autorange:'reversed'},
    margin:{t:45,b:50,l:120,r:80},paper_bgcolor:'#fafaf8'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ ì´ˆë¡=ë†’ì€ Qê°’(ì¢‹ì€ í–‰ë™), ë¹¨ê°•=ë‚®ì€ Qê°’(ë‚˜ìœ í–‰ë™). ê°€ê²© ìƒìŠ¹ ì‹œ Buy, í•˜ë½ ì‹œ Sellì˜ Qê°’ì´ ë†’ì€ íŒ¨í„´ì´ í•™ìŠµë¨.</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ Q-Learningì˜ í•œê³„:</strong> ìƒíƒœ ê³µê°„ì´ ì—°ì†ì ì´ê±°ë‚˜ ê³ ì°¨ì›ì´ë©´ Q-í…Œì´ë¸”ì„ ë§Œë“¤ ìˆ˜ ì—†ë‹¤. ì‹¤ì œ íŠ¸ë ˆì´ë”©ì—ì„œ ìƒíƒœëŠ” (ê°€ê²©, ê±°ë˜ëŸ‰, ê¸°ìˆ ì  ì§€í‘œ, í¬ì§€ì…˜, ...) ë“± ì—°ì†ì ì´ê³  ê³ ì°¨ì›ì´ë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì´ ë‹¤ìŒ ì¥ì˜ DQNì´ë‹¤.</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 7: DQN â€” ë”¥ Q-ë„¤íŠ¸ì›Œí¬
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch7">Chapter 7. DQN â€” ë”¥ Q-ë„¤íŠ¸ì›Œí¬</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Deep Q-Networks" / R7ì˜ ì‹ ê²½ë§ ê¸°ì´ˆ â€” DQNì€ Q-í…Œì´ë¸”ì„ ì‹ ê²½ë§ìœ¼ë¡œ ëŒ€ì²´í•œ ê²ƒ</p>
</div>

<h3>7.1 Q-í…Œì´ë¸”ì—ì„œ Q-ë„¤íŠ¸ì›Œí¬ë¡œ</h3>

<p>
Q-Learningì˜ Q-í…Œì´ë¸”ì€ ì´ì‚°ì ì´ê³  ìœ í•œí•œ ìƒíƒœ ê³µê°„ì—ì„œë§Œ ì‘ë™í•œë‹¤. ì—°ì† ìƒíƒœ ê³µê°„ì—ì„œëŠ” ì‹ ê²½ë§ìœ¼ë¡œ Q-í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•œë‹¤: \(Q(s, a; \theta) \approx Q^*(s, a)\). ì´ê²ƒì´ <strong>DQN(Deep Q-Network)</strong>ì´ë‹¤. DeepMindê°€ 2015ë…„ Natureì— ë°œí‘œí•˜ì—¬ Atari ê²Œì„ì—ì„œ ì¸ê°„ì„ ëŠ¥ê°€í•œ ê²ƒìœ¼ë¡œ ìœ ëª…í•˜ë‹¤.
</p>

<div class="eq">
\[ \mathcal{L}(\theta) = \mathbb{E} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right] \]
</div>

<h3>7.2 DQNì˜ í•µì‹¬ ê¸°ë²• ë‘ ê°€ì§€</h3>

<p>
ë‹¨ìˆœíˆ Q-í…Œì´ë¸”ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë°”ê¾¸ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ë‹¤. DQNì€ ë‘ ê°€ì§€ í•µì‹¬ ê¸°ë²•ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•œë‹¤:
</p>

<div style="margin:20px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#e8f4f8;padding:20px;border-radius:10px;border-left:4px solid #3498db">
<p class="ni" style="font-weight:bold;font-size:14px;color:#2980b9;margin-bottom:10px">1ï¸âƒ£ Experience Replay</p>
<p class="ni" style="font-size:12px">ê²½í—˜ \((s, a, r, s')\)ì„ ë²„í¼ì— ì €ì¥í•˜ê³ , ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµí•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ (1) ì—°ì†ëœ ê²½í—˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê¹¨ëœ¨ë¦¬ê³ , (2) í•˜ë‚˜ì˜ ê²½í—˜ì„ ì—¬ëŸ¬ ë²ˆ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>
</div>
<div style="flex:1;min-width:280px;background:#fff3cd;padding:20px;border-radius:10px;border-left:4px solid #f39c12">
<p class="ni" style="font-weight:bold;font-size:14px;color:#e67e22;margin-bottom:10px">2ï¸âƒ£ Target Network</p>
<p class="ni" style="font-size:12px">TD íƒ€ê²Ÿ ê³„ì‚°ì— ë³„ë„ì˜ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ \(\theta^-\)ë¥¼ ì‚¬ìš©í•œë‹¤. íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ(ì˜ˆ: 1000 ìŠ¤í…ë§ˆë‹¤) ë©”ì¸ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ "ì›€ì§ì´ëŠ” íƒ€ê²Ÿ" ë¬¸ì œë¥¼ ì™„í™”í•œë‹¤.</p>
</div>
</div>

<h3>7.3 DQN êµ¬í˜„</h3>

<p class="cc">â–¼ DQN ì—ì´ì „íŠ¸ (PyTorch)</p>
<pre>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> collections <span class="kw">import</span> deque
<span class="kw">import</span> random

<span class="kw">class</span> <span class="nb">QNetwork</span>(nn.Module):
    <span class="st">"""Q-í•¨ìˆ˜ ê·¼ì‚¬ ì‹ ê²½ë§"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, state_dim, action_dim, hidden=<span class="nu">128</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.net = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(state_dim, hidden),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden, hidden),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden, action_dim)
        )

    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        <span class="kw">return</span> self.<span class="fn">net</span>(x)

<span class="kw">class</span> <span class="nb">ReplayBuffer</span>:
    <span class="st">"""ê²½í—˜ ë¦¬í”Œë ˆì´ ë²„í¼"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, capacity=<span class="nu">10000</span>):
        self.buffer = <span class="fn">deque</span>(maxlen=capacity)

    <span class="kw">def</span> <span class="fn">push</span>(self, state, action, reward, next_state, done):
        self.buffer.<span class="fn">append</span>((state, action, reward, next_state, done))

    <span class="kw">def</span> <span class="fn">sample</span>(self, batch_size):
        batch = random.<span class="fn">sample</span>(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = <span class="fn">zip</span>(*batch)
        <span class="kw">return</span> (np.<span class="fn">array</span>(states), np.<span class="fn">array</span>(actions),
                np.<span class="fn">array</span>(rewards, dtype=np.float32),
                np.<span class="fn">array</span>(next_states),
                np.<span class="fn">array</span>(dones, dtype=np.float32))

    <span class="kw">def</span> <span class="fn">__len__</span>(self):
        <span class="kw">return</span> <span class="fn">len</span>(self.buffer)

<span class="kw">class</span> <span class="nb">DQNAgent</span>:
    <span class="st">"""DQN íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, state_dim, action_dim=<span class="nu">3</span>,
                 lr=<span class="nu">1e-3</span>, gamma=<span class="nu">0.99</span>, epsilon=<span class="nu">1.0</span>):
        self.q_net = <span class="fn">QNetwork</span>(state_dim, action_dim)
        self.target_net = <span class="fn">QNetwork</span>(state_dim, action_dim)
        self.target_net.<span class="fn">load_state_dict</span>(self.q_net.<span class="fn">state_dict</span>())
        self.optimizer = torch.optim.<span class="fn">Adam</span>(
            self.q_net.<span class="fn">parameters</span>(), lr=lr)
        self.buffer = <span class="fn">ReplayBuffer</span>()
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = <span class="nu">0.01</span>
        self.epsilon_decay = <span class="nu">0.995</span>
        self.action_dim = action_dim
        self.update_count = <span class="nu">0</span>

    <span class="kw">def</span> <span class="fn">choose_action</span>(self, state):
        <span class="kw">if</span> np.random.<span class="fn">random</span>() < self.epsilon:
            <span class="kw">return</span> np.random.<span class="fn">randint</span>(self.action_dim)
        state_t = torch.<span class="fn">FloatTensor</span>(state).<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
        <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
            q_vals = self.<span class="fn">q_net</span>(state_t)
        <span class="kw">return</span> q_vals.<span class="fn">argmax</span>().<span class="fn">item</span>()

    <span class="kw">def</span> <span class="fn">train_step</span>(self, batch_size=<span class="nu">64</span>):
        <span class="kw">if</span> <span class="fn">len</span>(self.buffer) < batch_size:
            <span class="kw">return</span>
        s, a, r, s2, d = self.buffer.<span class="fn">sample</span>(batch_size)
        s = torch.<span class="fn">FloatTensor</span>(s)
        a = torch.<span class="fn">LongTensor</span>(a)
        r = torch.<span class="fn">FloatTensor</span>(r)
        s2 = torch.<span class="fn">FloatTensor</span>(s2)
        d = torch.<span class="fn">FloatTensor</span>(d)

        <span class="cm"># í˜„ì¬ Qê°’</span>
        q_values = self.<span class="fn">q_net</span>(s).<span class="fn">gather</span>(<span class="nu">1</span>, a.<span class="fn">unsqueeze</span>(<span class="nu">1</span>)).<span class="fn">squeeze</span>()
        <span class="cm"># íƒ€ê²Ÿ Qê°’ (íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©)</span>
        <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
            next_q = self.<span class="fn">target_net</span>(s2).<span class="fn">max</span>(<span class="nu">1</span>)[<span class="nu">0</span>]
            target = r + self.gamma * next_q * (<span class="nu">1</span> - d)

        loss = nn.<span class="fn">MSELoss</span>()(q_values, target)
        self.optimizer.<span class="fn">zero_grad</span>()
        loss.<span class="fn">backward</span>()
        nn.utils.<span class="fn">clip_grad_norm_</span>(self.q_net.<span class="fn">parameters</span>(), <span class="nu">1.0</span>)
        self.optimizer.<span class="fn">step</span>()

        <span class="cm"># íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ë§¤ 100 ìŠ¤í…)</span>
        self.update_count += <span class="nu">1</span>
        <span class="kw">if</span> self.update_count % <span class="nu">100</span> == <span class="nu">0</span>:
            self.target_net.<span class="fn">load_state_dict</span>(
                self.q_net.<span class="fn">state_dict</span>())

        <span class="cm"># Îµ ê°ì†Œ</span>
        self.epsilon = <span class="fn">max</span>(self.epsilon_min,
                            self.epsilon * self.epsilon_decay)
        <span class="kw">return</span> loss.<span class="fn">item</span>()
</pre>
<div class="code-output"><span class="out-label">êµ¬ì¡° ìš”ì•½:</span>
QNetwork: Linear(state_dimâ†’128) â†’ ReLU â†’ Linear(128â†’128) â†’ ReLU â†’ Linear(128â†’3)
ReplayBuffer: deque(maxlen=10000), random sampling
DQNAgent: q_net + target_net + Îµ-greedy + experience replay</div>

<!-- â˜… Plotly: DQN Training Loss Curve -->
<div id="plot-ch7-dqn" style="width:100%;height:420px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(999);
  var N=200, episodes=[], losses=[], rewards=[], epsilons=[];
  var eps=1.0;
  for(var i=0;i<N;i++){
    episodes.push(i+1);
    // Simulated training curves
    var baseLoss=0.05/(1+i*0.02)+0.005+(rng()-0.5)*0.005;
    losses.push(Math.max(0.001,baseLoss));
    var baseReward=-0.5+i*0.008+(rng()-0.5)*0.3;
    rewards.push(baseReward);
    eps=Math.max(0.01,eps*0.985);
    epsilons.push(eps);
  }
  Plotly.newPlot('plot-ch7-dqn',[
    {x:episodes,y:losses,type:'scatter',mode:'lines',name:'Loss',line:{color:'#e74c3c',width:1.5}},
    {x:episodes,y:rewards,type:'scatter',mode:'lines',name:'Episode Reward',line:{color:'#2ecc71',width:1.5},yaxis:'y2'},
    {x:episodes,y:epsilons,type:'scatter',mode:'lines',name:'Îµ (íƒìƒ‰ë¥ )',line:{color:'#3498db',width:1,dash:'dot'},yaxis:'y3'}
  ],{
    title:{text:'ğŸ“Š DQN í•™ìŠµ ê³¡ì„ : Loss, Reward, Îµ ë³€í™”',font:{size:13}},
    xaxis:{title:'ì—í”¼ì†Œë“œ'},
    yaxis:{title:'Loss',side:'left',titlefont:{color:'#e74c3c'},range:[0,0.08]},
    yaxis2:{title:'Reward',overlaying:'y',side:'right',titlefont:{color:'#2ecc71'},position:1.0},
    yaxis3:{title:'Îµ',overlaying:'y',side:'right',showgrid:false,titlefont:{color:'#3498db'},position:0.95,range:[0,1.1],visible:false},
    legend:{x:0.4,y:1.15,orientation:'h',bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:60,b:50,r:80},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ ë¹¨ê°•=Loss ê°ì†Œ, ì´ˆë¡=ì—í”¼ì†Œë“œ ë³´ìƒ ì¦ê°€, íŒŒë‘ ì ì„ =Îµ ê°ì†Œ. í•™ìŠµ ì´ˆê¸°ì—ëŠ” íƒìƒ‰ ìœ„ì£¼, í›„ë°˜ì—ëŠ” í™œìš© ìœ„ì£¼.</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 8: Policy Gradient â€” REINFORCE
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch8">Chapter 8. Policy Gradient â€” REINFORCE ì•Œê³ ë¦¬ì¦˜</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Policy Gradient Methods" / R8 Gradient Descent ë³µìŠµ â€” Policy GradientëŠ” ì •ì±… íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê²½ì‚¬ ìƒìŠ¹ë²•</p>
</div>

<h3>8.1 Value-Based vs Policy-Based</h3>

<p>
Q-Learningê³¼ DQNì€ <strong>ê°€ì¹˜ ê¸°ë°˜(Value-Based)</strong> ë°©ë²•ì´ë‹¤ â€” Q-í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ê³ , ê·¸ë¡œë¶€í„° ì •ì±…ì„ ìœ ë„í•œë‹¤. ë°˜ë©´ <strong>ì •ì±… ê¸°ë°˜(Policy-Based)</strong> ë°©ë²•ì€ ì •ì±… \(\pi_\theta(a|s)\)ë¥¼ ì§ì ‘ íŒŒë¼ë¯¸í„°í™”í•˜ê³  ìµœì í™”í•œë‹¤. ì •ì±… ê¸°ë°˜ ë°©ë²•ì˜ ì¥ì :
</p>

<table>
<tr><th>íŠ¹ì„±</th><th>Value-Based (DQN)</th><th>Policy-Based (PG)</th></tr>
<tr><td>í–‰ë™ ê³µê°„</td><td>ì´ì‚°ë§Œ ê°€ëŠ¥</td><td>ì´ì‚° + ì—°ì† ëª¨ë‘ ê°€ëŠ¥</td></tr>
<tr><td>ì •ì±… ìœ í˜•</td><td>ê²°ì •ì  (argmax)</td><td>í™•ë¥ ì  (stochastic)</td></tr>
<tr><td>ìˆ˜ë ´</td><td>ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ</td><td>ë” ì•ˆì •ì  (ì‘ì€ ì—…ë°ì´íŠ¸)</td></tr>
<tr><td>ë¶„ì‚°</td><td>ë‚®ìŒ</td><td>ë†’ìŒ (Monte Carlo ì¶”ì •)</td></tr>
<tr><td>íŠ¸ë ˆì´ë”© ì ìš©</td><td>ì´ì‚° í–‰ë™ (buy/sell/hold)</td><td>ì—°ì† í–‰ë™ (ë¹„ì¤‘ ì¡°ì ˆ)</td></tr>
</table>

<h3>8.2 Policy Gradient ì •ë¦¬</h3>

<p>
ëª©ì  í•¨ìˆ˜ \(J(\theta) = \mathbb{E}_{\pi_\theta}[\sum_t \gamma^t r_t]\)ë¥¼ ìµœëŒ€í™”í•˜ê³  ì‹¶ë‹¤. <strong>Policy Gradient Theorem</strong>ì— ì˜í•´:
</p>

<div class="eq">
\[ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right] \]
</div>

<p>
ì—¬ê¸°ì„œ \(G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k\)ëŠ” ì‹œì  \(t\)ë¶€í„°ì˜ ëˆ„ì  ë³´ìƒ(return)ì´ë‹¤. ì§ê´€ì ìœ¼ë¡œ: ë†’ì€ ë³´ìƒì„ ë°›ì€ í–‰ë™ì˜ í™•ë¥ ì„ ë†’ì´ê³ , ë‚®ì€ ë³´ìƒì„ ë°›ì€ í–‰ë™ì˜ í™•ë¥ ì„ ë‚®ì¶˜ë‹¤.
</p>

<h3>8.3 REINFORCE ì•Œê³ ë¦¬ì¦˜</h3>

<p>
REINFORCEëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ Policy Gradient ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì—í”¼ì†Œë“œë¥¼ ëê¹Œì§€ ì‹¤í–‰í•œ í›„, ê° ì‹œì ì˜ \(\log \pi \cdot G_t\)ë¥¼ ê³„ì‚°í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
</p>

<p class="cc">â–¼ REINFORCE íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸</p>
<pre>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">class</span> <span class="nb">PolicyNetwork</span>(nn.Module):
    <span class="st">"""ì •ì±… ë„¤íŠ¸ì›Œí¬: ìƒíƒœ â†’ í–‰ë™ í™•ë¥ """</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, state_dim, action_dim=<span class="nu">3</span>, hidden=<span class="nu">64</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.net = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(state_dim, hidden),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden, hidden),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden, action_dim),
            nn.<span class="fn">Softmax</span>(dim=-<span class="nu">1</span>)
        )

    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        <span class="kw">return</span> self.<span class="fn">net</span>(x)

<span class="kw">class</span> <span class="nb">REINFORCEAgent</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, state_dim, action_dim=<span class="nu">3</span>,
                 lr=<span class="nu">1e-3</span>, gamma=<span class="nu">0.99</span>):
        self.policy = <span class="fn">PolicyNetwork</span>(state_dim, action_dim)
        self.optimizer = torch.optim.<span class="fn">Adam</span>(
            self.policy.<span class="fn">parameters</span>(), lr=lr)
        self.gamma = gamma
        self.log_probs = []
        self.rewards = []

    <span class="kw">def</span> <span class="fn">choose_action</span>(self, state):
        state_t = torch.<span class="fn">FloatTensor</span>(state).<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
        probs = self.<span class="fn">policy</span>(state_t)
        dist = torch.distributions.<span class="fn">Categorical</span>(probs)
        action = dist.<span class="fn">sample</span>()
        self.log_probs.<span class="fn">append</span>(dist.<span class="fn">log_prob</span>(action))
        <span class="kw">return</span> action.<span class="fn">item</span>()

    <span class="kw">def</span> <span class="fn">store_reward</span>(self, reward):
        self.rewards.<span class="fn">append</span>(reward)

    <span class="kw">def</span> <span class="fn">update</span>(self):
        <span class="st">"""ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì—…ë°ì´íŠ¸"""</span>
        <span class="cm"># í• ì¸ ëˆ„ì  ë³´ìƒ ê³„ì‚°</span>
        returns = []
        G = <span class="nu">0</span>
        <span class="kw">for</span> r <span class="kw">in</span> <span class="fn">reversed</span>(self.rewards):
            G = r + self.gamma * G
            returns.<span class="fn">insert</span>(<span class="nu">0</span>, G)
        returns = torch.<span class="fn">tensor</span>(returns)

        <span class="cm"># ì •ê·œí™” (ë¶„ì‚° ê°ì†Œ)</span>
        <span class="kw">if</span> returns.<span class="fn">std</span>() > <span class="nu">0</span>:
            returns = (returns - returns.<span class="fn">mean</span>()) / (returns.<span class="fn">std</span>() + <span class="nu">1e-8</span>)

        <span class="cm"># Policy Gradient ì†ì‹¤</span>
        loss = <span class="nu">0</span>
        <span class="kw">for</span> log_prob, G <span class="kw">in</span> <span class="fn">zip</span>(self.log_probs, returns):
            loss -= log_prob * G  <span class="cm"># ê²½ì‚¬ ìƒìŠ¹ â†’ ìŒìˆ˜ ë¶€í˜¸</span>

        self.optimizer.<span class="fn">zero_grad</span>()
        loss.<span class="fn">backward</span>()
        nn.utils.<span class="fn">clip_grad_norm_</span>(self.policy.<span class="fn">parameters</span>(), <span class="nu">1.0</span>)
        self.optimizer.<span class="fn">step</span>()

        <span class="cm"># ë²„í¼ ì´ˆê¸°í™”</span>
        self.log_probs = []
        self.rewards = []
        <span class="kw">return</span> loss.<span class="fn">item</span>()
</pre>
<div class="code-output"><span class="out-label">êµ¬ì¡° ìš”ì•½:</span>
PolicyNetwork: state â†’ [64] â†’ ReLU â†’ [64] â†’ ReLU â†’ [3] â†’ Softmax
ì¶œë ¥: P(buy), P(sell), P(hold) â€” í™•ë¥  ë¶„í¬
REINFORCE: ì—í”¼ì†Œë“œ ëì— í•œ ë²ˆ ì—…ë°ì´íŠ¸ (Monte Carlo)</div>

<h3>8.4 Baselineì„ ì´ìš©í•œ ë¶„ì‚° ê°ì†Œ</h3>

<p>
REINFORCEì˜ ê°€ì¥ í° ë¬¸ì œëŠ” ë†’ì€ ë¶„ì‚°ì´ë‹¤. ëª¨ë“  ë³´ìƒì´ ì–‘ìˆ˜ì´ë©´ ëª¨ë“  í–‰ë™ì˜ í™•ë¥ ì´ ì˜¬ë¼ê°€ëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong>baseline</strong> \(b(s)\)ë¥¼ ë¹¼ì¤€ë‹¤:
</p>

<div class="eq">
\[ \nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (G_t - b(s_t)) \right] \]
</div>

<p>
ê°€ì¥ í”í•œ baselineì€ ê°€ì¹˜ í•¨ìˆ˜ \(V(s)\)ë‹¤. \(G_t - V(s_t)\)ëŠ” <strong>ì–´ë“œë°´í‹°ì§€(Advantage)</strong> \(A(s_t, a_t)\)ë¼ ë¶€ë¥´ë©°, "ì´ í–‰ë™ì´ í‰ê· ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì¢‹ì•˜ëŠ”ê°€"ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ê²ƒì´ ë‹¤ìŒ ì¥ì˜ Actor-Criticìœ¼ë¡œ ì´ì–´ì§„ë‹¤.
</p>

<!-- â˜… Plotly: Policy Gradient Reward Curve -->
<div id="plot-ch8-pg" style="width:100%;height:420px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(555);
  var N=150, eps=[], rw_reinforce=[], rw_baseline=[];
  var cum1=0,cum2=0;
  for(var i=0;i<N;i++){
    eps.push(i+1);
    var base=-0.3+i*0.006;
    var r1=base+(rng()-0.5)*0.8;
    var r2=base+(rng()-0.5)*0.4; // baseline has lower variance
    rw_reinforce.push(r1);
    rw_baseline.push(r2);
  }
  // Moving average
  function movAvg(arr,w){var out=[];for(var i=0;i<arr.length;i++){var s=0,c=0;for(var j=Math.max(0,i-w+1);j<=i;j++){s+=arr[j];c++;}out.push(s/c);}return out;}
  var ma1=movAvg(rw_reinforce,20), ma2=movAvg(rw_baseline,20);
  Plotly.newPlot('plot-ch8-pg',[
    {x:eps,y:rw_reinforce,type:'scatter',mode:'markers',name:'REINFORCE (raw)',marker:{size:3,color:'rgba(231,76,60,0.3)'}},
    {x:eps,y:ma1,type:'scatter',mode:'lines',name:'REINFORCE (MA20)',line:{color:'#e74c3c',width:2}},
    {x:eps,y:rw_baseline,type:'scatter',mode:'markers',name:'+ Baseline (raw)',marker:{size:3,color:'rgba(46,204,113,0.3)'}},
    {x:eps,y:ma2,type:'scatter',mode:'lines',name:'+ Baseline (MA20)',line:{color:'#2ecc71',width:2}}
  ],{
    title:{text:'ğŸ“Š REINFORCE vs REINFORCE + Baseline: ì—í”¼ì†Œë“œ ë³´ìƒ ë¹„êµ',font:{size:13}},
    xaxis:{title:'ì—í”¼ì†Œë“œ'},
    yaxis:{title:'ì—í”¼ì†Œë“œ ë³´ìƒ',zeroline:true},
    legend:{x:0.01,y:0.99,bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:45,b:50},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff',
    annotations:[{x:120,y:ma2[119],text:'Baseline ì¶”ê°€ â†’<br>ë¶„ì‚° ê°ì†Œ',showarrow:true,arrowhead:2,ax:50,ay:30,font:{size:10,color:'#27ae60'}}]
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ ë¹¨ê°•=ìˆœìˆ˜ REINFORCE (ë†’ì€ ë¶„ì‚°), ì´ˆë¡=Baseline ì¶”ê°€ (ë‚®ì€ ë¶„ì‚°). ë‘ ë°©ë²• ëª¨ë‘ ìˆ˜ë ´í•˜ì§€ë§Œ, Baselineì´ í›¨ì”¬ ì•ˆì •ì .</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 9: Actor-Critic & PPO
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch9">Chapter 9. Actor-Critic &amp; PPO â€” ê·¼ì ‘ ì •ì±… ìµœì í™”</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.22 "Actor-Critic Methods" / MLAT Ch.23 "Proximal Policy Optimization"</p>
</div>

<h3>9.1 Actor-Critic ì•„í‚¤í…ì²˜</h3>

<p>
Actor-Criticì€ Value-Basedì™€ Policy-Basedì˜ ì¥ì ì„ ê²°í•©í•œë‹¤. <strong>Actor</strong>(ì •ì±… ë„¤íŠ¸ì›Œí¬)ê°€ í–‰ë™ì„ ì„ íƒí•˜ê³ , <strong>Critic</strong>(ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬)ì´ ê·¸ í–‰ë™ì„ í‰ê°€í•œë‹¤. REINFORCEì˜ Monte Carlo ë¦¬í„´ \(G_t\) ëŒ€ì‹  Criticì˜ TD ì¶”ì •ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë¶„ì‚°ì´ í¬ê²Œ ì¤„ì–´ë“ ë‹¤.
</p>

<div style="margin:25px 0;display:flex;gap:15px;flex-wrap:wrap;justify-content:center;align-items:center">
<div style="background:#e8f4f8;padding:15px 20px;border-radius:10px;text-align:center;min-width:140px">
<p class="ni" style="font-size:24px">ğŸ­</p>
<p class="ni" style="font-weight:bold;font-size:13px">Actor \(\pi_\theta(a|s)\)</p>
<p class="ni" style="font-size:11px;color:#666">"ì–´ë–¤ í–‰ë™ì„ í• ê¹Œ?"</p>
</div>
<div style="text-align:center;min-width:60px">
<p class="ni" style="font-size:20px">â‡„</p>
</div>
<div style="background:#fff3cd;padding:15px 20px;border-radius:10px;text-align:center;min-width:140px">
<p class="ni" style="font-size:24px">ğŸ“Š</p>
<p class="ni" style="font-weight:bold;font-size:13px">Critic \(V_\phi(s)\)</p>
<p class="ni" style="font-size:11px;color:#666">"ê·¸ í–‰ë™ì´ ì–¼ë§ˆë‚˜ ì¢‹ì•˜ë‚˜?"</p>
</div>
</div>

<div class="eq">
\[ \text{Advantage: } A(s_t, a_t) = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t) \]
</div>

<h3>9.2 PPO (Proximal Policy Optimization)</h3>

<p>
PPOëŠ” OpenAIê°€ 2017ë…„ì— ë°œí‘œí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, í˜„ì¬ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” RL ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ChatGPTì˜ RLHF(Reinforcement Learning from Human Feedback)ì—ë„ PPOê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. PPOì˜ í•µì‹¬ ì•„ì´ë””ì–´: ì •ì±… ì—…ë°ì´íŠ¸ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë³´ì¥í•œë‹¤.
</p>

<div class="eq">
\[ L^{CLIP}(\theta) = \mathbb{E} \left[ \min\left( r_t(\theta) A_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] \]
</div>

<p>
ì—¬ê¸°ì„œ \(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\)ëŠ” í™•ë¥  ë¹„ìœ¨ì´ê³ , \(\epsilon\)(ë³´í†µ 0.2)ì€ í´ë¦¬í•‘ ë²”ìœ„ë‹¤. í™•ë¥  ë¹„ìœ¨ì´ \([1-\epsilon, 1+\epsilon]\) ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ í´ë¦¬í•‘ë˜ì–´ ë„ˆë¬´ í° ì—…ë°ì´íŠ¸ë¥¼ ë°©ì§€í•œë‹¤.
</p>

<h3>9.3 PPO êµ¬í˜„</h3>

<p class="cc">â–¼ PPO íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸</p>
<pre>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">class</span> <span class="nb">ActorCritic</span>(nn.Module):
    <span class="st">"""Actor-Critic ê³µìœ  ë„¤íŠ¸ì›Œí¬"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, state_dim, action_dim=<span class="nu">3</span>, hidden=<span class="nu">128</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        <span class="cm"># ê³µìœ  íŠ¹ì§• ì¶”ì¶œê¸°</span>
        self.shared = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(state_dim, hidden),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden, hidden),
            nn.<span class="fn">ReLU</span>()
        )
        <span class="cm"># Actor í—¤ë“œ (ì •ì±…)</span>
        self.actor = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(hidden, action_dim),
            nn.<span class="fn">Softmax</span>(dim=-<span class="nu">1</span>)
        )
        <span class="cm"># Critic í—¤ë“œ (ê°€ì¹˜)</span>
        self.critic = nn.<span class="fn">Linear</span>(hidden, <span class="nu">1</span>)

    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        features = self.<span class="fn">shared</span>(x)
        policy = self.<span class="fn">actor</span>(features)
        value = self.<span class="fn">critic</span>(features)
        <span class="kw">return</span> policy, value

<span class="kw">class</span> <span class="nb">PPOAgent</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, state_dim, action_dim=<span class="nu">3</span>,
                 lr=<span class="nu">3e-4</span>, gamma=<span class="nu">0.99</span>, clip_eps=<span class="nu">0.2</span>,
                 epochs=<span class="nu">4</span>, batch_size=<span class="nu">64</span>):
        self.model = <span class="fn">ActorCritic</span>(state_dim, action_dim)
        self.optimizer = torch.optim.<span class="fn">Adam</span>(
            self.model.<span class="fn">parameters</span>(), lr=lr)
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.epochs = epochs
        self.batch_size = batch_size
        <span class="cm"># ì—í”¼ì†Œë“œ ë²„í¼</span>
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []
        self.values = []
        self.dones = []

    <span class="kw">def</span> <span class="fn">choose_action</span>(self, state):
        state_t = torch.<span class="fn">FloatTensor</span>(state).<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
        <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
            probs, value = self.<span class="fn">model</span>(state_t)
        dist = torch.distributions.<span class="fn">Categorical</span>(probs)
        action = dist.<span class="fn">sample</span>()
        self.states.<span class="fn">append</span>(state)
        self.actions.<span class="fn">append</span>(action.<span class="fn">item</span>())
        self.log_probs.<span class="fn">append</span>(dist.<span class="fn">log_prob</span>(action).<span class="fn">item</span>())
        self.values.<span class="fn">append</span>(value.<span class="fn">item</span>())
        <span class="kw">return</span> action.<span class="fn">item</span>()

    <span class="kw">def</span> <span class="fn">compute_gae</span>(self, next_value, lam=<span class="nu">0.95</span>):
        <span class="st">"""Generalized Advantage Estimation"""</span>
        advantages = []
        gae = <span class="nu">0</span>
        values = self.values + [next_value]
        <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">reversed</span>(<span class="fn">range</span>(<span class="fn">len</span>(self.rewards))):
            delta = (self.rewards[t] + self.gamma *
                     values[t+<span class="nu">1</span>] * (<span class="nu">1</span>-self.dones[t]) - values[t])
            gae = delta + self.gamma * lam * (<span class="nu">1</span>-self.dones[t]) * gae
            advantages.<span class="fn">insert</span>(<span class="nu">0</span>, gae)
        returns = [a + v <span class="kw">for</span> a, v <span class="kw">in</span> <span class="fn">zip</span>(advantages, self.values)]
        <span class="kw">return</span> advantages, returns

    <span class="kw">def</span> <span class="fn">update</span>(self):
        <span class="cm"># GAE ê³„ì‚°</span>
        <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
            last_state = torch.<span class="fn">FloatTensor</span>(self.states[-<span class="nu">1</span>])
            _, next_val = self.<span class="fn">model</span>(last_state.<span class="fn">unsqueeze</span>(<span class="nu">0</span>))
        advantages, returns = self.<span class="fn">compute_gae</span>(next_val.<span class="fn">item</span>())

        <span class="cm"># í…ì„œ ë³€í™˜</span>
        states = torch.<span class="fn">FloatTensor</span>(np.<span class="fn">array</span>(self.states))
        actions = torch.<span class="fn">LongTensor</span>(self.actions)
        old_log_probs = torch.<span class="fn">FloatTensor</span>(self.log_probs)
        advantages = torch.<span class="fn">FloatTensor</span>(advantages)
        returns = torch.<span class="fn">FloatTensor</span>(returns)

        <span class="cm"># ì •ê·œí™”</span>
        advantages = (advantages - advantages.<span class="fn">mean</span>()) / (advantages.<span class="fn">std</span>() + <span class="nu">1e-8</span>)

        <span class="cm"># PPO ì—…ë°ì´íŠ¸ (ì—¬ëŸ¬ ì—í­)</span>
        <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(self.epochs):
            probs, values = self.<span class="fn">model</span>(states)
            dist = torch.distributions.<span class="fn">Categorical</span>(probs)
            new_log_probs = dist.<span class="fn">log_prob</span>(actions)

            <span class="cm"># í™•ë¥  ë¹„ìœ¨</span>
            ratio = torch.<span class="fn">exp</span>(new_log_probs - old_log_probs)

            <span class="cm"># í´ë¦¬í•‘ëœ ëª©ì  í•¨ìˆ˜</span>
            surr1 = ratio * advantages
            surr2 = torch.<span class="fn">clamp</span>(ratio, <span class="nu">1</span>-self.clip_eps,
                                <span class="nu">1</span>+self.clip_eps) * advantages
            actor_loss = -torch.<span class="fn">min</span>(surr1, surr2).<span class="fn">mean</span>()

            <span class="cm"># Critic ì†ì‹¤</span>
            critic_loss = nn.<span class="fn">MSELoss</span>()(values.<span class="fn">squeeze</span>(), returns)

            <span class="cm"># ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (íƒìƒ‰ ì¥ë ¤)</span>
            entropy = dist.<span class="fn">entropy</span>().<span class="fn">mean</span>()

            loss = actor_loss + <span class="nu">0.5</span> * critic_loss - <span class="nu">0.01</span> * entropy

            self.optimizer.<span class="fn">zero_grad</span>()
            loss.<span class="fn">backward</span>()
            nn.utils.<span class="fn">clip_grad_norm_</span>(
                self.model.<span class="fn">parameters</span>(), <span class="nu">0.5</span>)
            self.optimizer.<span class="fn">step</span>()

        <span class="cm"># ë²„í¼ ì´ˆê¸°í™”</span>
        self.states, self.actions = [], []
        self.rewards, self.log_probs = [], []
        self.values, self.dones = [], []
</pre>
<div class="code-output"><span class="out-label">PPO í•µì‹¬ êµ¬ì„±:</span>
1. ActorCritic: ê³µìœ  íŠ¹ì§• ì¶”ì¶œ + Actor(ì •ì±…) + Critic(ê°€ì¹˜) í—¤ë“œ
2. GAE (Î»=0.95): ë¶„ì‚°-í¸í–¥ íŠ¸ë ˆì´ë“œì˜¤í”„ ìµœì í™”
3. Clipped Objective (Îµ=0.2): ì •ì±… ì—…ë°ì´íŠ¸ í¬ê¸° ì œí•œ
4. Entropy Bonus (0.01): íƒìƒ‰ ì¥ë ¤
5. Multiple Epochs (4): ê°™ì€ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸</div>

<div class="ok">
<p class="ni"><strong>ğŸ’¡ PPOê°€ íŠ¸ë ˆì´ë”©ì— ì í•©í•œ ì´ìœ :</strong> (1) ì•ˆì •ì ì¸ í•™ìŠµ â€” ê¸ˆìœµ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ê°•ê±´, (2) í™•ë¥ ì  ì •ì±… â€” í¬ì§€ì…˜ ë¹„ì¤‘ì„ ì—°ì†ì ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥, (3) ìƒ˜í”Œ íš¨ìœ¨ì„± â€” ê°™ì€ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ë²ˆ í•™ìŠµ, (4) êµ¬í˜„ ìš©ì´ì„± â€” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ë¹„êµì  ì‰¬ì›€.</p>
</div>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 10: íŠ¸ë ˆì´ë”© í™˜ê²½ ì„¤ê³„
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch10">Chapter 10. íŠ¸ë ˆì´ë”© í™˜ê²½ ì„¤ê³„ â€” Gymnasium ìŠ¤íƒ€ì¼</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.23 "Building Trading Environments" / í˜¼ê³µíŒŒ Ch.8 í´ë˜ìŠ¤ ìƒì† â€” Gymnasium í™˜ê²½ì€ í´ë˜ìŠ¤ ìƒì†ìœ¼ë¡œ êµ¬í˜„</p>
</div>

<h3>10.1 Gymnasium (OpenAI Gym) ì¸í„°í˜ì´ìŠ¤</h3>

<p>
ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚¤ë ¤ë©´ í™˜ê²½(environment)ì´ í•„ìš”í•˜ë‹¤. Gymnasium(êµ¬ OpenAI Gym)ì€ RL í™˜ê²½ì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•œë‹¤. ëª¨ë“  í™˜ê²½ì€ <code>reset()</code>, <code>step(action)</code> ë‘ ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ëœë‹¤.
</p>

<div class="eq">
\[ \texttt{obs, info} = \texttt{env.reset()} \]
\[ \texttt{obs, reward, terminated, truncated, info} = \texttt{env.step(action)} \]
</div>

<h3>10.2 íŠ¸ë ˆì´ë”© í™˜ê²½ êµ¬í˜„</h3>

<p>
ìš°ë¦¬ì˜ íŠ¸ë ˆì´ë”© í™˜ê²½ì€ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ê³„í•œë‹¤:
</p>

<table>
<tr><th>ìš”ì†Œ</th><th>ì„¤ê³„</th></tr>
<tr><td>ìƒíƒœ (observation)</td><td>ê³¼ê±° Nì¼ ìˆ˜ìµë¥  + ê¸°ìˆ ì  ì§€í‘œ + í˜„ì¬ í¬ì§€ì…˜ + ë¯¸ì‹¤í˜„ P&L</td></tr>
<tr><td>í–‰ë™ (action)</td><td>0=Hold, 1=Buy, 2=Sell (ì´ì‚°) ë˜ëŠ” [-1, +1] ë¹„ì¤‘ (ì—°ì†)</td></tr>
<tr><td>ë³´ìƒ (reward)</td><td>ë¦¬ìŠ¤í¬ ì¡°ì • ìˆ˜ìµë¥  (Sharpe-like)</td></tr>
<tr><td>ì¢…ë£Œ ì¡°ê±´</td><td>ë°ì´í„° ë ë„ë‹¬ ë˜ëŠ” ìµœëŒ€ ë‚™í­ ì´ˆê³¼</td></tr>
</table>

<p class="cc">â–¼ íŠ¸ë ˆì´ë”© í™˜ê²½ (Gymnasium ìŠ¤íƒ€ì¼)</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> gymnasium <span class="kw">as</span> gym
<span class="kw">from</span> gymnasium <span class="kw">import</span> spaces

<span class="kw">class</span> <span class="nb">TradingEnv</span>(gym.Env):
    <span class="st">"""
    ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© í™˜ê²½
    - ìƒíƒœ: [ê³¼ê±° Nì¼ ìˆ˜ìµë¥ , RSI, í¬ì§€ì…˜, ë¯¸ì‹¤í˜„ P&L]
    - í–‰ë™: 0=Hold, 1=Buy, 2=Sell
    - ë³´ìƒ: ë¦¬ìŠ¤í¬ ì¡°ì • ìˆ˜ìµë¥ 
    """</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, prices, window=<span class="nu">20</span>,
                 transaction_cost=<span class="nu">0.001</span>, max_drawdown=<span class="nu">0.2</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.prices = prices
        self.returns = np.<span class="fn">diff</span>(prices) / prices[:-<span class="nu">1</span>]
        self.window = window
        self.tc = transaction_cost
        self.max_dd = max_drawdown

        <span class="cm"># ìƒíƒœ: windowê°œ ìˆ˜ìµë¥  + RSI + í¬ì§€ì…˜ + ë¯¸ì‹¤í˜„ P&L</span>
        self.observation_space = spaces.<span class="fn">Box</span>(
            low=-np.inf, high=np.inf,
            shape=(window + <span class="nu">3</span>,), dtype=np.float32)
        <span class="cm"># í–‰ë™: Hold, Buy, Sell</span>
        self.action_space = spaces.<span class="fn">Discrete</span>(<span class="nu">3</span>)

    <span class="kw">def</span> <span class="fn">_compute_rsi</span>(self, returns, period=<span class="nu">14</span>):
        <span class="st">"""RSI ê³„ì‚°"""</span>
        gains = np.<span class="fn">maximum</span>(returns, <span class="nu">0</span>)
        losses = np.<span class="fn">maximum</span>(-returns, <span class="nu">0</span>)
        avg_gain = np.<span class="fn">mean</span>(gains[-period:])
        avg_loss = np.<span class="fn">mean</span>(losses[-period:]) + <span class="nu">1e-10</span>
        rs = avg_gain / avg_loss
        <span class="kw">return</span> <span class="nu">100</span> - <span class="nu">100</span> / (<span class="nu">1</span> + rs)

    <span class="kw">def</span> <span class="fn">_get_obs</span>(self):
        <span class="st">"""í˜„ì¬ ê´€ì¸¡ ë²¡í„° ìƒì„±"""</span>
        ret_window = self.returns[
            self.idx - self.window : self.idx]
        rsi = self.<span class="fn">_compute_rsi</span>(
            self.returns[:self.idx]) / <span class="nu">100.0</span>
        pos = <span class="nb">float</span>(self.position)
        unrealized = self.position * (
            self.prices[self.idx] - self.entry_price
        ) / self.entry_price <span class="kw">if</span> self.position != <span class="nu">0</span> <span class="kw">else</span> <span class="nu">0.0</span>
        <span class="kw">return</span> np.<span class="fn">concatenate</span>([
            ret_window, [rsi, pos, unrealized]
        ]).<span class="fn">astype</span>(np.float32)

    <span class="kw">def</span> <span class="fn">reset</span>(self, seed=<span class="kw">None</span>, options=<span class="kw">None</span>):
        <span class="fn">super</span>().<span class="fn">reset</span>(seed=seed)
        self.idx = self.window
        self.position = <span class="nu">0</span>  <span class="cm"># -1, 0, 1</span>
        self.entry_price = <span class="nu">0.0</span>
        self.portfolio_value = <span class="nu">1.0</span>
        self.peak_value = <span class="nu">1.0</span>
        self.trades = <span class="nu">0</span>
        <span class="kw">return</span> self.<span class="fn">_get_obs</span>(), {}

    <span class="kw">def</span> <span class="fn">step</span>(self, action):
        <span class="cm"># ì´ì „ í¬ì§€ì…˜</span>
        prev_pos = self.position
        reward = <span class="nu">0.0</span>

        <span class="cm"># í–‰ë™ ì‹¤í–‰</span>
        <span class="kw">if</span> action == <span class="nu">1</span> <span class="kw">and</span> self.position <= <span class="nu">0</span>:  <span class="cm"># Buy</span>
            <span class="kw">if</span> self.position == -<span class="nu">1</span>:  <span class="cm"># ìˆ ì²­ì‚°</span>
                pnl = -(self.prices[self.idx] - self.entry_price) \
                      / self.entry_price
                reward += pnl - self.tc
            self.position = <span class="nu">1</span>
            self.entry_price = self.prices[self.idx]
            self.trades += <span class="nu">1</span>
            reward -= self.tc  <span class="cm"># ê±°ë˜ ë¹„ìš©</span>

        <span class="kw">elif</span> action == <span class="nu">2</span> <span class="kw">and</span> self.position >= <span class="nu">0</span>:  <span class="cm"># Sell</span>
            <span class="kw">if</span> self.position == <span class="nu">1</span>:  <span class="cm"># ë¡± ì²­ì‚°</span>
                pnl = (self.prices[self.idx] - self.entry_price) \
                      / self.entry_price
                reward += pnl - self.tc
            self.position = -<span class="nu">1</span>
            self.entry_price = self.prices[self.idx]
            self.trades += <span class="nu">1</span>
            reward -= self.tc

        <span class="cm"># í¬ì§€ì…˜ ìœ ì§€ ë³´ìƒ (unrealized P&L ë³€í™”)</span>
        <span class="kw">if</span> self.position != <span class="nu">0</span>:
            daily_ret = self.returns[self.idx]
            reward += self.position * daily_ret

        <span class="cm"># í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ì—…ë°ì´íŠ¸</span>
        self.portfolio_value *= (<span class="nu">1</span> + reward)
        self.peak_value = <span class="fn">max</span>(self.peak_value,
                               self.portfolio_value)
        drawdown = (self.peak_value - self.portfolio_value) \
                   / self.peak_value

        <span class="cm"># ë‹¤ìŒ ìŠ¤í…</span>
        self.idx += <span class="nu">1</span>
        terminated = self.idx >= <span class="fn">len</span>(self.returns) - <span class="nu">1</span>
        truncated = drawdown > self.max_dd

        obs = self.<span class="fn">_get_obs</span>() <span class="kw">if</span> <span class="kw">not</span> terminated <span class="kw">else</span> \
              np.<span class="fn">zeros</span>(self.window + <span class="nu">3</span>, dtype=np.float32)

        info = {<span class="st">'portfolio_value'</span>: self.portfolio_value,
                <span class="st">'drawdown'</span>: drawdown,
                <span class="st">'trades'</span>: self.trades,
                <span class="st">'position'</span>: self.position}

        <span class="kw">return</span> obs, reward, terminated, truncated, info
</pre>
<div class="code-output"><span class="out-label">í™˜ê²½ ì¸í„°í˜ì´ìŠ¤:</span>
obs = [ret_t-20, ..., ret_t-1, RSI/100, position, unrealized_pnl]
action âˆˆ {0: Hold, 1: Buy, 2: Sell}
reward = realized_pnl + unrealized_change - transaction_cost
done = ë°ì´í„° ë or MDD > 20%</div>

<h3>10.3 ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ì˜ ì¤‘ìš”ì„±</h3>

<p>
ë³´ìƒ í•¨ìˆ˜ëŠ” RL ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ê²°ì •í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œë‹¤. ë‹¨ìˆœíˆ ìˆ˜ìµë¥ ì„ ë³´ìƒìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ì—ì´ì „íŠ¸ê°€ ê³¼ë„í•œ ë¦¬ìŠ¤í¬ë¥¼ ì·¨í•  ìˆ˜ ìˆë‹¤. ë‹¤ì–‘í•œ ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„:
</p>

<table>
<tr><th>ë³´ìƒ í•¨ìˆ˜</th><th>ìˆ˜ì‹</th><th>íŠ¹ì„±</th></tr>
<tr><td>ë‹¨ìˆœ ìˆ˜ìµë¥ </td><td>\(r_t = \text{position} \times \text{return}_t\)</td><td>ë¦¬ìŠ¤í¬ ë¬´ì‹œ, ê³¼ë„í•œ ê±°ë˜</td></tr>
<tr><td>ë¡œê·¸ ìˆ˜ìµë¥ </td><td>\(r_t = \log(1 + \text{position} \times \text{return}_t)\)</td><td>í° ì†ì‹¤ì— ë” í° í˜ë„í‹°</td></tr>
<tr><td>ìƒ¤í”„ ë³´ìƒ</td><td>\(r_t = \frac{\mu_t}{\sigma_t + \epsilon}\)</td><td>ë¦¬ìŠ¤í¬ ì¡°ì •, ì•ˆì •ì </td></tr>
<tr><td>Sortino ë³´ìƒ</td><td>\(r_t = \frac{\mu_t}{\sigma_t^{down} + \epsilon}\)</td><td>í•˜ë°© ë¦¬ìŠ¤í¬ë§Œ í˜ë„í‹°</td></tr>
<tr><td>ê±°ë˜ ë¹„ìš© í¬í•¨</td><td>\(r_t = \text{pnl}_t - c \cdot |\Delta \text{pos}|\)</td><td>ê³¼ë„í•œ ê±°ë˜ ì–µì œ</td></tr>
</table>

<!-- â˜… Plotly: Trading Environment Visualization -->
<div id="plot-ch10-env" style="width:100%;height:500px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(2024);
  // Generate price series
  var N=250, prices=[100], positions=[], actions=[];
  for(var i=1;i<N;i++){
    var drift=0.0002, vol=0.015;
    var ret=drift+(rng()-0.5)*2*vol;
    prices.push(prices[i-1]*(1+ret));
  }
  // Simple momentum agent simulation
  var pos=0, pv=1.0, pvs=[1.0];
  var buyX=[],buyY=[],sellX=[],sellY=[];
  for(var i=20;i<N-1;i++){
    var ma5=0,ma20=0;
    for(var j=i-4;j<=i;j++) ma5+=prices[j]; ma5/=5;
    for(var j=i-19;j<=i;j++) ma20+=prices[j]; ma20/=20;
    var oldPos=pos;
    if(ma5>ma20&&pos<=0){pos=1;buyX.push(i);buyY.push(prices[i]);}
    else if(ma5<ma20&&pos>=0){pos=-1;sellX.push(i);sellY.push(prices[i]);}
    var ret=(prices[i+1]-prices[i])/prices[i];
    pv*=(1+pos*ret-Math.abs(pos-oldPos)*0.001);
    pvs.push(pv);
  }
  var days=Array.from({length:N},function(_,i){return i;});
  var pvDays=Array.from({length:pvs.length},function(_,i){return i+20;});
  Plotly.newPlot('plot-ch10-env',[
    {x:days,y:prices,type:'scatter',mode:'lines',name:'ê°€ê²©',line:{color:'#555',width:1.5}},
    {x:buyX,y:buyY,type:'scatter',mode:'markers',name:'Buy',marker:{symbol:'triangle-up',size:10,color:'#2ecc71'}},
    {x:sellX,y:sellY,type:'scatter',mode:'markers',name:'Sell',marker:{symbol:'triangle-down',size:10,color:'#e74c3c'}},
    {x:pvDays,y:pvs,type:'scatter',mode:'lines',name:'í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜',line:{color:'#3498db',width:2},yaxis:'y2'}
  ],{
    title:{text:'ğŸ“Š íŠ¸ë ˆì´ë”© í™˜ê²½: ê°€ê²© + ì—ì´ì „íŠ¸ í–‰ë™ + í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜',font:{size:13}},
    xaxis:{title:'ê±°ë˜ì¼'},
    yaxis:{title:'ê°€ê²© ($)',side:'left'},
    yaxis2:{title:'í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜',overlaying:'y',side:'right',titlefont:{color:'#3498db'}},
    legend:{x:0.01,y:0.99,bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:45,b:50,r:70},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ íšŒìƒ‰=ê°€ê²©, ì´ˆë¡â–²=ë§¤ìˆ˜, ë¹¨ê°•â–¼=ë§¤ë„, íŒŒë‘=í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜. ì—ì´ì „íŠ¸ê°€ MA í¬ë¡œìŠ¤ì˜¤ë²„ ì‹œê·¸ë„ì— ë”°ë¼ ë§¤ë§¤í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜.</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 11: DQN íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ êµ¬í˜„
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch11">Chapter 11. DQN íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ â€” ì „ì²´ íŒŒì´í”„ë¼ì¸</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.23 "RL Trading Agent" / MLDSF Ch.14 "End-to-End Trading System"</p>
</div>

<h3>11.1 ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸</h3>

<p>
Ch.7ì˜ DQN ì—ì´ì „íŠ¸ì™€ Ch.10ì˜ íŠ¸ë ˆì´ë”© í™˜ê²½ì„ ê²°í•©í•˜ì—¬ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤. ë°ì´í„° ì¤€ë¹„ â†’ í™˜ê²½ ìƒì„± â†’ ì—ì´ì „íŠ¸ í•™ìŠµ â†’ í‰ê°€ì˜ íë¦„ì´ë‹¤.
</p>

<p class="cc">â–¼ DQN íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ í•™ìŠµ ì „ì²´ ì½”ë“œ</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf

<span class="cm"># â”€â”€ 1. ë°ì´í„° ì¤€ë¹„ â”€â”€</span>
data = yf.<span class="fn">download</span>(<span class="st">'AAPL'</span>, start=<span class="st">'2018-01-01'</span>, end=<span class="st">'2024-01-01'</span>)
close = data[<span class="st">'Close'</span>]
<span class="kw">if</span> <span class="fn">hasattr</span>(close, <span class="st">'columns'</span>):
    close = close.<span class="fn">droplevel</span>(<span class="st">'Ticker'</span>, axis=<span class="nu">1</span>)
prices = close.<span class="fn">values</span>.<span class="fn">flatten</span>()

<span class="cm"># Train/Test ë¶„í•  (80/20)</span>
split = <span class="fn">int</span>(<span class="fn">len</span>(prices) * <span class="nu">0.8</span>)
train_prices = prices[:split]
test_prices = prices[split:]

<span class="cm"># â”€â”€ 2. í™˜ê²½ + ì—ì´ì „íŠ¸ ìƒì„± â”€â”€</span>
train_env = <span class="fn">TradingEnv</span>(train_prices, window=<span class="nu">20</span>)
test_env = <span class="fn">TradingEnv</span>(test_prices, window=<span class="nu">20</span>)

state_dim = train_env.observation_space.shape[<span class="nu">0</span>]  <span class="cm"># 23</span>
agent = <span class="fn">DQNAgent</span>(state_dim=state_dim, action_dim=<span class="nu">3</span>)

<span class="cm"># â”€â”€ 3. í•™ìŠµ ë£¨í”„ â”€â”€</span>
n_episodes = <span class="nu">200</span>
best_reward = -np.inf

<span class="kw">for</span> ep <span class="kw">in</span> <span class="fn">range</span>(n_episodes):
    obs, _ = train_env.<span class="fn">reset</span>()
    total_reward = <span class="nu">0</span>
    done = <span class="kw">False</span>

    <span class="kw">while</span> <span class="kw">not</span> done:
        action = agent.<span class="fn">choose_action</span>(obs)
        next_obs, reward, terminated, truncated, info = \
            train_env.<span class="fn">step</span>(action)
        done = terminated <span class="kw">or</span> truncated

        agent.buffer.<span class="fn">push</span>(obs, action, reward, next_obs, done)
        agent.<span class="fn">train_step</span>(batch_size=<span class="nu">64</span>)

        obs = next_obs
        total_reward += reward

    <span class="kw">if</span> total_reward > best_reward:
        best_reward = total_reward
        <span class="cm"># ìµœê³  ëª¨ë¸ ì €ì¥</span>
        best_weights = agent.q_net.<span class="fn">state_dict</span>().<span class="fn">copy</span>()

    <span class="kw">if</span> (ep + <span class="nu">1</span>) % <span class="nu">20</span> == <span class="nu">0</span>:
        <span class="fn">print</span>(<span class="st">f"Episode {ep+1:3d} | Reward: {total_reward:.4f} | "</span>
              <span class="st">f"PV: {info['portfolio_value']:.4f} | "</span>
              <span class="st">f"Trades: {info['trades']} | Îµ: {agent.epsilon:.3f}"</span>)

<span class="cm"># â”€â”€ 4. í…ŒìŠ¤íŠ¸ â”€â”€</span>
agent.q_net.<span class="fn">load_state_dict</span>(best_weights)
agent.epsilon = <span class="nu">0.0</span>  <span class="cm"># íƒìƒ‰ ì—†ì´ ìˆœìˆ˜ í™œìš©</span>

obs, _ = test_env.<span class="fn">reset</span>()
done = <span class="kw">False</span>
portfolio_values = [<span class="nu">1.0</span>]

<span class="kw">while</span> <span class="kw">not</span> done:
    action = agent.<span class="fn">choose_action</span>(obs)
    obs, reward, terminated, truncated, info = test_env.<span class="fn">step</span>(action)
    done = terminated <span class="kw">or</span> truncated
    portfolio_values.<span class="fn">append</span>(info[<span class="st">'portfolio_value'</span>])

<span class="cm"># ì„±ê³¼ ì§€í‘œ</span>
pv = np.<span class="fn">array</span>(portfolio_values)
total_return = (pv[-<span class="nu">1</span>] - <span class="nu">1</span>) * <span class="nu">100</span>
daily_returns = np.<span class="fn">diff</span>(pv) / pv[:-<span class="nu">1</span>]
sharpe = np.<span class="fn">mean</span>(daily_returns) / (np.<span class="fn">std</span>(daily_returns) + <span class="nu">1e-8</span>) * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)
mdd = np.<span class="fn">max</span>(np.<span class="fn">maximum</span>.<span class="fn">accumulate</span>(pv) - pv) / np.<span class="fn">maximum</span>.<span class="fn">accumulate</span>(pv).<span class="fn">max</span>()

<span class="fn">print</span>(<span class="st">f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ==="</span>)
<span class="fn">print</span>(<span class="st">f"ì´ ìˆ˜ìµë¥ : {total_return:.2f}%"</span>)
<span class="fn">print</span>(<span class="st">f"ìƒ¤í”„ë¹„ìœ¨: {sharpe:.2f}"</span>)
<span class="fn">print</span>(<span class="st">f"ìµœëŒ€ ë‚™í­: {mdd*100:.2f}%"</span>)
<span class="fn">print</span>(<span class="st">f"ì´ ê±°ë˜ íšŸìˆ˜: {info['trades']}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output (ì˜ˆì‹œ):</span>
Episode  20 | Reward: 0.0234 | PV: 1.0234 | Trades: 45 | Îµ: 0.818
Episode  40 | Reward: 0.0512 | PV: 1.0512 | Trades: 38 | Îµ: 0.669
Episode  60 | Reward: 0.0891 | PV: 1.0891 | Trades: 32 | Îµ: 0.547
Episode  80 | Reward: 0.1203 | PV: 1.1203 | Trades: 28 | Îµ: 0.447
Episode 100 | Reward: 0.1456 | PV: 1.1456 | Trades: 25 | Îµ: 0.366
...
Episode 200 | Reward: 0.2134 | PV: 1.2134 | Trades: 18 | Îµ: 0.010

=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===
ì´ ìˆ˜ìµë¥ : 15.23%
ìƒ¤í”„ë¹„ìœ¨: 1.12
ìµœëŒ€ ë‚™í­: 8.45%
ì´ ê±°ë˜ íšŸìˆ˜: 22</div>

<!-- â˜… Plotly: DQN Agent Cumulative Returns -->
<div id="plot-ch11-returns" style="width:100%;height:450px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(314);
  var N=250, days=[];
  var buyHold=[1.0], dqn=[1.0], ppo=[1.0];
  for(var i=0;i<N;i++){
    days.push(i+1);
    var mktRet=(rng()-0.48)*0.03; // slight positive drift
    buyHold.push(buyHold[i]*(1+mktRet));
    // DQN: slightly better than market
    var dqnRet=mktRet*0.6+(rng()-0.45)*0.008;
    dqn.push(dqn[i]*(1+dqnRet));
    // PPO: best
    var ppoRet=mktRet*0.5+(rng()-0.42)*0.007;
    ppo.push(ppo[i]*(1+ppoRet));
  }
  days.unshift(0);
  Plotly.newPlot('plot-ch11-returns',[
    {x:days,y:buyHold,type:'scatter',mode:'lines',name:'Buy & Hold',line:{color:'#95a5a6',width:1.5}},
    {x:days,y:dqn,type:'scatter',mode:'lines',name:'DQN Agent',line:{color:'#e74c3c',width:2}},
    {x:days,y:ppo,type:'scatter',mode:'lines',name:'PPO Agent',line:{color:'#2ecc71',width:2}}
  ],{
    title:{text:'ğŸ“Š RL ì—ì´ì „íŠ¸ vs Buy & Hold: ëˆ„ì  ìˆ˜ìµë¥  ë¹„êµ (í…ŒìŠ¤íŠ¸ ê¸°ê°„)',font:{size:13}},
    xaxis:{title:'ê±°ë˜ì¼'},
    yaxis:{title:'í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ (ì‹œì‘=1.0)',zeroline:false},
    legend:{x:0.01,y:0.99,bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:45,b:50},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff',
    shapes:[{type:'line',x0:0,x1:N,y0:1,y1:1,line:{color:'#ccc',width:1,dash:'dot'}}]
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ íšŒìƒ‰=Buy&Hold, ë¹¨ê°•=DQN ì—ì´ì „íŠ¸, ì´ˆë¡=PPO ì—ì´ì „íŠ¸. RL ì—ì´ì „íŠ¸ëŠ” í•˜ë½ì¥ì—ì„œ í¬ì§€ì…˜ì„ ì¤„ì—¬ ë‚™í­ì„ ì œí•œí•œë‹¤.</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 12: ì‹¤ì „ íŒŒì´í”„ë¼ì¸
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch12">Chapter 12. ì‹¤ì „ íŒŒì´í”„ë¼ì¸ â€” ë°±í…ŒìŠ¤íŠ¸ + ë¦¬ìŠ¤í¬ ê´€ë¦¬</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLDSF Ch.14 "Backtesting and Risk Management" / MLAT Ch.23 "Practical Considerations"</p>
</div>

<h3>12.1 ë°±í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬</h3>

<p>
RL ì—ì´ì „íŠ¸ë¥¼ ì‹¤ì „ì— ë°°í¬í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì—„ê²©í•œ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ë°±í…ŒìŠ¤íŠ¸ì˜ í•µì‹¬ ì›ì¹™: (1) ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ(look-ahead bias) ë°©ì§€, (2) ê±°ë˜ ë¹„ìš© ë°˜ì˜, (3) ìŠ¬ë¦¬í”¼ì§€ ëª¨ë¸ë§, (4) ì¶©ë¶„í•œ ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ê¸°ê°„.
</p>

<p class="cc">â–¼ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd

<span class="kw">class</span> <span class="nb">Backtester</span>:
    <span class="st">"""RL ì—ì´ì „íŠ¸ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, agent, env, initial_capital=<span class="nu">100000</span>):
        self.agent = agent
        self.env = env
        self.capital = initial_capital

    <span class="kw">def</span> <span class="fn">run</span>(self):
        obs, _ = self.env.<span class="fn">reset</span>()
        done = <span class="kw">False</span>
        history = {<span class="st">'pv'</span>: [<span class="nu">1.0</span>], <span class="st">'actions'</span>: [],
                   <span class="st">'positions'</span>: [<span class="nu">0</span>], <span class="st">'rewards'</span>: []}

        <span class="kw">while</span> <span class="kw">not</span> done:
            action = self.agent.<span class="fn">choose_action</span>(obs)
            obs, reward, terminated, truncated, info = \
                self.env.<span class="fn">step</span>(action)
            done = terminated <span class="kw">or</span> truncated
            history[<span class="st">'pv'</span>].<span class="fn">append</span>(info[<span class="st">'portfolio_value'</span>])
            history[<span class="st">'actions'</span>].<span class="fn">append</span>(action)
            history[<span class="st">'positions'</span>].<span class="fn">append</span>(info[<span class="st">'position'</span>])
            history[<span class="st">'rewards'</span>].<span class="fn">append</span>(reward)

        <span class="kw">return</span> self.<span class="fn">compute_metrics</span>(history)

    <span class="kw">def</span> <span class="fn">compute_metrics</span>(self, history):
        pv = np.<span class="fn">array</span>(history[<span class="st">'pv'</span>])
        daily_ret = np.<span class="fn">diff</span>(pv) / pv[:-<span class="nu">1</span>]

        <span class="cm"># ì—°ê°„í™” ì§€í‘œ</span>
        ann_return = (pv[-<span class="nu">1</span>] ** (<span class="nu">252</span> / <span class="fn">len</span>(pv)) - <span class="nu">1</span>) * <span class="nu">100</span>
        ann_vol = np.<span class="fn">std</span>(daily_ret) * np.<span class="fn">sqrt</span>(<span class="nu">252</span>) * <span class="nu">100</span>
        sharpe = (np.<span class="fn">mean</span>(daily_ret) / (np.<span class="fn">std</span>(daily_ret) + <span class="nu">1e-8</span>)
                  * np.<span class="fn">sqrt</span>(<span class="nu">252</span>))

        <span class="cm"># ìµœëŒ€ ë‚™í­</span>
        peak = np.<span class="fn">maximum</span>.<span class="fn">accumulate</span>(pv)
        dd = (peak - pv) / peak
        mdd = np.<span class="fn">max</span>(dd) * <span class="nu">100</span>

        <span class="cm"># Sortino Ratio</span>
        downside = daily_ret[daily_ret < <span class="nu">0</span>]
        sortino = (np.<span class="fn">mean</span>(daily_ret) /
                   (np.<span class="fn">std</span>(downside) + <span class="nu">1e-8</span>) * np.<span class="fn">sqrt</span>(<span class="nu">252</span>))

        <span class="cm"># Calmar Ratio</span>
        calmar = ann_return / (mdd + <span class="nu">1e-8</span>)

        metrics = {
            <span class="st">'ì´ ìˆ˜ìµë¥  (%)'</span>: (pv[-<span class="nu">1</span>] - <span class="nu">1</span>) * <span class="nu">100</span>,
            <span class="st">'ì—°ê°„ ìˆ˜ìµë¥  (%)'</span>: ann_return,
            <span class="st">'ì—°ê°„ ë³€ë™ì„± (%)'</span>: ann_vol,
            <span class="st">'ìƒ¤í”„ë¹„ìœ¨'</span>: sharpe,
            <span class="st">'ì†Œë¥´í‹°ë…¸ë¹„ìœ¨'</span>: sortino,
            <span class="st">'ìµœëŒ€ ë‚™í­ (%)'</span>: mdd,
            <span class="st">'ì¹¼ë§ˆë¹„ìœ¨'</span>: calmar,
            <span class="st">'ì´ ê±°ë˜ íšŸìˆ˜'</span>: <span class="fn">sum</span>(<span class="nu">1</span> <span class="kw">for</span> a <span class="kw">in</span> history[<span class="st">'actions'</span>]
                              <span class="kw">if</span> a != <span class="nu">0</span>)
        }
        <span class="kw">return</span> metrics, history
</pre>
<div class="code-output"><span class="out-label">Output (ì˜ˆì‹œ):</span>
=== ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===
ì´ ìˆ˜ìµë¥  (%):     15.23
ì—°ê°„ ìˆ˜ìµë¥  (%):   12.45
ì—°ê°„ ë³€ë™ì„± (%):   18.32
ìƒ¤í”„ë¹„ìœ¨:          0.68
ì†Œë¥´í‹°ë…¸ë¹„ìœ¨:      1.02
ìµœëŒ€ ë‚™í­ (%):     8.45
ì¹¼ë§ˆë¹„ìœ¨:          1.47
ì´ ê±°ë˜ íšŸìˆ˜:      22</div>

<h3>12.2 ë¦¬ìŠ¤í¬ ê´€ë¦¬ ëª¨ë“ˆ</h3>

<p>
ì•„ë¬´ë¦¬ ì¢‹ì€ RL ì—ì´ì „íŠ¸ë¼ë„ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì—†ì´ëŠ” ì‹¤ì „ì—ì„œ ì‚´ì•„ë‚¨ì„ ìˆ˜ ì—†ë‹¤. ë¦¬ìŠ¤í¬ ê´€ë¦¬ëŠ” ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ "í•„í„°ë§"í•˜ëŠ” ì•ˆì „ì¥ì¹˜ë‹¤.
</p>

<p class="cc">â–¼ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë˜í¼</p>
<pre>
<span class="kw">class</span> <span class="nb">RiskManager</span>:
    <span class="st">"""ì—ì´ì „íŠ¸ í–‰ë™ì„ í•„í„°ë§í•˜ëŠ” ë¦¬ìŠ¤í¬ ê´€ë¦¬"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(self, max_position=<span class="nu">1</span>, max_daily_loss=<span class="nu">0.02</span>,
                 max_drawdown=<span class="nu">0.1</span>, max_trades_per_day=<span class="nu">10</span>):
        self.max_pos = max_position
        self.max_daily_loss = max_daily_loss
        self.max_dd = max_drawdown
        self.max_trades = max_trades_per_day
        self.daily_pnl = <span class="nu">0.0</span>
        self.daily_trades = <span class="nu">0</span>
        self.peak_value = <span class="nu">1.0</span>

    <span class="kw">def</span> <span class="fn">check_action</span>(self, action, position, portfolio_value):
        <span class="st">"""í–‰ë™ í—ˆìš© ì—¬ë¶€ íŒë‹¨"""</span>
        <span class="cm"># ì¼ì¼ ì†ì‹¤ í•œë„ ì´ˆê³¼</span>
        <span class="kw">if</span> self.daily_pnl < -self.max_daily_loss:
            <span class="kw">return</span> <span class="nu">0</span>  <span class="cm"># ê°•ì œ Hold</span>

        <span class="cm"># ìµœëŒ€ ë‚™í­ ì´ˆê³¼</span>
        self.peak_value = <span class="fn">max</span>(self.peak_value, portfolio_value)
        dd = (self.peak_value - portfolio_value) / self.peak_value
        <span class="kw">if</span> dd > self.max_dd:
            <span class="cm"># í¬ì§€ì…˜ ì²­ì‚°</span>
            <span class="kw">if</span> position > <span class="nu">0</span>: <span class="kw">return</span> <span class="nu">2</span>  <span class="cm"># Sell</span>
            <span class="kw">if</span> position < <span class="nu">0</span>: <span class="kw">return</span> <span class="nu">1</span>  <span class="cm"># Buy (ìˆ ì²­ì‚°)</span>
            <span class="kw">return</span> <span class="nu">0</span>

        <span class="cm"># ì¼ì¼ ê±°ë˜ íšŸìˆ˜ ì´ˆê³¼</span>
        <span class="kw">if</span> action != <span class="nu">0</span> <span class="kw">and</span> self.daily_trades >= self.max_trades:
            <span class="kw">return</span> <span class="nu">0</span>

        <span class="kw">if</span> action != <span class="nu">0</span>:
            self.daily_trades += <span class="nu">1</span>
        <span class="kw">return</span> action

    <span class="kw">def</span> <span class="fn">update_daily_pnl</span>(self, pnl):
        self.daily_pnl += pnl

    <span class="kw">def</span> <span class="fn">reset_daily</span>(self):
        self.daily_pnl = <span class="nu">0.0</span>
        self.daily_trades = <span class="nu">0</span>
</pre>

<h3>12.3 Walk-Forward ê²€ì¦</h3>

<p>
ê¸ˆìœµ ë°ì´í„°ì—ì„œ ë‹¨ìˆœ Train/Test ë¶„í• ì€ ë¶€ì¡±í•˜ë‹¤. <strong>Walk-Forward ê²€ì¦</strong>ì€ ì‹œê°„ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ í•™ìŠµ ê¸°ê°„ì„ ì ì§„ì ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë°©ë²•ì´ë‹¤:
</p>

<div style="margin:20px 0;font-family:'Space Mono',monospace;font-size:11px;background:#1e1e1e;color:#d4d4d4;padding:15px;border-radius:6px;line-height:1.8">
<span style="color:#6a9955">Period 1:</span> [====Train====][Test].....................<br>
<span style="color:#6a9955">Period 2:</span> .[=====Train=====][Test].................<br>
<span style="color:#6a9955">Period 3:</span> ..[======Train======][Test]..............<br>
<span style="color:#6a9955">Period 4:</span> ...[=======Train=======][Test]...........<br>
<span style="color:#569cd6">â†’ ê° Periodì˜ Test ê²°ê³¼ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ì„±ê³¼ í‰ê°€</span>
</div>

<div class="warn">
<p class="ni"><strong>âš ï¸ RL ë°±í…ŒìŠ¤íŠ¸ì˜ í•¨ì •ë“¤:</strong></p>
<ul style="font-size:13px;margin-top:8px">
<li><strong>ê³¼ì í•©:</strong> ì—í”¼ì†Œë“œë¥¼ ë„ˆë¬´ ë§ì´ í•™ìŠµí•˜ë©´ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©. Early stopping í•„ìˆ˜.</li>
<li><strong>ìƒì¡´ í¸í–¥:</strong> ìƒì¥íì§€ëœ ì¢…ëª©ì„ ì œì™¸í•˜ë©´ ì„±ê³¼ê°€ ê³¼ëŒ€í‰ê°€ë¨.</li>
<li><strong>ê±°ë˜ ë¹„ìš©:</strong> ì‹¤ì œ ê±°ë˜ ë¹„ìš©(ìŠ¤í”„ë ˆë“œ + ìˆ˜ìˆ˜ë£Œ + ìŠ¬ë¦¬í”¼ì§€)ì„ ê³¼ì†Œí‰ê°€í•˜ë©´ ì•ˆ ë¨.</li>
<li><strong>ì‹œì¥ ì¶©ê²©:</strong> ëŒ€ëŸ‰ ì£¼ë¬¸ì˜ ê°€ê²© ì¶©ê²©ì„ ë¬´ì‹œí•˜ë©´ ì•ˆ ë¨ (Ch.2ì˜ Kyle's Lambda).</li>
</ul>
</div>

<!-- â˜… Plotly: Walk-Forward Performance -->
<div id="plot-ch12-wf" style="width:100%;height:420px;margin:20px 0"></div>
<script>
(function(){
  function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;};}
  var rng=mulberry32(628);
  // Walk-forward: 4 periods, each with train + test
  var periods=['2019','2020','2021','2022','2023'];
  var trainSharpe=[],testSharpe=[],trainRet=[],testRet=[];
  for(var i=0;i<5;i++){
    var ts=0.8+rng()*1.2; trainSharpe.push(parseFloat(ts.toFixed(2)));
    var te=ts*0.5+rng()*0.6-0.1; testSharpe.push(parseFloat(te.toFixed(2)));
    var tr=10+rng()*20; trainRet.push(parseFloat(tr.toFixed(1)));
    var ttr=tr*0.4+rng()*10-3; testRet.push(parseFloat(ttr.toFixed(1)));
  }
  Plotly.newPlot('plot-ch12-wf',[
    {x:periods,y:trainSharpe,type:'bar',name:'Train Sharpe',marker:{color:'rgba(52,152,219,0.7)'},offsetgroup:1},
    {x:periods,y:testSharpe,type:'bar',name:'Test Sharpe',marker:{color:'rgba(231,76,60,0.7)'},offsetgroup:2},
    {x:periods,y:trainRet,type:'scatter',mode:'lines+markers',name:'Train Return (%)',line:{color:'#3498db',width:2,dash:'dot'},marker:{size:6},yaxis:'y2'},
    {x:periods,y:testRet,type:'scatter',mode:'lines+markers',name:'Test Return (%)',line:{color:'#e74c3c',width:2,dash:'dot'},marker:{size:6},yaxis:'y2'}
  ],{
    title:{text:'ğŸ“Š Walk-Forward ê²€ì¦: Train vs Test ì„±ê³¼ (ê¸°ê°„ë³„)',font:{size:13}},
    xaxis:{title:'í…ŒìŠ¤íŠ¸ ê¸°ê°„'},
    yaxis:{title:'ìƒ¤í”„ë¹„ìœ¨',side:'left'},
    yaxis2:{title:'ìˆ˜ìµë¥  (%)',overlaying:'y',side:'right'},
    barmode:'group',
    legend:{x:0.01,y:1.15,orientation:'h',bgcolor:'rgba(255,255,255,0.85)'},
    margin:{t:60,b:50,r:70},paper_bgcolor:'#fafaf8',plot_bgcolor:'#fff'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ íŒŒë‘=í•™ìŠµ ê¸°ê°„, ë¹¨ê°•=í…ŒìŠ¤íŠ¸ ê¸°ê°„. í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ì˜ ì„±ê³¼ ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ê³¼ì í•©ì´ ì ë‹¤.</p>


<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 13: í”¼ë“œë°± + Quiz + R10 Preview
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch13">Chapter 13. í”¼ë“œë°± + Quiz â€” R7~R9 ì¢…í•© ì ê²€</h2>

<div class="info">
<p class="ni"><strong>ğŸ“‹ í”¼ë“œë°± ë¼ìš´ë“œ:</strong> R3, R6ì— ì´ì–´ ì„¸ ë²ˆì§¸ í”¼ë“œë°± ì„¸ì…˜ì´ë‹¤. R7(ë”¥ëŸ¬ë‹), R8(ë³¼ë¡ ìµœì í™” + Transformer), R9(HFT + ê°•í™”í•™ìŠµ)ì˜ í•µì‹¬ ê°œë…ì„ ì¢…í•© ì ê²€í•œë‹¤.</p>
</div>

<h3>13.1 R7~R9 í•™ìŠµ ë¡œë“œë§µ íšŒê³ </h3>

<table>
<tr><th>ë¼ìš´ë“œ</th><th>í•µì‹¬ ì£¼ì œ</th><th>í•µì‹¬ ë„êµ¬</th><th>íŠ¸ë ˆì´ë”© ì ìš©</th></tr>
<tr><td>R7</td><td>ë”¥ëŸ¬ë‹ ê¸°ì´ˆ</td><td>PyTorch, CNN, RNN, LSTM</td><td>ì‹œê³„ì—´ ì˜ˆì¸¡, íŒ¨í„´ ì¸ì‹</td></tr>
<tr><td>R8</td><td>ë³¼ë¡ ìµœì í™” + Transformer</td><td>CVXPY, Self-Attention</td><td>í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”, ì¥ê¸° ì˜ì¡´ì„±</td></tr>
<tr><td>R9</td><td>HFT + ê°•í™”í•™ìŠµ</td><td>Gymnasium, DQN, PPO</td><td>ìë™ ë§¤ë§¤ ì—ì´ì „íŠ¸, ë§ˆì¼“ë©”ì´í‚¹</td></tr>
</table>

<h3>13.2 í•µì‹¬ ê°œë… í€´ì¦ˆ</h3>

<div class="def">
<p class="ni"><strong>Q1. [R7]</strong> LSTMì˜ Forget Gateê°€ í•˜ëŠ” ì—­í• ì€? ì™œ ì¼ë°˜ RNNë³´ë‹¤ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì— ìœ ë¦¬í•œê°€?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: Cell Stateì™€ Gradient Flowë¥¼ ìƒê°í•´ë³´ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q2. [R7]</strong> Dropoutì´ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ëª…í•˜ë¼. í•™ìŠµ ì‹œì™€ ì¶”ë¡  ì‹œì˜ ì°¨ì´ëŠ”?</p>
</div>

<div class="def">
<p class="ni"><strong>Q3. [R8]</strong> ë³¼ë¡ ìµœì í™”ì—ì„œ KKT ì¡°ê±´ì˜ "ìƒë³´ ì´ì™„" \(\lambda_i g_i(x^*) = 0\)ì˜ ì˜ë¯¸ëŠ”?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: ì œì•½ì´ í™œì„±(active)ì¸ ê²½ìš°ì™€ ë¹„í™œì„±ì¸ ê²½ìš°ë¥¼ ë‚˜ëˆ„ì–´ ìƒê°í•˜ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q4. [R8]</strong> Transformerì˜ Self-Attentionì—ì„œ \(\sqrt{d_k}\)ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ”?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: Softmaxì˜ ì…ë ¥ í¬ê¸°ì™€ gradientë¥¼ ìƒê°í•´ë³´ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q5. [R9]</strong> ë§ˆì¼“ë©”ì´ì»¤ê°€ ìŠ¤í”„ë ˆë“œë¥¼ ìš”êµ¬í•˜ëŠ” ê·¼ë³¸ì  ì´ìœ ëŠ”? (Ch.1 ì—­ì„ íƒ)</p>
</div>

<div class="def">
<p class="ni"><strong>Q6. [R9]</strong> Q-Learningì˜ ì—…ë°ì´íŠ¸ ê·œì¹™ì—ì„œ TD ì—ëŸ¬ê°€ 0ì´ ë˜ë©´ ì–´ë–¤ ë°©ì •ì‹ì´ ë§Œì¡±ë˜ëŠ”ê°€?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: Ch.5ì˜ ë²¨ë§Œ ìµœì  ë°©ì •ì‹.</p>
</div>

<div class="def">
<p class="ni"><strong>Q7. [R9]</strong> DQNì—ì„œ Experience Replayì™€ Target Networkê°€ ê°ê° í•´ê²°í•˜ëŠ” ë¬¸ì œëŠ”?</p>
</div>

<div class="def">
<p class="ni"><strong>Q8. [R9]</strong> PPOì˜ í´ë¦¬í•‘ ë©”ì»¤ë‹ˆì¦˜ì´ í•™ìŠµ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ëŠ” ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: í™•ë¥  ë¹„ìœ¨ \(r_t(\theta)\)ê°€ \([1-\epsilon, 1+\epsilon]\) ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´?</p>
</div>

<div class="def">
<p class="ni"><strong>Q9. [R9]</strong> íŠ¸ë ˆì´ë”© RLì—ì„œ ë³´ìƒ í•¨ìˆ˜ë¥¼ ë‹¨ìˆœ ìˆ˜ìµë¥  ëŒ€ì‹  ìƒ¤í”„ë¹„ìœ¨ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„í•˜ëŠ” ì´ìœ ëŠ”?</p>
</div>

<div class="def">
<p class="ni"><strong>Q10. [ì¢…í•©]</strong> R8ì˜ Transformer ìˆ˜ìµë¥  ì˜ˆì¸¡ â†’ R9ì˜ RL ì—ì´ì „íŠ¸ë¥¼ ê²°í•©í•˜ë©´ ì–´ë–¤ ì‹œìŠ¤í…œì„ ë§Œë“¤ ìˆ˜ ìˆëŠ”ê°€? êµ¬ì²´ì ì¸ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: Transformerì˜ ì¶œë ¥ì„ RL ì—ì´ì „íŠ¸ì˜ ìƒíƒœ(state)ì— í¬í•¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ìƒê°í•´ë³´ì.</p>
</div>

<h3>13.3 Mini Project: RL íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ ëŒ€ê²°</h3>

<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#e8eaf6,#ede7f6);border-radius:12px;border:2px solid #5c6bc0;box-shadow:0 4px 15px rgba(0,0,0,.1)">
<p class="ni" style="font-weight:bold;font-size:16px;color:#283593;margin-bottom:15px">ğŸ¯ Mini Project: DQN vs PPO íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ ëŒ€ê²°</p>

<p class="ni" style="font-size:13px;margin-bottom:12px">
<strong>ëª©í‘œ:</strong> ë™ì¼í•œ íŠ¸ë ˆì´ë”© í™˜ê²½ì—ì„œ DQNê³¼ PPO ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚¤ê³ , Buy&Hold ë²¤ì¹˜ë§ˆí¬ì™€ ì„±ê³¼ë¥¼ ë¹„êµí•œë‹¤.
</p>

<p class="ni" style="font-size:13px;font-weight:bold;color:#3949ab;margin-bottom:8px">ğŸ“‹ ê³¼ì œ ë‹¨ê³„:</p>

<ol style="font-size:13px">
<li style="margin-bottom:8px"><strong>ë°ì´í„°:</strong> AAPL 2018~2024 ì¼ê°„ ë°ì´í„°, Train(2018~2022) / Test(2023~2024)</li>
<li style="margin-bottom:8px"><strong>í™˜ê²½:</strong> Ch.10ì˜ TradingEnv ì‚¬ìš© (window=20, ê±°ë˜ë¹„ìš©=0.1%)</li>
<li style="margin-bottom:8px"><strong>ì—ì´ì „íŠ¸ í•™ìŠµ:</strong> DQN(Ch.7) 200 ì—í”¼ì†Œë“œ, PPO(Ch.9) 200 ì—í”¼ì†Œë“œ</li>
<li style="margin-bottom:8px"><strong>ë°±í…ŒìŠ¤íŠ¸:</strong> Ch.12ì˜ Backtesterë¡œ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„±ê³¼ ì¸¡ì •</li>
<li style="margin-bottom:8px"><strong>ë¹„êµ ì§€í‘œ:</strong> ì´ ìˆ˜ìµë¥ , ìƒ¤í”„ë¹„ìœ¨, ìµœëŒ€ ë‚™í­, ì†Œë¥´í‹°ë…¸ë¹„ìœ¨, ê±°ë˜ íšŸìˆ˜</li>
<li style="margin-bottom:8px"><strong>ì‹œê°í™”:</strong> ëˆ„ì  ìˆ˜ìµë¥  ê³¡ì„  (3ê°œ ì „ëµ ë¹„êµ), ë‚™í­ ê³¡ì„ , í¬ì§€ì…˜ ë³€í™”</li>
</ol>

<p class="ni" style="font-size:12px;color:#5c6bc0;margin-top:12px">
<strong>ğŸ’¡ ë³´ë„ˆìŠ¤:</strong> ë³´ìƒ í•¨ìˆ˜ë¥¼ ë³€ê²½(ë‹¨ìˆœ ìˆ˜ìµë¥  â†’ ìƒ¤í”„ ë³´ìƒ)í•˜ì—¬ ì—ì´ì „íŠ¸ í–‰ë™ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ë¹„êµí•´ë³´ì.
</p>
</div>

<h3>13.4 R10 ì˜ˆê³ : Final Project â€” í†µí•© HFT ì‹œìŠ¤í…œ</h3>

<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fce4ec,#f3e5f5);border-radius:12px;border:2px solid #ab47bc">
<p class="ni" style="font-weight:bold;font-size:14px;color:#6a1b9a;margin-bottom:10px">ğŸš€ R10 Preview: Final Project â€” í†µí•© HFT ì‹œìŠ¤í…œ</p>
<p class="ni" style="font-size:13px">
R1~R9ì˜ ëª¨ë“  ê²ƒì„ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œìœ¼ë¡œ í†µí•©í•œë‹¤. ë°ì´í„° ìˆ˜ì§‘(R1,R3) â†’ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§(R3,R5) â†’ ì˜ˆì¸¡ ëª¨ë¸(R4,R7,R8) â†’ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”(R8) â†’ RL ì—ì´ì „íŠ¸(R9) â†’ ë°±í…ŒìŠ¤íŠ¸ + ë¦¬ìŠ¤í¬ ê´€ë¦¬(R9,R12)ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤.
</p>
<ul style="font-size:13px;margin-top:8px">
<li><strong>ë°ì´í„° íŒŒì´í”„ë¼ì¸:</strong> yfinance + ëŒ€ì•ˆ ë°ì´í„° (ë‰´ìŠ¤ ê°ì„±, R6)</li>
<li><strong>ì˜ˆì¸¡ ì—”ì§„:</strong> Transformer + XGBoost ì•™ìƒë¸”</li>
<li><strong>ìµœì í™” ì—”ì§„:</strong> CVXPY (Black-Litterman + Risk Parity)</li>
<li><strong>ì‹¤í–‰ ì—”ì§„:</strong> PPO ì—ì´ì „íŠ¸ + ë¦¬ìŠ¤í¬ ê´€ë¦¬</li>
<li><strong>í‰ê°€:</strong> Walk-Forward ë°±í…ŒìŠ¤íŠ¸ + ì¢…í•© ì„±ê³¼ ë¦¬í¬íŠ¸</li>
</ul>
<p class="ni" style="font-size:12px;color:#888;margin-top:10px">êµì¬: ì „ êµì¬ ì¢…í•© / ìµœì¢… í”„ë¡œì íŠ¸</p>
</div>

<!-- â˜… Plotly: R1~R10 Curriculum Progress 3D -->
<div id="plot-ch13-progress" style="width:100%;height:500px;margin:20px 0"></div>
<script>
(function(){
  var rounds=['R1','R2','R3','R4','R5','R6','R7','R8','R9','R10'];
  var theory=[60,80,70,85,80,75,90,95,95,100];
  var coding=[50,60,80,85,75,80,85,90,90,100];
  var trading=[30,40,70,75,70,65,80,90,95,100];
  // 3D scatter
  Plotly.newPlot('plot-ch13-progress',[{
    type:'scatter3d',mode:'lines+markers+text',
    x:theory,y:coding,z:trading,
    text:rounds,textposition:'top center',textfont:{size:10,color:'#333'},
    marker:{size:6,color:rounds.map(function(_,i){return i;}),colorscale:'Viridis',
      colorbar:{title:'Round',tickvals:[0,4,9],ticktext:['R1','R5','R10']},
      line:{width:1,color:'#333'}},
    line:{color:'#3498db',width:3},
    hovertemplate:'%{text}<br>ì´ë¡ : %{x}%<br>ì½”ë”©: %{y}%<br>íŠ¸ë ˆì´ë”©: %{z}%'
  }],{
    title:{text:'ğŸ“Š R1â†’R10 í•™ìŠµ ê¶¤ì : ì´ë¡  Ã— ì½”ë”© Ã— íŠ¸ë ˆì´ë”© ì—­ëŸ‰',font:{size:13}},
    scene:{
      xaxis:{title:'ì´ë¡  ì—­ëŸ‰ (%)',range:[20,105]},
      yaxis:{title:'ì½”ë”© ì—­ëŸ‰ (%)',range:[20,105]},
      zaxis:{title:'íŠ¸ë ˆì´ë”© ì—­ëŸ‰ (%)',range:[20,105]},
      camera:{eye:{x:1.5,y:1.5,z:1.2}}
    },
    margin:{t:45,b:10,l:10,r:10},paper_bgcolor:'#fafaf8'
  },{responsive:true});
})();
</script>
<p class="ni" style="font-size:11px;color:#888;text-align:center">ğŸ–±ï¸ 3D ê¶¤ì : R1ì—ì„œ ì‹œì‘í•˜ì—¬ R10ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì´ë¡ Â·ì½”ë”©Â·íŠ¸ë ˆì´ë”© ì—­ëŸ‰ì´ ëª¨ë‘ ì„±ì¥. ë§ˆìš°ìŠ¤ë¡œ íšŒì „í•˜ì—¬ ë‹¤ì–‘í•œ ê°ë„ì—ì„œ í™•ì¸í•˜ì„¸ìš”.</p>

<div class="ok">
<p class="ni"><strong>ğŸ‰ R9 ì™„ë£Œ!</strong> ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ì˜ ì„¸ê³„ë¥¼ ì´í•´í•˜ê³ , ê°•í™”í•™ìŠµì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜(Q-Learning â†’ DQN â†’ Policy Gradient â†’ PPO)ì„ ëª¨ë‘ ë°°ì› ë‹¤. ì´ì œ ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œ ë§¤ë§¤ ì „ëµì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. R10ì—ì„œ ëª¨ë“  ê²ƒì„ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ìµœì¢… í”„ë¡œì íŠ¸ê°€ ê¸°ë‹¤ë¦¬ê³  ìˆë‹¤.</p>
</div>

</div><!-- paper-content -->
</div><!-- container -->
</div><!-- main-wrapper -->

</body>
</html>
