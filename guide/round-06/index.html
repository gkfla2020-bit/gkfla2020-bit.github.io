<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Round 6 - NLP + Sentiment Analysis for Algorithmic Trading</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300;400;500&family=Space+Mono:wght@400&family=Inter:wght@300;400&display=swap" rel="stylesheet">
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:'Inter',sans-serif;background:#fafaf8;color:#1a1a1a;line-height:1.7;overflow-x:hidden}
.sidebar{position:fixed;left:0;top:0;width:260px;height:100vh;background:rgba(255,255,255,.97);border-right:1px solid rgba(0,0,0,.06);padding:32px 24px;z-index:100;overflow-y:auto;display:flex;flex-direction:column}
.sidebar-profile{text-align:center;margin-bottom:28px;padding-bottom:24px;border-bottom:1px solid rgba(0,0,0,.08)}
.profile-icon{font-size:48px;margin-bottom:8px}
.profile-name{font-family:'Cormorant Garamond',serif;font-size:1.3rem;font-weight:500;margin-bottom:4px}
.profile-title{font-size:.68rem;color:#888;letter-spacing:.08em;text-transform:uppercase;margin-bottom:8px}
.profile-bio{font-size:.78rem;color:#666;line-height:1.5}
.sidebar-nav{flex:1;margin-top:16px}
.nav-section{margin-bottom:20px}
.nav-section-title{font-size:.6rem;font-weight:600;color:#aaa;letter-spacing:.15em;text-transform:uppercase;margin-bottom:10px}
.nav-list{list-style:none}
.nav-list li{margin-bottom:5px}
.nav-list a{font-size:.78rem;color:#555;text-decoration:none;transition:all .2s;display:block;padding:3px 0}
.nav-list a:hover{color:#0080c6;padding-left:4px}
.nav-list a.active{color:#0080c6;font-weight:500}
.nav-list a.done{color:#28a745}
.badge{display:inline-block;font-size:.5rem;background:#0080c6;color:#fff;padding:1px 5px;border-radius:8px;margin-left:3px;vertical-align:middle}
.badge-done{background:#28a745}
.sidebar-footer{padding-top:16px;border-top:1px solid rgba(0,0,0,.06);font-size:.65rem;color:#aaa;text-align:center}
.main-wrapper{margin-left:260px;min-height:100vh}
.container{max-width:1100px;margin:0 auto;padding:50px 40px 80px}
.paper-content{font-family:'Times New Roman','Nanum Myeongjo',serif;line-height:1.8;background:#fff;padding:40px;border-radius:8px;box-shadow:0 2px 20px rgba(0,0,0,.05)}
.paper-header{text-align:center;margin-bottom:40px;padding-bottom:30px;border-bottom:2px solid #333}
.paper-category{font-size:14px;color:#666;margin-bottom:10px}
.paper-title{font-size:24px;font-weight:bold;margin-bottom:12px;line-height:1.4}
.paper-subtitle{font-size:14px;color:#555;margin-bottom:8px}
.paper-team{font-size:13px;color:#444}
.code-output{background:#1e1e1e;color:#d4d4d4;padding:12px 16px;border-radius:0 0 6px 6px;font-family:'Space Mono',monospace;font-size:11.5px;line-height:1.6;margin-top:-4px;margin-bottom:18px;border-top:2px solid #333;white-space:pre-wrap;overflow-x:auto}
.code-output .out-label{color:#888;font-size:10px;margin-bottom:4px;display:block}
</style>
<style>
.abstract{background:#f8f9fa;padding:25px;margin:30px 0;border-left:4px solid #2c3e50}
.abstract-title{font-weight:bold;font-size:16px;margin-bottom:15px}
h2{font-size:18px;margin:35px 0 20px;padding-bottom:8px;border-bottom:1px solid #ddd;color:#2c3e50}
h3{font-size:15px;margin:25px 0 15px;color:#34495e}
h4{font-size:14px;margin:20px 0 12px;color:#34495e}
p{text-align:justify;margin-bottom:15px;text-indent:2em}
p.ni{text-indent:0}
table{width:100%;border-collapse:collapse;margin:20px 0;font-size:12px}
th,td{border:1px solid #ddd;padding:10px 8px;text-align:center}
th{background:#2c3e50;color:white;font-weight:bold}
tr:nth-child(even){background:#f8f9fa}
tr:hover{background:#e8f4f8}
.tc{font-size:13px;font-weight:bold;margin:15px 0 10px;text-align:center}
.eq{text-align:center;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:4px;overflow-x:auto}
ul,ol{margin-left:2em;margin-bottom:15px}
li{margin-bottom:6px}
.def{background:#fff9e6;border:1px solid #ffc107;border-radius:4px;padding:20px;margin:20px 0}
.info{background:#e8f4f8;border-left:4px solid #3498db;padding:20px;margin:20px 0}
.warn{background:#fff3cd;border-left:4px solid #f39c12;padding:20px;margin:20px 0}
.ok{background:#d4edda;border-left:4px solid #28a745;padding:20px;margin:20px 0}
pre{background:#1e1e1e;color:#d4d4d4;padding:20px;border-radius:6px;overflow-x:auto;margin:20px 0;font-family:'Space Mono','Consolas',monospace;font-size:13px;line-height:1.6}
code{font-family:'Space Mono','Consolas',monospace;font-size:13px}
p code,li code,td code{background:#f0f0f0;padding:2px 6px;border-radius:3px;color:#c7254e;font-size:12px}
.cc{font-size:12px;font-weight:bold;color:#2c3e50;margin-top:15px;margin-bottom:4px}
.cm{color:#6a9955}.kw{color:#569cd6}.st{color:#ce9178}.fn{color:#dcdcaa}.nb{color:#4ec9b0}.nu{color:#b5cea8}
.progress-bar{width:100%;height:6px;background:#e0e0e0;border-radius:3px;margin-top:16px}
.progress-fill{height:100%;background:linear-gradient(90deg,#0080c6,#00b894);border-radius:3px;width:60%}
.progress-label{font-size:11px;color:#888;margin-top:4px;text-align:center}
@media(max-width:1024px){
.sidebar{width:100%;height:auto;position:relative;border-right:none;border-bottom:1px solid rgba(0,0,0,.08);padding:16px}
.sidebar-profile{margin-bottom:10px;padding-bottom:10px;display:flex;align-items:center;gap:12px;text-align:left}
.profile-icon{font-size:32px;margin-bottom:0}.profile-bio{display:none}
.nav-section{display:inline-block;margin-right:16px;margin-bottom:8px}
.nav-list{display:flex;gap:10px;flex-wrap:wrap}.nav-list li{margin-bottom:0}
.sidebar-footer{display:none}
.main-wrapper{margin-left:0}
.container{padding:0}.paper-content{padding:20px 16px;border-radius:0;box-shadow:none}
.paper-title{font-size:18px}p{font-size:14px;text-indent:1.5em;text-align:left}
pre{font-size:11px;padding:14px}table{font-size:10px;display:block;overflow-x:auto}
}
</style>
</head>
<body>

<div class="sidebar">
<div class="sidebar-profile">
<div class="profile-icon">&#x1F680;</div>
<div class="profile-name">HFT ML Master Plan</div>
<div class="profile-title">Convex Opt + DL + HFT</div>
<div class="profile-bio">10 Rounds: Zero to HFT System Trading</div>
</div>
<div class="sidebar-nav">
<div class="nav-section">
<div class="nav-section-title">Curriculum</div>
<ul class="nav-list">
<li><a class="done" href="../round_01/lecture_01.html">R1. Python + Finance <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_02/lecture_02.html">R2. Linear Algebra + Stats <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_03/lecture_03.html">R3. Data / Feature Eng. <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_04/lecture_04.html">R4. Supervised Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_05/lecture_05.html">R5. Unsupervised + TS <span class="badge badge-done">DONE</span></a></li>
<li><a class="active" href="#">R6. NLP + Sentiment <span class="badge">NOW</span></a></li>
<li><a href="#">R7. Deep Learning</a></li>
<li><a href="#">R8. Convex Opt + Transformer</a></li>
<li><a href="#">R9. HFT + RL</a></li>
<li><a href="#">R10. Final Project</a></li>
</ul>
</div>
<div class="nav-section">
<div class="nav-section-title">This Lecture</div>
<ul class="nav-list">
<li><a href="#ch1">1. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ í˜</a></li>
<li><a href="#ch2">2. NLP íŒŒì´í”„ë¼ì¸</a></li>
<li><a href="#ch3">3. í† í°í™”ì™€ ì „ì²˜ë¦¬</a></li>
<li><a href="#ch4">4. Bag-of-Words</a></li>
<li><a href="#ch5">5. TF-IDF</a></li>
<li><a href="#ch6">6. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ</a></li>
<li><a href="#ch7">7. ê°ì„±ë¶„ì„ ì‹¤ì „</a></li>
<li><a href="#ch8">8. í† í”½ ëª¨ë¸ë§ (LDA)</a></li>
<li><a href="#ch9">9. Word2Vec</a></li>
<li><a href="#ch10">10. Doc2Vec + ê°ì„±</a></li>
<li><a href="#ch11">11. Transformer/BERT</a></li>
<li><a href="#ch12">12. ê¸ˆìœµ NLP ì‹¤ì „</a></li>
<li><a href="#ch13">13. í”¼ë“œë°± + Quiz</a></li>
</ul>
</div>
</div>
<div class="sidebar-footer">Round 6 of 10 Â· ğŸ“ NLP + Sentiment</div>
</div>

<div class="main-wrapper">
<div class="container">
<div class="paper-content">

<div class="paper-header">
<div class="paper-category">Round 6 / 10 Â· ML í•µì‹¬ ë¬´ê¸° í™•ì¥</div>
<h1 class="paper-title">NLP &amp; Sentiment Analysis for Algorithmic Trading</h1>
<div class="paper-subtitle">í…ìŠ¤íŠ¸ì—ì„œ ì•ŒíŒŒë¥¼ ì¶”ì¶œí•œë‹¤ â€” ë‰´ìŠ¤, ê³µì‹œ, ì–´ë‹ì½œì´ ìˆ«ìë³´ë‹¤ ë¨¼ì € ë§í•œë‹¤</div>
<div class="paper-team">Textbooks: MLAT Ch.14~16 / MLDSF Ch.11~12</div>
<div class="progress-bar"><div class="progress-fill"></div></div>
<div class="progress-label">Overall Progress: 60%</div>
</div>

<div class="abstract">
<div class="abstract-title">Abstract</div>
<p class="ni">
ë¼ìš´ë“œ 5ê¹Œì§€ ìš°ë¦¬ëŠ” ìˆ«ì ë°ì´í„°ë§Œ ë‹¤ë¤˜ë‹¤. ì£¼ê°€, ìˆ˜ìµë¥ , ë³€ë™ì„±, ìƒê´€ê³„ìˆ˜ â€” ëª¨ë‘ ê¹”ë”í•œ ìˆ˜ì¹˜ì˜€ë‹¤. í•˜ì§€ë§Œ ê¸ˆìœµ ì‹œì¥ì„ ì›€ì§ì´ëŠ” ì •ë³´ì˜ ëŒ€ë¶€ë¶„ì€ í…ìŠ¤íŠ¸ë¡œ ì¡´ì¬í•œë‹¤. ì—°ì¤€ ì˜ì¥ì˜ ê¸°ìíšŒê²¬ í•œ ë§ˆë””ê°€ ì‹œì¥ì„ ë’¤í”ë“¤ê³ , ê¸°ì—…ì˜ 10-K ê³µì‹œ í•œ ë¬¸ì¥ì´ ì£¼ê°€ë¥¼ 10% ì›€ì§ì¸ë‹¤. Bloomberg í„°ë¯¸ë„ì— ìŸì•„ì§€ëŠ” ë‰´ìŠ¤ í—¤ë“œë¼ì¸, íŠ¸ìœ„í„°ì˜ ì‹¤ì‹œê°„ ì—¬ë¡ , ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ì˜ ë¯¸ë¬˜í•œ ì–´ì¡° ë³€í™” â€” ì´ ëª¨ë“  ê²ƒì´ ì•ŒíŒŒì˜ ì›ì²œì´ë‹¤.
</p>
<p class="ni" style="margin-top:10px">
ì´ë²ˆ ë¼ìš´ë“œì—ì„œëŠ” <strong>ìì—°ì–´ ì²˜ë¦¬(NLP)</strong>ì˜ í•µì‹¬ ê¸°ë²•ì„ ë°°ìš´ë‹¤. í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ìª¼ê°œê³ (í† í°í™”), ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ (BoW, TF-IDF), ì˜ë¯¸ë¥¼ ì••ì¶•í•˜ê³ (í† í”½ ëª¨ë¸ë§, Word2Vec), ê°ì„±ì„ ë¶„ë¥˜í•˜ê³ (ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ, LightGBM), ìµœì‹  ì–¸ì–´ ëª¨ë¸(BERT, Transformer)ê¹Œì§€ â€” í…ìŠ¤íŠ¸ì—ì„œ ë§¤ë§¤ ì‹œê·¸ë„ì„ ì¶”ì¶œí•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤.
</p>
<p class="ni" style="margin-top:10px">
<strong>êµì¬ ì—°ë™:</strong> MLAT Ch.14 "Text Data for Trading â€“ Sentiment Analysis" + Ch.15 "Topic Modeling â€“ Summarizing Financial News" + Ch.16 "Word Embeddings for Earnings Calls and SEC Filings" / MLDSF Ch.11~12 (NLP ì¼€ì´ìŠ¤ìŠ¤í„°ë””)
</p>
</div>


<!-- ==================== Ch.1 ==================== -->
<h2 id="ch1">Chapter 1. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ í˜ â€” ìˆ«ìë³´ë‹¤ ë¨¼ì € ë§í•˜ëŠ” ì‹œì¥</h2>

<h3>1.1 ì™œ í…ìŠ¤íŠ¸ì¸ê°€: ì •ë³´ì˜ ë¹„ëŒ€ì¹­</h3>

<p>
ê¸ˆìœµ ì‹œì¥ì—ì„œ ê°€ê²©ì€ ì •ë³´ë¥¼ ë°˜ì˜í•œë‹¤ â€” ì´ê²ƒì´ íš¨ìœ¨ì  ì‹œì¥ ê°€ì„¤(EMH)ì˜ í•µì‹¬ì´ë‹¤. í•˜ì§€ë§Œ ëª¨ë“  ì •ë³´ê°€ ë™ì‹œì— ê°€ê²©ì— ë°˜ì˜ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ê¸°ì—…ì˜ 10-K ì—°ê°„ ë³´ê³ ì„œê°€ SECì— ì œì¶œë˜ëŠ” ìˆœê°„, ê·¸ ìˆ˜ë°± í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  í•´ì„í•˜ëŠ” ë°ëŠ” ì‹œê°„ì´ ê±¸ë¦°ë‹¤. ì—°ì¤€ ì˜ì¥ì˜ ê¸°ìíšŒê²¬ ë°œì–¸ì´ ë‚˜ì˜¤ëŠ” ìˆœê°„, "hawkish"ì¸ì§€ "dovish"ì¸ì§€ íŒë‹¨í•˜ëŠ” ë°ë„ ì‹œê°„ì´ ê±¸ë¦°ë‹¤. ì´ ì‹œê°„ ì°¨ì´ê°€ ë°”ë¡œ ì•ŒíŒŒì˜ ì›ì²œì´ë‹¤.
</p>

<p>
MLAT Ch.14ì—ì„œ Stefan Jansenì€ ì´ë ‡ê²Œ ë§í•œë‹¤: "Text data can be extremely valuable given how much information humans communicate and store using natural language." ì‹¤ì œë¡œ ê¸ˆìœµ ì‹œì¥ì—ì„œ ìƒì„±ë˜ëŠ” ë°ì´í„°ì˜ 80% ì´ìƒì´ ë¹„ì •í˜•(unstructured) ë°ì´í„°ì´ë©°, ê·¸ ëŒ€ë¶€ë¶„ì´ í…ìŠ¤íŠ¸ë‹¤. ë‰´ìŠ¤ ê¸°ì‚¬, ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸, ê¸°ì—… ê³µì‹œ, ì†Œì…œ ë¯¸ë””ì–´, ì¤‘ì•™ì€í–‰ ì˜ì‚¬ë¡ â€” ì´ ëª¨ë“  ê²ƒì´ ê°€ê²©ì— ë°˜ì˜ë˜ê¸° ì „ì— í…ìŠ¤íŠ¸ë¡œ ë¨¼ì € ì¡´ì¬í•œë‹¤.
</p>

<!-- ì •ë³´ íë¦„ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#f0f4f8,#e8eaf6);border-radius:12px;border:1px solid #c5cae9">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:20px;font-size:14px;color:#283593">ğŸ“° í…ìŠ¤íŠ¸ â†’ ì‹œê·¸ë„ â†’ ê°€ê²©: ì •ë³´ íë¦„ì˜ ì‹œê°„ ì°¨</p>
<div style="display:flex;align-items:center;justify-content:center;gap:8px;flex-wrap:wrap;font-size:12px">
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #1565c0;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">ğŸ“„</div>
<div style="font-weight:bold;color:#1565c0">í…ìŠ¤íŠ¸ ë°œìƒ</div>
<div style="color:#777;font-size:10px;margin-top:4px">10-K ê³µì‹œ, ë‰´ìŠ¤, ì–´ë‹ì½œ</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = 0</div>
</div>
<div style="font-size:20px;color:#999">â†’</div>
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #7b1fa2;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">ğŸ¤–</div>
<div style="font-weight:bold;color:#7b1fa2">NLP ë¶„ì„</div>
<div style="color:#777;font-size:10px;margin-top:4px">ê°ì„± ì¶”ì¶œ, í† í”½ ë¶„ë¥˜</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = ë°€ë¦¬ì´ˆ</div>
</div>
<div style="font-size:20px;color:#999">â†’</div>
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #2e7d32;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">ğŸ“Š</div>
<div style="font-weight:bold;color:#2e7d32">ë§¤ë§¤ ì‹œê·¸ë„</div>
<div style="color:#777;font-size:10px;margin-top:4px">Long/Short ê²°ì •</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = ì´ˆ</div>
</div>
<div style="font-size:20px;color:#999">â†’</div>
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #e65100;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">ğŸ’°</div>
<div style="font-weight:bold;color:#e65100">ê°€ê²© ë°˜ì˜</div>
<div style="color:#777;font-size:10px;margin-top:4px">ì‹œì¥ ì°¸ì—¬ì ë°˜ì‘</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = ë¶„~ì‹œê°„</div>
</div>
</div>
<p class="ni" style="text-align:center;margin-top:14px;font-size:11px;color:#666">NLPë¡œ í…ìŠ¤íŠ¸ë¥¼ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ë©´, ì‹œì¥ì´ ë°˜ì‘í•˜ê¸° ì „ì— í¬ì§€ì…˜ì„ ì¡ì„ ìˆ˜ ìˆë‹¤</p>
</div>

<h3>1.2 ê¸ˆìœµ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì¢…ë¥˜</h3>

<p>
ê¸ˆìœµì—ì„œ í™œìš© ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” í¬ê²Œ ë„¤ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ë‰œë‹¤. ê°ê°ì˜ íŠ¹ì„±ê³¼ í™œìš© ë°©ë²•ì´ ë‹¤ë¥´ë¯€ë¡œ, NLP íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•  ë•Œ ë°ì´í„° ì†ŒìŠ¤ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
</p>

<div class="tc">í‘œ 1-1. ê¸ˆìœµ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ 4ëŒ€ ë²”ì£¼</div>
<table>
<thead>
<tr><th>ë²”ì£¼</th><th>ì˜ˆì‹œ</th><th>íŠ¹ì„±</th><th>ë¹ˆë„</th><th>NLP í™œìš©</th></tr>
</thead>
<tbody>
<tr><td><strong>ê¸°ì—… ê³µì‹œ</strong></td><td>10-K, 10-Q, 8-K, ì–´ë‹ì½œ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸</td><td>ê³µì‹ì , ë²•ì  êµ¬ì†ë ¥, ì¥ë¬¸</td><td>ë¶„ê¸°/ì—°ê°„</td><td>ê°ì„±ë¶„ì„, í† í”½ ë³€í™” ì¶”ì </td></tr>
<tr><td><strong>ë‰´ìŠ¤</strong></td><td>Bloomberg, Reuters, WSJ, FT</td><td>ì‹¤ì‹œê°„, ê°„ê²°, í—¤ë“œë¼ì¸ ì¤‘ì‹¬</td><td>ì´ˆ~ë¶„ ë‹¨ìœ„</td><td>ì´ë²¤íŠ¸ ê°ì§€, ê°ì„± ìŠ¤ì½”ì–´ë§</td></tr>
<tr><td><strong>ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸</strong></td><td>Goldman Sachs, Morgan Stanley ë¦¬ì„œì¹˜</td><td>ì „ë¬¸ì , ì˜ê²¬ í¬í•¨, íƒ€ê²Ÿ í”„ë¼ì´ìŠ¤</td><td>ìˆ˜ì‹œ</td><td>ì¶”ì²œ ë³€ê²½ ê°ì§€, ì–´ì¡° ë¶„ì„</td></tr>
<tr><td><strong>ì†Œì…œ ë¯¸ë””ì–´</strong></td><td>Twitter/X, Reddit, StockTwits</td><td>ë¹„ê³µì‹, ë…¸ì´ì¦ˆ ë§ìŒ, ì‹¤ì‹œê°„</td><td>ì´ˆ ë‹¨ìœ„</td><td>êµ°ì¤‘ ê°ì„±, ë°ˆ ì£¼ì‹ ê°ì§€</td></tr>
</tbody>
</table>

<h3>1.3 R5ê¹Œì§€ì˜ ì—°ê²°: ìˆ«ì â†’ í…ìŠ¤íŠ¸</h3>

<p>
ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ê°€ ë°°ìš´ ê²ƒì„ ì •ë¦¬í•´ë³´ì. R1ì—ì„œ íŒŒì´ì¬ ê¸°ì´ˆë¥¼ ìµíˆê³ , R2ì—ì„œ ì„ í˜•ëŒ€ìˆ˜/í†µê³„ë¥¼ ë°°ìš°ê³ , R3ì—ì„œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ í•˜ê³ , R4ì—ì„œ ì§€ë„í•™ìŠµ ëª¨ë¸ì„ ë§Œë“¤ê³ , R5ì—ì„œ ë¹„ì§€ë„í•™ìŠµê³¼ ì‹œê³„ì—´ì„ ë‹¤ë¤˜ë‹¤. ì´ ëª¨ë“  ê³¼ì •ì—ì„œ ì…ë ¥ ë°ì´í„°ëŠ” í•­ìƒ ìˆ«ìì˜€ë‹¤ â€” ì£¼ê°€, ìˆ˜ìµë¥ , ê¸°ìˆ ì  ì§€í‘œ, ê³µë¶„ì‚° í–‰ë ¬.
</p>

<p>
ì´ë²ˆ ë¼ìš´ë“œì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ê°€ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¬ë¼ì§„ë‹¤. í…ìŠ¤íŠ¸ëŠ” ìˆ«ìê°€ ì•„ë‹ˆë‹¤. "Apple reported strong earnings"ë¼ëŠ” ë¬¸ì¥ì„ XGBoostì— ë°”ë¡œ ë„£ì„ ìˆ˜ ì—†ë‹¤. í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì • â€” ì´ê²ƒì´ NLPì˜ í•µì‹¬ì´ë‹¤. ê·¸ë¦¬ê³  ì´ ë³€í™˜ ë°©ë²•ì— ë”°ë¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ê·¹ì ìœ¼ë¡œ ë‹¬ë¼ì§„ë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ì´ë²ˆ ë¼ìš´ë“œì˜ í•µì‹¬ ì§ˆë¬¸</strong></p>
<p class="ni" style="margin-top:8px">"í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ ìˆ«ìë¡œ ë°”ê¿€ ê²ƒì¸ê°€?" â€” ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ NLPì˜ ì—­ì‚¬ ê·¸ ìì²´ë‹¤.</p>
<ol style="margin-top:10px">
<li><strong>Bag-of-Words (Ch.4~5):</strong> ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ì„¼ë‹¤ â†’ í¬ì†Œ(sparse) ë²¡í„°</li>
<li><strong>TF-IDF (Ch.5):</strong> ë¹ˆë„ì— ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•œë‹¤ â†’ ê°œì„ ëœ í¬ì†Œ ë²¡í„°</li>
<li><strong>í† í”½ ëª¨ë¸ë§ (Ch.8):</strong> ë¬¸ì„œì˜ ì ì¬ ì£¼ì œë¥¼ ì¶”ì¶œí•œë‹¤ â†’ ì €ì°¨ì› ë°€ì§‘(dense) ë²¡í„°</li>
<li><strong>Word2Vec (Ch.9):</strong> ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ì„ë² ë”©í•œë‹¤ â†’ ë°€ì§‘ ë²¡í„°</li>
<li><strong>Transformer/BERT (Ch.11):</strong> ë¬¸ë§¥ê¹Œì§€ ê³ ë ¤í•œ ì„ë² ë”© â†’ ìµœì‹  ê¸°ìˆ </li>
</ol>
</div>

<h3>1.4 ì´ë²ˆ ë¼ìš´ë“œì˜ ë¡œë“œë§µ</h3>

<!-- ë¡œë“œë§µ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#f8f9fa,#e8eaf6);border-radius:12px;border:1px solid #c5cae9">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:20px;font-size:15px;color:#283593">ğŸ—ºï¸ Round 6 í•™ìŠµ ë¡œë“œë§µ</p>
<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(220px,1fr));gap:14px">
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #1976d2">
<div style="font-size:11px;color:#1976d2;font-weight:bold;margin-bottom:4px">PART 1 Â· Ch.1~3</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">ğŸ“ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬</div>
<div style="font-size:11px;color:#666;line-height:1.5">í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ìª¼ê°œê³ <br>ì •ì œí•˜ì—¬ ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“ ë‹¤</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#e3f2fd;color:#1565c0;padding:2px 8px;border-radius:10px;font-size:10px">í† í°í™”</span>
<span style="background:#e3f2fd;color:#1565c0;padding:2px 8px;border-radius:10px;font-size:10px">spaCy</span>
<span style="background:#e3f2fd;color:#1565c0;padding:2px 8px;border-radius:10px;font-size:10px">í‘œì œì–´í™”</span>
</div>
</div>
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #7b1fa2">
<div style="font-size:11px;color:#7b1fa2;font-weight:bold;margin-bottom:4px">PART 2 Â· Ch.4~7</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">ğŸ“Š ë²¡í„°í™” + ê°ì„±ë¶„ì„</div>
<div style="font-size:11px;color:#666;line-height:1.5">BoW, TF-IDFë¡œ ë²¡í„°í™”í•˜ê³ <br>ë‚˜ì´ë¸Œ ë² ì´ì¦ˆë¡œ ê°ì„±ì„ ë¶„ë¥˜í•œë‹¤</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#f3e5f5;color:#6a1b9a;padding:2px 8px;border-radius:10px;font-size:10px">BoW</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:2px 8px;border-radius:10px;font-size:10px">TF-IDF</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:2px 8px;border-radius:10px;font-size:10px">Naive Bayes</span>
</div>
</div>
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #2e7d32">
<div style="font-size:11px;color:#2e7d32;font-weight:bold;margin-bottom:4px">PART 3 Â· Ch.8~10</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">ğŸ§  ì„ë² ë”© + í† í”½</div>
<div style="font-size:11px;color:#666;line-height:1.5">LDAë¡œ í† í”½ì„ ì¶”ì¶œí•˜ê³ <br>Word2Vec/Doc2Vecìœ¼ë¡œ ì˜ë¯¸ë¥¼ ì„ë² ë”©í•œë‹¤</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#e8f5e9;color:#1b5e20;padding:2px 8px;border-radius:10px;font-size:10px">LDA</span>
<span style="background:#e8f5e9;color:#1b5e20;padding:2px 8px;border-radius:10px;font-size:10px">Word2Vec</span>
<span style="background:#e8f5e9;color:#1b5e20;padding:2px 8px;border-radius:10px;font-size:10px">Doc2Vec</span>
</div>
</div>
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #e65100">
<div style="font-size:11px;color:#e65100;font-weight:bold;margin-bottom:4px">PART 4 Â· Ch.11~13</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">ğŸ”® ìµœì‹  ê¸°ìˆ  + ì‹¤ì „</div>
<div style="font-size:11px;color:#666;line-height:1.5">Transformer/BERTë¥¼ ì´í•´í•˜ê³ <br>ê¸ˆìœµ NLP íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#fff3e0;color:#e65100;padding:2px 8px;border-radius:10px;font-size:10px">BERT</span>
<span style="background:#fff3e0;color:#e65100;padding:2px 8px;border-radius:10px;font-size:10px">FinBERT</span>
<span style="background:#fff3e0;color:#e65100;padding:2px 8px;border-radius:10px;font-size:10px">í”¼ë“œë°±</span>
</div>
</div>
</div>
<div style="text-align:center;margin-top:14px;font-size:11px;color:#666">
<span style="display:inline-block;width:10px;height:10px;background:#1976d2;border-radius:50%;margin-right:3px;vertical-align:middle"></span> ì „ì²˜ë¦¬
<span style="margin:0 8px">â†’</span>
<span style="display:inline-block;width:10px;height:10px;background:#7b1fa2;border-radius:50%;margin-right:3px;vertical-align:middle"></span> ë²¡í„°í™”+ê°ì„±
<span style="margin:0 8px">â†’</span>
<span style="display:inline-block;width:10px;height:10px;background:#2e7d32;border-radius:50%;margin-right:3px;vertical-align:middle"></span> ì„ë² ë”©
<span style="margin:0 8px">â†’</span>
<span style="display:inline-block;width:10px;height:10px;background:#e65100;border-radius:50%;margin-right:3px;vertical-align:middle"></span> ì‹¤ì „
</div>
</div>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.14ì˜ ì œëª©ì€ "Text Data for Trading â€“ Sentiment Analysis"ì´ë‹¤. ì œëª©ì´ ëª¨ë“  ê²ƒì„ ë§í•´ì¤€ë‹¤ â€” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ë”©ì— í™œìš©í•˜ë˜, ê°ì„±ë¶„ì„ì´ í•µì‹¬ ë„êµ¬ë‹¤. Ch.15 "Topic Modeling â€“ Summarizing Financial News"ëŠ” ì–´ë‹ì½œê³¼ ê¸ˆìœµ ë‰´ìŠ¤ì˜ í† í”½ì„ ì¶”ì¶œí•˜ê³ , Ch.16 "Word Embeddings for Earnings Calls and SEC Filings"ëŠ” SEC ê³µì‹œì—ì„œ ì›Œë“œ ì„ë² ë”©ì„ í•™ìŠµí•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ì— í™œìš©í•œë‹¤.</p>
</div>


<!-- ==================== Ch.2 ==================== -->
<h2 id="ch2">Chapter 2. NLP íŒŒì´í”„ë¼ì¸ â€” í…ìŠ¤íŠ¸ì—ì„œ í”¼ì²˜ê¹Œì§€ì˜ ì—¬ì •</h2>

<h3>2.1 NLP ì›Œí¬í”Œë¡œìš° ê°œê´€</h3>

<p>
MLAT Ch.14ì—ì„œ Jansenì€ NLP ì›Œí¬í”Œë¡œìš°ë¥¼ ëª…í™•í•˜ê²Œ ì •ë¦¬í•œë‹¤. í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ML ëª¨ë¸ì— ë„£ê¸°ê¹Œì§€ëŠ” ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤. ê° ë‹¨ê³„ê°€ ì™œ í•„ìš”í•œì§€, ì–´ë–¤ ì„ íƒì§€ê°€ ìˆëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
</p>

<!-- NLP íŒŒì´í”„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:18px;font-size:14px;color:#2c3e50">ğŸ”§ NLP íŒŒì´í”„ë¼ì¸: í…ìŠ¤íŠ¸ â†’ ML í”¼ì²˜</p>
<div style="display:flex;flex-direction:column;gap:10px;max-width:700px;margin:0 auto">

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#1976d2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">1</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#1976d2">í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (Data Acquisition)</div>
<div style="font-size:11px;color:#666;margin-top:4px">ë‰´ìŠ¤ API, SEC EDGAR, ì›¹ ìŠ¤í¬ë˜í•‘, ì†Œì…œ ë¯¸ë””ì–´ API</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">2</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#7b1fa2">íŒŒì‹± + í† í°í™” (Parsing & Tokenization)</div>
<div style="font-size:11px;color:#666;margin-top:4px">ë¬¸ì¥ ë¶„ë¦¬ â†’ ë‹¨ì–´ ë¶„ë¦¬ â†’ ì†Œë¬¸ìí™” â†’ ë¶ˆìš©ì–´ ì œê±° â†’ í‘œì œì–´í™”(Lemmatization)</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">3</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#2e7d32">ì–¸ì–´í•™ì  ì£¼ì„ (Linguistic Annotation)</div>
<div style="font-size:11px;color:#666;margin-top:4px">í’ˆì‚¬ íƒœê¹…(POS), ê°œì²´ëª… ì¸ì‹(NER), ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„(Dependency Parsing)</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#e65100;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">4</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#e65100">ë²¡í„°í™” (Vectorization)</div>
<div style="font-size:11px;color:#666;margin-top:4px">BoW / TF-IDF â†’ í¬ì†Œ í–‰ë ¬ | Word2Vec / Doc2Vec â†’ ë°€ì§‘ ë²¡í„° | BERT â†’ ë¬¸ë§¥ ì„ë² ë”©</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#c62828;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">5</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#c62828">ML ëª¨ë¸ë§ (Classification / Regression)</div>
<div style="font-size:11px;color:#666;margin-top:4px">ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ, LightGBM, ë¡œì§€ìŠ¤í‹± íšŒê·€, ë”¥ëŸ¬ë‹ â†’ ê°ì„± ì ìˆ˜, í† í”½ ë¶„ë¥˜, ì‹œê·¸ë„ ìƒì„±</div>
</div>
</div>

</div>
</div>

<h3>2.2 í•µì‹¬ ê°œë…: ë¬¸ì„œ(Document)ì™€ ì½”í¼ìŠ¤(Corpus)</h3>

<p>
NLPì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸ ìš©ì–´ ë‘ ê°€ì§€ë¥¼ ì •ë¦¬í•˜ì. <strong>ë¬¸ì„œ(Document)</strong>ëŠ” ë¶„ì„ì˜ ê¸°ë³¸ ë‹¨ìœ„ë‹¤. í•˜ë‚˜ì˜ ë‰´ìŠ¤ ê¸°ì‚¬, í•˜ë‚˜ì˜ 10-K ê³µì‹œ, í•˜ë‚˜ì˜ íŠ¸ìœ—ì´ ê°ê° í•˜ë‚˜ì˜ ë¬¸ì„œë‹¤. <strong>ì½”í¼ìŠ¤(Corpus)</strong>ëŠ” ë¬¸ì„œì˜ ì§‘í•©ì´ë‹¤. "2024ë…„ Bloomberg ê¸ˆìœµ ë‰´ìŠ¤ ì „ì²´"ê°€ í•˜ë‚˜ì˜ ì½”í¼ìŠ¤ê°€ ëœë‹¤.
</p>

<p>
R4ì˜ ì§€ë„í•™ìŠµê³¼ ë¹„êµí•˜ë©´ ì´í•´ê°€ ì‰½ë‹¤. R4ì—ì„œ í•˜ë‚˜ì˜ "ìƒ˜í”Œ"ì´ í•˜ë‚˜ì˜ ì¢…ëª©-ë‚ ì§œ ì¡°í•©ì´ì—ˆë‹¤ë©´, NLPì—ì„œ í•˜ë‚˜ì˜ "ìƒ˜í”Œ"ì€ í•˜ë‚˜ì˜ ë¬¸ì„œë‹¤. R4ì—ì„œ "í”¼ì²˜"ê°€ RSI, MACD ê°™ì€ ìˆ«ìì˜€ë‹¤ë©´, NLPì—ì„œ "í”¼ì²˜"ëŠ” ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë‚˜ ì„ë² ë”© ë²¡í„°ë‹¤.
</p>

<div class="tc">í‘œ 2-1. R4 ì§€ë„í•™ìŠµ vs R6 NLPì˜ ëŒ€ì‘ ê´€ê³„</div>
<table>
<thead>
<tr><th>ê°œë…</th><th>R4 (ìˆ«ì ë°ì´í„°)</th><th>R6 (í…ìŠ¤íŠ¸ ë°ì´í„°)</th></tr>
</thead>
<tbody>
<tr><td><strong>ìƒ˜í”Œ</strong></td><td>ì¢…ëª©-ë‚ ì§œ ì¡°í•©</td><td>ë¬¸ì„œ (ë‰´ìŠ¤ ê¸°ì‚¬, ê³µì‹œ)</td></tr>
<tr><td><strong>í”¼ì²˜</strong></td><td>RSI, MACD, ë³¼ë¦°ì €ë°´ë“œ</td><td>ë‹¨ì–´ ë¹ˆë„, TF-IDF, ì„ë² ë”© ë²¡í„°</td></tr>
<tr><td><strong>ë¼ë²¨</strong></td><td>ìƒìŠ¹(1) / í•˜ë½(0)</td><td>ê¸ì • / ë¶€ì • / ì¤‘ë¦½</td></tr>
<tr><td><strong>í”¼ì²˜ í–‰ë ¬</strong></td><td>N Ã— 20 (ë°€ì§‘)</td><td>N Ã— 50,000 (í¬ì†Œ) ë˜ëŠ” N Ã— 300 (ë°€ì§‘)</td></tr>
<tr><td><strong>ëª¨ë¸</strong></td><td>XGBoost, RF</td><td>ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ, LightGBM, BERT</td></tr>
</tbody>
</table>

<h3>2.3 NLPì˜ í•µì‹¬ ë„ì „: ì–¸ì–´ì˜ ëª¨í˜¸ì„±</h3>

<p>
ìˆ«ì ë°ì´í„°ì™€ ë‹¬ë¦¬, í…ìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ê³ ìœ í•œ ë„ì „ì´ ìˆë‹¤. ê°™ì€ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê³ (ë‹¤ì˜ì–´), ë‹¤ë¥¸ ë‹¨ì–´ê°€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤(ë™ì˜ì–´). "Apple"ì´ ê³¼ì¼ì¸ì§€ íšŒì‚¬ì¸ì§€ëŠ” ë¬¸ë§¥ì„ ë´ì•¼ ì•ˆë‹¤. "ì¢‹ë‹¤"ì™€ "í›Œë¥­í•˜ë‹¤"ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ì§€ë§Œ ë¹„ìŠ·í•œ ê°ì„±ì„ í‘œí˜„í•œë‹¤.
</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ ê¸ˆìœµ í…ìŠ¤íŠ¸ì˜ íŠ¹ìˆ˜í•œ ë„ì „ (MLAT Ch.14)</strong></p>
<ul>
<li><strong>ë„ë©”ì¸ íŠ¹ìˆ˜ ì–´íœ˜:</strong> "bull market", "dovish", "headwinds" ê°™ì€ ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ëŠ” ì¼ë°˜ NLP ëª¨ë¸ì´ ì˜ ì²˜ë¦¬í•˜ì§€ ëª»í•œë‹¤.</li>
<li><strong>ë¯¸ë¬˜í•œ ì–´ì¡°:</strong> "We expect moderate growth"ì™€ "We expect robust growth"ì˜ ì°¨ì´ëŠ” ë¯¸ë¬˜í•˜ì§€ë§Œ ì‹œì¥ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤.</li>
<li><strong>ë¶€ì •ì˜ ë³µì¡ì„±:</strong> "not unlikely to raise rates"ëŠ” ì´ì¤‘ ë¶€ì •ìœ¼ë¡œ, ë‹¨ìˆœ ê°ì„± ì‚¬ì „ìœ¼ë¡œëŠ” ì˜ëª» ë¶„ë¥˜ë  ìˆ˜ ìˆë‹¤.</li>
<li><strong>ì‹œê°„ ë¯¼ê°ì„±:</strong> ê°™ì€ ë¬¸ì¥ì´ë¼ë„ ì‹œì¥ ìƒí™©ì— ë”°ë¼ í•´ì„ì´ ë‹¬ë¼ì§„ë‹¤. "inflation is rising"ì€ 2020ë…„ê³¼ 2023ë…„ì— ì „í˜€ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤.</li>
</ul>
</div>


<h3>2.4 NLP ê¸°ë²•ì˜ ì§„í™”: ì „í†µ â†’ ë”¥ëŸ¬ë‹</h3>

<p>
NLP ê¸°ë²•ì€ í¬ê²Œ ì„¸ ì„¸ëŒ€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ê° ì„¸ëŒ€ëŠ” ì´ì „ ì„¸ëŒ€ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ë©´ì„œ ë°œì „í–ˆë‹¤. 
ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ìš°ë¦¬ëŠ” ì´ ì„¸ ì„¸ëŒ€ë¥¼ ëª¨ë‘ ë‹¤ë£¬ë‹¤.
</p>

<!-- NLP ì§„í™” íƒ€ì„ë¼ì¸ -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:18px;font-size:14px;color:#2c3e50">NLP ê¸°ë²•ì˜ 3ì„¸ëŒ€ ì§„í™”</p>

<div style="display:flex;gap:16px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:220px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #ff9800">
<p class="ni" style="font-weight:bold;color:#e65100;font-size:13px;margin-bottom:8px">ğŸ›ï¸ 1ì„¸ëŒ€: ê·œì¹™ ê¸°ë°˜ (1950s~)</p>
<ul style="font-size:11px;color:#555;margin:0;padding-left:16px;line-height:1.7">
<li>ê°ì„± ì‚¬ì „ (Loughran-McDonald)</li>
<li>ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­</li>
<li>ê·œì¹™ ê¸°ë°˜ íŒŒì‹±</li>
</ul>
<p class="ni" style="font-size:10px;color:#999;margin-top:8px">ì¥ì : í•´ì„ ê°€ëŠ¥, ë„ë©”ì¸ ì§€ì‹ ë°˜ì˜<br>í•œê³„: í™•ì¥ì„± ë¶€ì¡±, ë¬¸ë§¥ ë¬´ì‹œ</p>
</div>

<div style="flex:1;min-width:220px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #2196f3">
<p class="ni" style="font-weight:bold;color:#1565c0;font-size:13px;margin-bottom:8px">ğŸ“Š 2ì„¸ëŒ€: í†µê³„/ML ê¸°ë°˜ (1990s~)</p>
<ul style="font-size:11px;color:#555;margin:0;padding-left:16px;line-height:1.7">
<li>BoW, TF-IDF (Ch.4~5)</li>
<li>ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ, SVM (Ch.6)</li>
<li>LDA í† í”½ ëª¨ë¸ë§ (Ch.8)</li>
<li>Word2Vec, Doc2Vec (Ch.9~10)</li>
</ul>
<p class="ni" style="font-size:10px;color:#999;margin-top:8px">ì¥ì : ë°ì´í„° ê¸°ë°˜ í•™ìŠµ, í™•ì¥ ê°€ëŠ¥<br>í•œê³„: ê³ ì • ë²¡í„°, ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¶€ì¡±</p>
</div>

<div style="flex:1;min-width:220px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #4caf50">
<p class="ni" style="font-weight:bold;color:#2e7d32;font-size:13px;margin-bottom:8px">ğŸ¤– 3ì„¸ëŒ€: ë”¥ëŸ¬ë‹/ì‚¬ì „í•™ìŠµ (2017~)</p>
<ul style="font-size:11px;color:#555;margin:0;padding-left:16px;line-height:1.7">
<li>Transformer / Self-Attention (Ch.11)</li>
<li>BERT, GPT (ì‚¬ì „í•™ìŠµ)</li>
<li>FinBERT (ê¸ˆìœµ íŠ¹í™”)</li>
</ul>
<p class="ni" style="font-size:10px;color:#999;margin-top:8px">ì¥ì : ë¬¸ë§¥ ì´í•´, ì „ì´í•™ìŠµ, SOTA ì„±ëŠ¥<br>í•œê³„: ê³„ì‚° ë¹„ìš©, ë¸”ë™ë°•ìŠ¤</p>
</div>
</div>
</div>

<h3>2.5 ê¸ˆìœµ NLPì˜ ë°ì´í„° ì†ŒìŠ¤ (MLAT Ch.14)</h3>

<p>
Jansenì€ MLAT Ch.14ì—ì„œ ê¸ˆìœµ NLPì— ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œë‹¤. 
ê° ì†ŒìŠ¤ì˜ íŠ¹ì„±ê³¼ ì ‘ê·¼ ë°©ë²•ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì‹¤ì „ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì˜ ì²« ê±¸ìŒì´ë‹¤.
</p>

<div class="tc">í‘œ 2-2. ê¸ˆìœµ NLP ë°ì´í„° ì†ŒìŠ¤ (MLAT Ch.14 ê¸°ë°˜)</div>
<table>
<thead>
<tr><th>ë°ì´í„° ì†ŒìŠ¤</th><th>ì ‘ê·¼ ë°©ë²•</th><th>ê°±ì‹  ì£¼ê¸°</th><th>NLP í™œìš©</th><th>ë‚œì´ë„</th></tr>
</thead>
<tbody>
<tr><td><strong>SEC EDGAR</strong></td><td>EDGAR Full-Text Search API</td><td>ë¶„ê¸°/ì—°ê°„</td><td>10-K/10-Q ê°ì„± ë³€í™” ì¶”ì </td><td>â­â­</td></tr>
<tr><td><strong>ì–´ë‹ì½œ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸</strong></td><td>Seeking Alpha, S&P Capital IQ</td><td>ë¶„ê¸°</td><td>ê²½ì˜ì§„ ì–´ì¡° ë¶„ì„, Q&A ê°ì„±</td><td>â­â­â­</td></tr>
<tr><td><strong>ê¸ˆìœµ ë‰´ìŠ¤</strong></td><td>Reuters, Bloomberg, NewsAPI</td><td>ì‹¤ì‹œê°„</td><td>ì´ë²¤íŠ¸ ê°ì§€, ê°ì„± ì‹œê·¸ë„</td><td>â­â­</td></tr>
<tr><td><strong>ì—°ì¤€ ì˜ì‚¬ë¡</strong></td><td>Federal Reserve ì›¹ì‚¬ì´íŠ¸</td><td>6ì£¼ë§ˆë‹¤</td><td>í†µí™”ì •ì±… ë°©í–¥ ì˜ˆì¸¡</td><td>â­â­</td></tr>
<tr><td><strong>ì†Œì…œ ë¯¸ë””ì–´</strong></td><td>Twitter/X API, Reddit API</td><td>ì‹¤ì‹œê°„</td><td>êµ°ì¤‘ ì‹¬ë¦¬, ë°ˆ ì£¼ì‹ ê°ì§€</td><td>â­</td></tr>
<tr><td><strong>ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸</strong></td><td>Bloomberg, Refinitiv</td><td>ìˆ˜ì‹œ</td><td>ëª©í‘œê°€ ë³€ê²½, íˆ¬ìì˜ê²¬ ì¶”ì¶œ</td><td>â­â­â­</td></tr>
</tbody>
</table>

<div class="ok">
<p class="ni"><strong>ì‹¤ì „ íŒ: ë¬´ë£Œë¡œ ì‹œì‘í•˜ê¸°</strong></p>
<p class="ni" style="margin-top:8px">
í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” ë¹„ìš©ì´ ë“¤ì§€ ì•ŠëŠ” ë°ì´í„° ì†ŒìŠ¤ë¶€í„° ì‹œì‘í•˜ëŠ” ê²ƒì´ í˜„ì‹¤ì ì´ë‹¤. 
SEC EDGARëŠ” ì™„ì „ ë¬´ë£Œì´ê³  APIê°€ ì˜ ì •ë¹„ë˜ì–´ ìˆë‹¤. ì—°ì¤€ ì˜ì‚¬ë¡ë„ ê³µê°œ ë°ì´í„°ë‹¤. 
NewsAPIëŠ” ë¬´ë£Œ í”Œëœ(í•˜ë£¨ 100ê±´)ì„ ì œê³µí•œë‹¤. Redditì€ PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. 
Bloomberg Terminalì´ ì—†ì–´ë„ ì¶©ë¶„íˆ ì˜ë¯¸ ìˆëŠ” NLP í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
</p>
</div>

<!-- ==================== Ch.3 ==================== -->
<h2 id="ch3">Chapter 3. í† í°í™”ì™€ ì „ì²˜ë¦¬ â€” í…ìŠ¤íŠ¸ë¥¼ ì›ì ë‹¨ìœ„ë¡œ ìª¼ê°œë‹¤</h2>

<h3>3.1 í† í°(Token)ì´ë€ ë¬´ì—‡ì¸ê°€</h3>

<p>
í† í°(Token)ì€ í…ìŠ¤íŠ¸ì˜ ìµœì†Œ ë¶„ì„ ë‹¨ìœ„ë‹¤. ë³´í†µ í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ í•˜ë‚˜ì˜ í† í°ì´ì§€ë§Œ, ë°˜ë“œì‹œ ê·¸ëŸ° ê²ƒì€ ì•„ë‹ˆë‹¤. "New York"ì€ ë‘ ë‹¨ì–´ì§€ë§Œ í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„ì´ë¯€ë¡œ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. "don't"ëŠ” "do"ì™€ "not"ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ë„ ìˆê³ , í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ìœ ì§€í•  ìˆ˜ë„ ìˆë‹¤. ì´ëŸ° ì„ íƒì´ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
</p>

<p>
MLAT Ch.14ì—ì„œ Jansenì€ í† í°í™”ì˜ í•µì‹¬ì„ ì´ë ‡ê²Œ ì •ë¦¬í•œë‹¤: "A token is an instance of a sequence of characters in a given document and is considered a useful semantic unit for processing." ì¦‰, í† í°ì€ ì˜ë¯¸ ìˆëŠ” ìµœì†Œ ë‹¨ìœ„ì—¬ì•¼ í•œë‹¤.
</p>

<h3>3.2 spaCyë¥¼ ì´ìš©í•œ NLP íŒŒì´í”„ë¼ì¸</h3>

<p>
MLATì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” <code>spaCy</code>ë‹¤. spaCyëŠ” ì‚°ì—…ìš© NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, í† í°í™”, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹, ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„ì„ í•œ ë²ˆì— ì²˜ë¦¬í•œë‹¤. NLTKë³´ë‹¤ ë¹ ë¥´ê³ , íŒŒì´í”„ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì–´ ìˆì–´ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.
</p>

<div class="cc">ì½”ë“œ 3-1. spaCy ê¸°ë³¸ íŒŒì´í”„ë¼ì¸</div>
<pre><code><span class="kw">import</span> spacy

<span class="cm"># ì˜ì–´ ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ í•œ ë²ˆ: python -m spacy download en_core_web_sm)</span>
nlp = spacy.<span class="fn">load</span>(<span class="st">'en_core_web_sm'</span>)

<span class="cm"># ê¸ˆìœµ ë‰´ìŠ¤ ì˜ˆì‹œ ë¬¸ì¥</span>
text = <span class="st">"Apple reported strong Q4 earnings, beating Wall Street estimates by 15%."</span>

<span class="cm"># spaCy íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ â€” í•œ ì¤„ë¡œ í† í°í™” + POS + NER + ì˜ì¡´êµ¬ë¬¸ ë¶„ì„</span>
doc = <span class="fn">nlp</span>(text)

<span class="cm"># ê° í† í°ì˜ ì •ë³´ ì¶œë ¥</span>
<span class="fn">print</span>(<span class="st">f"{'í† í°':<12} {'í‘œì œì–´':<12} {'í’ˆì‚¬':<8} {'íƒœê·¸':<6} {'ì˜ì¡´ê´€ê³„':<10} {'ë¶ˆìš©ì–´?'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">70</span>)
<span class="kw">for</span> token <span class="kw">in</span> doc:
    <span class="fn">print</span>(<span class="st">f"{token.text:<12} {token.lemma_:<12} {token.pos_:<8} {token.tag_:<6} {token.dep_:<10} {token.is_stop}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>ì‹¤í–‰ ê²°ê³¼:</strong></p>
<pre style="background:#f8f9fa;color:#333;font-size:11px;padding:12px"><code>í† í°         í‘œì œì–´        í’ˆì‚¬     íƒœê·¸    ì˜ì¡´ê´€ê³„     ë¶ˆìš©ì–´?
----------------------------------------------------------------------
Apple        apple        PROPN    NNP    nsubj      False
reported     report       VERB     VBD    ROOT       False
strong       strong       ADJ      JJ     amod       False
Q4           q4           NOUN     NN     compound   False
earnings     earning      NOUN     NNS    dobj       False
,            ,            PUNCT    ,      punct      False
beating      beat         VERB     VBG    advcl      False
Wall         Wall         PROPN    NNP    compound   False
Street       Street       PROPN    NNP    compound   False
estimates    estimate     NOUN     NNS    dobj       False
by           by           ADP      IN     prep       True
15           15           NUM      CD     pobj       False
%            %            NOUN     NN     nmod       False
.            .            PUNCT    .      punct      False</code></pre>
</div>

<p>
ì´ í•œ ì¤„ì˜ ì½”ë“œ(<code>nlp(text)</code>)ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì„ ë¶„í•´í•´ë³´ì:
</p>

<div class="tc">í‘œ 3-1. spaCy íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„</div>
<table>
<thead>
<tr><th>ë‹¨ê³„</th><th>ì´ë¦„</th><th>ì„¤ëª…</th><th>ì˜ˆì‹œ</th></tr>
</thead>
<tbody>
<tr><td>1</td><td><strong>Tokenizer</strong></td><td>í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬</td><td>"don't" â†’ "do", "n't"</td></tr>
<tr><td>2</td><td><strong>Tagger</strong></td><td>í’ˆì‚¬(POS) íƒœê¹…</td><td>"reported" â†’ VERB</td></tr>
<tr><td>3</td><td><strong>Parser</strong></td><td>ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„</td><td>"Apple" â†’ nsubj (ì£¼ì–´)</td></tr>
<tr><td>4</td><td><strong>NER</strong></td><td>ê°œì²´ëª… ì¸ì‹</td><td>"Apple" â†’ ORG, "15%" â†’ PERCENT</td></tr>
<tr><td>5</td><td><strong>Lemmatizer</strong></td><td>í‘œì œì–´ ì¶”ì¶œ</td><td>"reported" â†’ "report"</td></tr>
</tbody>
</table>

<h3>3.3 í‘œì œì–´í™”(Lemmatization) vs ì–´ê°„ ì¶”ì¶œ(Stemming)</h3>

<p>
ê°™ì€ ë‹¨ì–´ì˜ ë³€í˜•ì„ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆë‹¤. <strong>ì–´ê°„ ì¶”ì¶œ(Stemming)</strong>ì€ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì ‘ë¯¸ì‚¬ë¥¼ ì˜ë¼ë‚¸ë‹¤. "running" â†’ "run", "better" â†’ "bet" (ì˜¤ë¥˜!). ë¹ ë¥´ì§€ë§Œ ë¶€ì •í™•í•˜ë‹¤. <strong>í‘œì œì–´í™”(Lemmatization)</strong>ì€ ì‚¬ì „ì„ ì°¸ì¡°í•˜ì—¬ ì›í˜•ì„ ì°¾ëŠ”ë‹¤. "running" â†’ "run", "better" â†’ "good". ëŠë¦¬ì§€ë§Œ ì •í™•í•˜ë‹¤.
</p>

<div class="cc">ì½”ë“œ 3-2. Stemming vs Lemmatization ë¹„êµ</div>
<pre><code><span class="kw">from</span> nltk.stem <span class="kw">import</span> SnowballStemmer
<span class="kw">import</span> spacy

stemmer = <span class="fn">SnowballStemmer</span>(<span class="st">'english'</span>)
nlp = spacy.<span class="fn">load</span>(<span class="st">'en_core_web_sm'</span>)

words = [<span class="st">'running'</span>, <span class="st">'better'</span>, <span class="st">'earnings'</span>, <span class="st">'reported'</span>, <span class="st">'companies'</span>]

<span class="fn">print</span>(<span class="st">f"{'ì›ë³¸':<12} {'Stemming':<12} {'Lemmatization'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">40</span>)
<span class="kw">for</span> word <span class="kw">in</span> words:
    stem = stemmer.<span class="fn">stem</span>(word)
    doc = <span class="fn">nlp</span>(word)
    lemma = doc[<span class="nu">0</span>].lemma_
    <span class="fn">print</span>(<span class="st">f"{word:<12} {stem:<12} {lemma}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>ì‹¤í–‰ ê²°ê³¼:</strong></p>
<pre style="background:#f8f9fa;color:#333;font-size:11px;padding:12px"><code>ì›ë³¸         Stemming     Lemmatization
----------------------------------------
running      run          run
better       better       well
earnings     earn         earning
reported     report       report
companies    compani      company</code></pre>
<p class="ni" style="margin-top:8px;font-size:12px"><strong>í•µì‹¬:</strong> Stemmingì€ "companies"ë¥¼ "compani"ë¡œ ì˜ëª» ìë¥´ì§€ë§Œ, Lemmatizationì€ "company"ë¡œ ì •í™•íˆ ë³µì›í•œë‹¤. ê¸ˆìœµ í…ìŠ¤íŠ¸ì—ì„œëŠ” ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ Lemmatizationì„ ê¶Œì¥í•œë‹¤.</p>
</div>

<h3>3.4 ë¶ˆìš©ì–´(Stop Words) ì œê±°</h3>

<p>
"the", "is", "at", "which" ê°™ì€ ë‹¨ì–´ëŠ” ê±°ì˜ ëª¨ë“  ë¬¸ì„œì— ë“±ì¥í•˜ë¯€ë¡œ ë¬¸ì„œë¥¼ êµ¬ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ì´ëŸ° ë‹¨ì–´ë¥¼ <strong>ë¶ˆìš©ì–´(Stop Words)</strong>ë¼ê³  í•˜ë©°, ë³´í†µ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì œê±°í•œë‹¤. spaCyëŠ” ê° í† í°ì— <code>is_stop</code> ì†ì„±ì„ ì œê³µí•˜ì—¬ ë¶ˆìš©ì–´ ì—¬ë¶€ë¥¼ ì‰½ê²Œ íŒë³„í•  ìˆ˜ ìˆë‹¤.
</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ ê¸ˆìœµì—ì„œ ë¶ˆìš©ì–´ ì œê±° ì‹œ ì£¼ì˜ì </strong></p>
<p class="ni" style="margin-top:8px">ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ ëª©ë¡ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ ìœ„í—˜í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "not"ì€ ì¼ë°˜ NLPì—ì„œ ë¶ˆìš©ì–´ë¡œ ë¶„ë¥˜ë˜ì§€ë§Œ, ê¸ˆìœµ ê°ì„±ë¶„ì„ì—ì„œëŠ” í•µì‹¬ ë‹¨ì–´ë‹¤. "not profitable"ì—ì„œ "not"ì„ ì œê±°í•˜ë©´ "profitable"ë§Œ ë‚¨ì•„ ê°ì„±ì´ ì™„ì „íˆ ë’¤ì§‘íŒë‹¤. ë”°ë¼ì„œ ê¸ˆìœµ NLPì—ì„œëŠ” ë¶ˆìš©ì–´ ëª©ë¡ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•´ì•¼ í•œë‹¤.</p>
</div>

<h3>3.5 ê°œì²´ëª… ì¸ì‹(NER): í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ</h3>

<p>
MLAT Ch.14ì—ì„œ ê°•ì¡°í•˜ëŠ” ë˜ í•˜ë‚˜ì˜ í•µì‹¬ ê¸°ë²•ì´ <strong>ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER)</strong>ì´ë‹¤. NERì€ í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ëŒ, ì¡°ì§, ì¥ì†Œ, ë‚ ì§œ, ê¸ˆì•¡ ë“±ì˜ ì—”í‹°í‹°ë¥¼ ìë™ìœ¼ë¡œ ì‹ë³„í•œë‹¤. ê¸ˆìœµì—ì„œëŠ” íŠ¹íˆ ê¸°ì—…ëª…, ì¸ë¬¼ëª…, ê¸ˆì•¡, ë‚ ì§œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
</p>

<div class="cc">ì½”ë“œ 3-3. spaCy NERë¡œ ê¸ˆìœµ ì—”í‹°í‹° ì¶”ì¶œ</div>
<pre><code>nlp = spacy.<span class="fn">load</span>(<span class="st">'en_core_web_sm'</span>)

text = <span class="st">"""
Federal Reserve Chair Jerome Powell signaled on Wednesday that the central bank 
may cut interest rates by 25 basis points in September. Goldman Sachs analysts 
expect the S&P 500 to reach 5,500 by year-end.
"""</span>

doc = <span class="fn">nlp</span>(text)

<span class="fn">print</span>(<span class="st">f"{'ì—”í‹°í‹°':<25} {'ë¼ë²¨':<12} {'ì„¤ëª…'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">60</span>)
<span class="kw">for</span> ent <span class="kw">in</span> doc.ents:
    <span class="fn">print</span>(<span class="st">f"{ent.text:<25} {ent.label_:<12} {spacy.explain(ent.label_)}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>ì‹¤í–‰ ê²°ê³¼:</strong></p>
<pre style="background:#f8f9fa;color:#333;font-size:11px;padding:12px"><code>ì—”í‹°í‹°                    ë¼ë²¨         ì„¤ëª…
------------------------------------------------------------
Federal Reserve          ORG          Companies, agencies, institutions
Jerome Powell            PERSON       People, including fictional
Wednesday                DATE         Absolute or relative dates
25 basis points          QUANTITY     Measurements
September                DATE         Absolute or relative dates
Goldman Sachs            ORG          Companies, agencies, institutions
S&P 500                  ORG          Companies, agencies, institutions
5,500                    CARDINAL     Numerals
year-end                 DATE         Absolute or relative dates</code></pre>
</div>

<h3>3.6 N-gram: ì—°ì† í† í°ì˜ ì¡°í•©</h3>

<p>
ë‹¨ì¼ í† í°ë§Œìœ¼ë¡œëŠ” í¬ì°©í•  ìˆ˜ ì—†ëŠ” ì˜ë¯¸ê°€ ìˆë‹¤. "interest rate"ëŠ” "interest"ì™€ "rate"ë¥¼ ë”°ë¡œ ë³´ë©´ ì˜ë¯¸ê°€ ë‹¬ë¼ì§„ë‹¤. <strong>N-gram</strong>ì€ Nê°œì˜ ì—°ì† í† í°ì„ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ë¬¶ëŠ” ê¸°ë²•ì´ë‹¤. Bigram(2-gram)ì€ "interest rate", "stock market", "earnings call" ê°™ì€ 2ë‹¨ì–´ ì¡°í•©ì´ê³ , Trigram(3-gram)ì€ "federal reserve bank", "year over year" ê°™ì€ 3ë‹¨ì–´ ì¡°í•©ì´ë‹¤.
</p>

<p>
MLAT Ch.14ì—ì„œëŠ” N-gramì´ Bag-of-Words ëª¨ë¸ì—ì„œ íŠ¹íˆ ìœ ìš©í•˜ë‹¤ê³  ê°•ì¡°í•œë‹¤. ë‹¨ì–´ ìˆœì„œ ì •ë³´ê°€ ì™„ì „íˆ ì‚¬ë¼ì§€ëŠ” BoWì—ì„œ, bigramì„ í¬í•¨í•˜ë©´ ì¼ë¶€ ìˆœì„œ ì •ë³´ë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.
</p>

<div class="cc">ì½”ë“œ 3-4. sklearn CountVectorizerë¡œ N-gram ìƒì„±</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer

corpus = [
    <span class="st">"Federal Reserve raised interest rates"</span>,
    <span class="st">"Interest rates impact stock market"</span>,
    <span class="st">"Stock market rallied after Fed announcement"</span>
]

<span class="cm"># unigram + bigram</span>
vectorizer = <span class="fn">CountVectorizer</span>(ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>), stop_words=<span class="st">'english'</span>)
X = vectorizer.<span class="fn">fit_transform</span>(corpus)

<span class="fn">print</span>(<span class="st">"ì–´íœ˜ (unigram + bigram):"</span>)
<span class="kw">for</span> term, idx <span class="kw">in</span> <span class="fn">sorted</span>(vectorizer.vocabulary_.items(), key=<span class="kw">lambda</span> x: x[<span class="nu">1</span>]):
    <span class="fn">print</span>(<span class="st">f"  [{idx:2d}] {term}"</span>)

<span class="fn">print</span>(<span class="st">f"\në¬¸ì„œ-ìš©ì–´ í–‰ë ¬ (shape: {X.shape}):"</span>)
<span class="fn">print</span>(X.<span class="fn">toarray</span>())
</code></pre>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.14ì—ì„œ Jansenì€ BBC ë‰´ìŠ¤ 2,225ê°œ ê¸°ì‚¬ë¥¼ ì‚¬ìš©í•˜ì—¬ CountVectorizerì˜ ë‹¤ì–‘í•œ ì„¤ì •(min_df, max_df, ngram_range)ì´ ì–´íœ˜ í¬ê¸°ì™€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤í—˜í•œë‹¤. í•µì‹¬ ë°œê²¬: bigramì„ í¬í•¨í•˜ë©´ ì–´íœ˜ í¬ê¸°ê°€ í¬ê²Œ ëŠ˜ì–´ë‚˜ì§€ë§Œ, ë¶„ë¥˜ ì •í™•ë„ë„ í–¥ìƒëœë‹¤.</p>
</div>


<!-- ==================== Ch.4 ==================== -->
<h2 id="ch4">Chapter 4. Bag-of-Words â€” ë‹¨ì–´ë¥¼ ì„¸ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•</h2>

<h3>4.1 BoWì˜ í•µì‹¬ ì•„ì´ë””ì–´</h3>

<p>
Bag-of-Words(BoW)ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì´ë‹¤. ì•„ì´ë””ì–´ëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ ë‹¨ìˆœí•˜ë‹¤: ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ê° ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì„¸ì„œ ë²¡í„°ë¡œ ë§Œë“ ë‹¤. ë‹¨ì–´ì˜ ìˆœì„œëŠ” ì™„ì „íˆ ë¬´ì‹œí•œë‹¤ â€” ê·¸ë˜ì„œ "bag(ê°€ë°©)"ì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆë‹¤. ë‹¨ì–´ë“¤ì„ ê°€ë°©ì— ë„£ê³  í”ë“¤ë©´ ìˆœì„œê°€ ì‚¬ë¼ì§€ì§€ë§Œ, ì–´ë–¤ ë‹¨ì–´ê°€ ëª‡ ê°œ ìˆëŠ”ì§€ëŠ” ì•Œ ìˆ˜ ìˆë‹¤.
</p>

<div class="def">
<p class="ni"><strong>Bag-of-Words ëª¨ë¸ (í•œ ë¬¸ì¥ ì •ì˜)</strong></p>
<p class="ni" style="margin-top:8px">ë¬¸ì„œë¥¼ ì–´íœ˜(vocabulary)ì˜ ê° ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¡œ í‘œí˜„í•œë‹¤. ê²°ê³¼ëŠ” <strong>ë¬¸ì„œ-ìš©ì–´ í–‰ë ¬(Document-Term Matrix, DTM)</strong>ì´ë‹¤.</p>
</div>

<h3>4.2 ë¬¸ì„œ-ìš©ì–´ í–‰ë ¬(DTM) êµ¬ì¶•</h3>

<p>
êµ¬ì²´ì ì¸ ì˜ˆë¥¼ ë“¤ì–´ë³´ì. ì„¸ ê°œì˜ ê¸ˆìœµ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì´ ìˆë‹¤ê³  í•˜ì:
</p>

<ul>
<li>Doc 1: "Fed raises rates"</li>
<li>Doc 2: "Fed cuts rates sharply"</li>
<li>Doc 3: "Market rallies after rate cuts"</li>
</ul>

<p>
ì „ì²´ ì–´íœ˜ëŠ” {after, cuts, fed, market, raises, rallies, rate, rates, sharply}ì´ë‹¤. ê° ë¬¸ì„œë¥¼ ì´ ì–´íœ˜ì— ëŒ€í•œ ë¹ˆë„ ë²¡í„°ë¡œ í‘œí˜„í•˜ë©´:
</p>

<!-- DTM ì‹œê°í™” -->
<div style="margin:20px 0;overflow-x:auto">
<table style="font-size:12px">
<thead>
<tr><th></th><th>after</th><th>cuts</th><th>fed</th><th>market</th><th>raises</th><th>rallies</th><th>rate</th><th>rates</th><th>sharply</th></tr>
</thead>
<tbody>
<tr><td class="left"><strong>Doc 1</strong></td><td>0</td><td>0</td><td style="background:#e3f2fd;font-weight:bold">1</td><td>0</td><td style="background:#e3f2fd;font-weight:bold">1</td><td>0</td><td>0</td><td style="background:#e3f2fd;font-weight:bold">1</td><td>0</td></tr>
<tr><td class="left"><strong>Doc 2</strong></td><td>0</td><td style="background:#f3e5f5;font-weight:bold">1</td><td style="background:#f3e5f5;font-weight:bold">1</td><td>0</td><td>0</td><td>0</td><td>0</td><td style="background:#f3e5f5;font-weight:bold">1</td><td style="background:#f3e5f5;font-weight:bold">1</td></tr>
<tr><td class="left"><strong>Doc 3</strong></td><td style="background:#e8f5e9;font-weight:bold">1</td><td style="background:#e8f5e9;font-weight:bold">1</td><td>0</td><td style="background:#e8f5e9;font-weight:bold">1</td><td>0</td><td style="background:#e8f5e9;font-weight:bold">1</td><td style="background:#e8f5e9;font-weight:bold">1</td><td>0</td><td>0</td></tr>
</tbody>
</table>
</div>

<p>
ì´ê²ƒì´ ë¬¸ì„œ-ìš©ì–´ í–‰ë ¬(DTM)ì´ë‹¤. í–‰ì€ ë¬¸ì„œ, ì—´ì€ ì–´íœ˜ì˜ ê° ë‹¨ì–´, ê°’ì€ ì¶œí˜„ ë¹ˆë„ë‹¤. ì´ì œ ê° ë¬¸ì„œëŠ” 9ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ëœë‹¤. ì´ ë²¡í„°ë¥¼ R4ì—ì„œ ë°°ìš´ ML ëª¨ë¸ì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆë‹¤!
</p>

<h3>4.3 sklearn CountVectorizer ì‹¤ì „</h3>

<div class="cc">ì½”ë“œ 4-1. CountVectorizerë¡œ DTM êµ¬ì¶•</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer
<span class="kw">import</span> pandas <span class="kw">as</span> pd

<span class="cm"># ê¸ˆìœµ ë‰´ìŠ¤ ì½”í¼ìŠ¤</span>
corpus = [
    <span class="st">"Apple reported record quarterly earnings beating analyst expectations"</span>,
    <span class="st">"Tesla shares dropped after disappointing earnings report"</span>,
    <span class="st">"Federal Reserve raised interest rates citing persistent inflation"</span>,
    <span class="st">"Goldman Sachs upgraded Apple stock to buy rating"</span>,
    <span class="st">"Oil prices surged amid supply disruptions in Middle East"</span>
]

<span class="cm"># CountVectorizer ì„¤ì •</span>
vectorizer = <span class="fn">CountVectorizer</span>(
    stop_words=<span class="st">'english'</span>,     <span class="cm"># ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°</span>
    min_df=<span class="nu">1</span>,                  <span class="cm"># ìµœì†Œ 1ê°œ ë¬¸ì„œì— ë“±ì¥</span>
    max_df=<span class="nu">0.9</span>,                <span class="cm"># 90% ì´ìƒ ë¬¸ì„œì— ë“±ì¥í•˜ë©´ ì œê±°</span>
    ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>),       <span class="cm"># unigram + bigram</span>
    max_features=<span class="nu">1000</span>          <span class="cm"># ìµœëŒ€ 1000ê°œ í”¼ì²˜</span>
)

<span class="cm"># DTM ìƒì„±</span>
dtm = vectorizer.<span class="fn">fit_transform</span>(corpus)

<span class="cm"># DataFrameìœ¼ë¡œ ì‹œê°í™”</span>
feature_names = vectorizer.<span class="fn">get_feature_names_out</span>()
df_dtm = pd.<span class="fn">DataFrame</span>(
    dtm.<span class="fn">toarray</span>(),
    columns=feature_names,
    index=[<span class="st">f'Doc {i+1}'</span> <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus))]
)

<span class="fn">print</span>(<span class="st">f"DTM shape: {dtm.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ì–´íœ˜ í¬ê¸°: {len(feature_names)}"</span>)
<span class="fn">print</span>(<span class="st">f"í¬ì†Œìœ¨: {1 - dtm.nnz / (dtm.shape[0] * dtm.shape[1]):.1%}"</span>)
<span class="fn">print</span>(<span class="st">f"\nìƒìœ„ 10ê°œ í”¼ì²˜:\n{df_dtm.sum().sort_values(ascending=False).head(10)}"</span>)
</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
DTM shape: (5, 28)
ì–´íœ˜ í¬ê¸°: 28
í¬ì†Œìœ¨: 80.0%

ìƒìœ„ 10ê°œ í”¼ì²˜:
earnings        2
apple           2
stock           2
report          1
record          1</div>


<h3>4.4 BoWì˜ í•œê³„: ì™œ ì´ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•œê°€</h3>

<p>
BoWëŠ” ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì´ì§€ë§Œ, ì‹¬ê°í•œ í•œê³„ê°€ ìˆë‹¤. ì²«ì§¸, <strong>ë‹¨ì–´ ìˆœì„œë¥¼ ì™„ì „íˆ ë¬´ì‹œ</strong>í•œë‹¤. "Fed raises rates"ì™€ "rates raises Fed"ê°€ ë™ì¼í•œ ë²¡í„°ë¥¼ ê°–ëŠ”ë‹¤. ë‘˜ì§¸, <strong>ê³ ë¹ˆë„ ë‹¨ì–´ê°€ ì§€ë°°</strong>í•œë‹¤. "the", "is", "of" ê°™ì€ ë‹¨ì–´ê°€ ëª¨ë“  ë¬¸ì„œì— ë§ì´ ë“±ì¥í•˜ì—¬ ë²¡í„°ë¥¼ ì§€ë°°í•˜ì§€ë§Œ, ì´ëŸ° ë‹¨ì–´ëŠ” ë¬¸ì„œë¥¼ êµ¬ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ì…‹ì§¸, <strong>í¬ì†Œì„±(Sparsity)</strong> ë¬¸ì œê°€ ìˆë‹¤. ì–´íœ˜ê°€ 50,000ê°œì´ê³  ë¬¸ì„œì— í‰ê·  200ê°œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´, ë²¡í„°ì˜ 99.6%ê°€ 0ì´ë‹¤.
</p>

<!-- BoW í•œê³„ ì‹œê°í™” -->
<div style="margin:25px 0;display:flex;gap:16px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:200px;max-width:300px;background:#fff;padding:16px;border-radius:10px;border:2px solid #e74c3c;text-align:center">
<div style="font-size:28px;margin-bottom:6px">ğŸ”€</div>
<div style="font-weight:bold;color:#e74c3c;font-size:13px;margin-bottom:6px">ìˆœì„œ ë¬´ì‹œ</div>
<div style="font-size:11px;color:#666">"dog bites man" =<br>"man bites dog"<br>ê°™ì€ ë²¡í„°!</div>
</div>
<div style="flex:1;min-width:200px;max-width:300px;background:#fff;padding:16px;border-radius:10px;border:2px solid #f39c12;text-align:center">
<div style="font-size:28px;margin-bottom:6px">ğŸ“Š</div>
<div style="font-weight:bold;color:#f39c12;font-size:13px;margin-bottom:6px">ê³ ë¹ˆë„ ì§€ë°°</div>
<div style="font-size:11px;color:#666">"the"ê°€ 100ë²ˆ ë“±ì¥í•˜ë©´<br>í•µì‹¬ ë‹¨ì–´ "earnings"(1ë²ˆ)ì„<br>ì••ë„í•œë‹¤</div>
</div>
<div style="flex:1;min-width:200px;max-width:300px;background:#fff;padding:16px;border-radius:10px;border:2px solid #9b59b6;text-align:center">
<div style="font-size:28px;margin-bottom:6px">ğŸ•³ï¸</div>
<div style="font-weight:bold;color:#9b59b6;font-size:13px;margin-bottom:6px">ê·¹ë‹¨ì  í¬ì†Œì„±</div>
<div style="font-size:11px;color:#666">50,000ì°¨ì› ë²¡í„°ì—ì„œ<br>99.6%ê°€ 0<br>ë©”ëª¨ë¦¬ ë‚­ë¹„ + ê³¼ì í•©</div>
</div>
</div>

<p>
ì´ í•œê³„ë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ TF-IDFê°€ ë“±ì¥í•œë‹¤. ë‹¤ìŒ ì±•í„°ì—ì„œ ìì„¸íˆ ë‹¤ë£¬ë‹¤.
</p>


<h3>4.5 BoWì˜ ìˆ˜í•™ì  í‘œí˜„</h3>

<p>
BoWë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì–´íœ˜(vocabulary) \(V = \{w_1, w_2, \ldots, w_{|V|}\}\)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, 
ë¬¸ì„œ \(d\)ì˜ BoW í‘œí˜„ì€:
</p>

$$\mathbf{x}_d = [\text{count}(w_1, d), \; \text{count}(w_2, d), \; \ldots, \; \text{count}(w_{|V|}, d)]$$

<p>
ì—¬ê¸°ì„œ \(\text{count}(w_i, d)\)ëŠ” ë‹¨ì–´ \(w_i\)ê°€ ë¬¸ì„œ \(d\)ì— ë“±ì¥í•˜ëŠ” íšŸìˆ˜ë‹¤. 
ì „ì²´ ì½”í¼ìŠ¤ \(D = \{d_1, d_2, \ldots, d_N\}\)ì— ëŒ€í•´ ì´ë¥¼ í–‰ë ¬ë¡œ ìŒ“ìœ¼ë©´ 
<strong>ë¬¸ì„œ-ìš©ì–´ í–‰ë ¬(DTM)</strong> \(\mathbf{X} \in \mathbb{R}^{N \times |V|}\)ê°€ ëœë‹¤.
</p>

<p>
ì´ì§„(binary) ë³€í˜•ë„ ìˆë‹¤. ë‹¨ì–´ì˜ ì¶œí˜„ ì—¬ë¶€ë§Œ ê¸°ë¡í•˜ëŠ” ë°©ì‹ì´ë‹¤:
</p>

$$x_{d,i} = \begin{cases} 1 & \text{if } w_i \in d \\ 0 & \text{otherwise} \end{cases}$$

<p>
MLAT Ch.14ì—ì„œ Jansenì€ ì´ì§„ BoWê°€ ê¸´ ë¬¸ì„œì—ì„œ ë” ì•ˆì •ì ì¼ ìˆ˜ ìˆë‹¤ê³  ì§€ì í•œë‹¤. 
10-K ê³µì‹œì²˜ëŸ¼ ìˆ˜ë§Œ ë‹¨ì–´ì˜ ë¬¸ì„œì—ì„œëŠ” ë‹¨ì–´ ë¹ˆë„ì˜ ì ˆëŒ€ê°’ë³´ë‹¤ ì¶œí˜„ ì—¬ë¶€ê°€ ë” ìœ ì˜ë¯¸í•œ í”¼ì²˜ê°€ ë  ìˆ˜ ìˆë‹¤.
</p>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.14 "The Document-Term Matrix"ì—ì„œ Jansenì€ DTMì˜ êµ¬ì¶• ê³¼ì •ì„ 
ìƒì„¸íˆ ë‹¤ë£¨ë©°, sklearnì˜ CountVectorizer íŒŒë¼ë¯¸í„°(min_df, max_df, max_features, ngram_range)ì˜ 
ì‹¤ì „ì  íŠœë‹ ê°€ì´ë“œë¥¼ ì œê³µí•œë‹¤. íŠ¹íˆ ê¸ˆìœµ í…ìŠ¤íŠ¸ì—ì„œëŠ” max_df=0.95, min_df=5 ì •ë„ê°€ ì¢‹ì€ ì¶œë°œì ì´ë¼ê³  ê¶Œì¥í•œë‹¤.</p>
</div>

<h3>4.6 ì‹¤ì „: BoWë¡œ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ë¥˜ (ê¸°ì´ˆ)</h3>

<div class="cc">ì½”ë“œ 4-2. BoW + ë¡œì§€ìŠ¤í‹± íšŒê·€ë¡œ ê°ì„± ë¶„ë¥˜</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer
<span class="kw">from</span> sklearn.linear_model <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë°ì´í„° (ì‹œë®¬ë ˆì´ì…˜)</span>
texts = [
    <span class="st">"Apple reports record quarterly revenue beating expectations"</span>,
    <span class="st">"Strong earnings growth drives stock to new highs"</span>,
    <span class="st">"Company announces massive share buyback program"</span>,
    <span class="st">"Analysts upgrade rating citing robust demand"</span>,
    <span class="st">"Revenue surges on strong consumer spending"</span>,
    <span class="st">"Stock plunges after disappointing earnings miss"</span>,
    <span class="st">"Company warns of supply chain disruptions"</span>,
    <span class="st">"Analysts downgrade amid slowing growth concerns"</span>,
    <span class="st">"Shares tumble on weak guidance and rising costs"</span>,
    <span class="st">"Regulatory lawsuit threatens core business model"</span>,
]
labels = [<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>]  <span class="cm"># 1=ê¸ì •, 0=ë¶€ì •</span>

<span class="cm"># BoW ë²¡í„°í™”</span>
vec = <span class="fn">CountVectorizer</span>(stop_words=<span class="st">'english'</span>)
X = vec.<span class="fn">fit_transform</span>(texts)

<span class="fn">print</span>(<span class="st">f"DTM shape: {X.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"ì–´íœ˜: {vec.get_feature_names_out()}"</span>)

<span class="cm"># ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ</span>
clf = <span class="fn">LogisticRegression</span>()
clf.<span class="fn">fit</span>(X, labels)

<span class="cm"># ìƒˆ ë‰´ìŠ¤ ì˜ˆì¸¡</span>
new_texts = [
    <span class="st">"Company beats earnings expectations with strong revenue"</span>,
    <span class="st">"Stock drops after disappointing quarterly results"</span>,
]
X_new = vec.<span class="fn">transform</span>(new_texts)
predictions = clf.<span class="fn">predict</span>(X_new)
probas = clf.<span class="fn">predict_proba</span>(X_new)

<span class="kw">for</span> text, pred, prob <span class="kw">in</span> <span class="nb">zip</span>(new_texts, predictions, probas):
    sentiment = <span class="st">"ê¸ì • ğŸ“ˆ"</span> <span class="kw">if</span> pred == <span class="nu">1</span> <span class="kw">else</span> <span class="st">"ë¶€ì • ğŸ“‰"</span>
    <span class="fn">print</span>(<span class="st">f"[{sentiment}] (í™•ë¥ : {prob[pred]:.2%}) {text}"</span>)

<span class="cm"># í”¼ì²˜ ì¤‘ìš”ë„ (R4 ì—°ê²°: ì–´ë–¤ ë‹¨ì–´ê°€ ê°ì„±ì„ ê²°ì •í•˜ëŠ”ê°€?)</span>
coefs = pd.<span class="fn">Series</span>(clf.coef_[<span class="nu">0</span>], index=vec.<span class="fn">get_feature_names_out</span>())
<span class="fn">print</span>(<span class="st">"\n=== ê¸ì • ë‹¨ì–´ Top 5 ==="</span>)
<span class="fn">print</span>(coefs.<span class="fn">nlargest</span>(<span class="nu">5</span>))
<span class="fn">print</span>(<span class="st">"\n=== ë¶€ì • ë‹¨ì–´ Top 5 ==="</span>)
<span class="fn">print</span>(coefs.<span class="fn">nsmallest</span>(<span class="nu">5</span>))</code></pre>

<div class="ok">
<p class="ni"><strong>BoW â†’ TF-IDFë¡œì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜</strong></p>
<p class="ni" style="margin-top:8px">
ìœ„ ì½”ë“œì—ì„œ CountVectorizerë¥¼ TfidfVectorizerë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ TF-IDF ê¸°ë°˜ ë¶„ë¥˜ê°€ ëœë‹¤. 
ì½”ë“œ í•œ ì¤„ì˜ ì°¨ì´ì§€ë§Œ, ì„±ëŠ¥ì€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì±•í„°ì—ì„œ ê·¸ ì´ìœ ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ íŒŒí—¤ì¹œë‹¤.
</p>
</div>

<!-- ==================== Ch.5 ==================== -->
<h2 id="ch5">Chapter 5. TF-IDF â€” ë‹¨ì–´ì˜ ì¤‘ìš”ë„ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ë‹¤</h2>

<h3>5.1 TF-IDFì˜ ì§ê´€</h3>

<p>
TF-IDF(Term Frequencyâ€“Inverse Document Frequency)ëŠ” BoWì˜ í•µì‹¬ í•œê³„ì¸ "ê³ ë¹ˆë„ ë‹¨ì–´ ì§€ë°°" ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤: íŠ¹ì • ë¬¸ì„œì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ(TF ë†’ìŒ), ì „ì²´ ì½”í¼ìŠ¤ì—ì„œëŠ” ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´(IDF ë†’ìŒ)ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤.
</p>

<p>
ë¹„ìœ ë¥¼ ë“¤ì–´ë³´ì. ëª¨ë“  ê¸ˆìœµ ë‰´ìŠ¤ì— "market"ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë“±ì¥í•œë‹¤ë©´, ì´ ë‹¨ì–´ëŠ” íŠ¹ì • ë‰´ìŠ¤ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ "quantitative easing"ì´ë¼ëŠ” ë‹¨ì–´ê°€ íŠ¹ì • ë‰´ìŠ¤ì—ë§Œ ë“±ì¥í•œë‹¤ë©´, ì´ ë‹¨ì–´ëŠ” ê·¸ ë‰´ìŠ¤ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. TF-IDFëŠ” ì´ëŸ° ì§ê´€ì„ ìˆ˜í•™ì ìœ¼ë¡œ êµ¬í˜„í•œë‹¤.
</p>

<h3>5.2 ìˆ˜í•™ì  ì •ì˜</h3>

<p>
TF-IDFëŠ” ë‘ ê°€ì§€ ìš”ì†Œì˜ ê³±ì´ë‹¤:
</p>

<p class="ni"><strong>Term Frequency (TF):</strong> ë‹¨ì–´ \(t\)ê°€ ë¬¸ì„œ \(d\)ì— ë“±ì¥í•˜ëŠ” ë¹ˆë„</p>
<div class="eq">
$$\text{TF}(t, d) = \frac{\text{count}(t, d)}{\sum_{t' \in d} \text{count}(t', d)}$$
</div>

<p class="ni"><strong>Inverse Document Frequency (IDF):</strong> ë‹¨ì–´ \(t\)ê°€ ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ì–¼ë§ˆë‚˜ ë“œë¬¸ì§€</p>
<div class="eq">
$$\text{IDF}(t) = \log\left(\frac{N}{\text{df}(t)}\right) + 1$$
</div>

<p>
ì—¬ê¸°ì„œ \(N\)ì€ ì „ì²´ ë¬¸ì„œ ìˆ˜, \(\text{df}(t)\)ëŠ” ë‹¨ì–´ \(t\)ê°€ ë“±ì¥í•˜ëŠ” ë¬¸ì„œ ìˆ˜ë‹¤. ëª¨ë“  ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” \(\text{IDF} = \log(1) + 1 = 1\)ë¡œ ê°€ì¤‘ì¹˜ê°€ ë‚®ê³ , í•˜ë‚˜ì˜ ë¬¸ì„œì—ë§Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” \(\text{IDF} = \log(N) + 1\)ë¡œ ê°€ì¤‘ì¹˜ê°€ ë†’ë‹¤.
</p>

<p class="ni"><strong>TF-IDF ìµœì¢… ê°€ì¤‘ì¹˜:</strong></p>
<div class="eq">
$$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)$$
</div>

<div class="warn">
<p class="ni"><strong>âš ï¸ sklearnì˜ ìŠ¤ë¬´ë”© (MLAT Ch.14)</strong></p>
<p class="ni" style="margin-top:8px">sklearnì˜ <code>TfidfVectorizer</code>ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¤ë¬´ë”©ì„ ì ìš©í•œë‹¤. <code>smooth_idf=True</code>ì´ë©´ ë¶„ëª¨ì— 1ì„ ë”í•´ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤: \(\text{IDF}(t) = \log\left(\frac{1 + N}{1 + \text{df}(t)}\right) + 1\). ë˜í•œ <code>sublinear_tf=True</code>ë¡œ ì„¤ì •í•˜ë©´ TFì— ë¡œê·¸ë¥¼ ì·¨í•œë‹¤: \(\text{TF} = 1 + \log(\text{count})\). ì´ëŠ” í•œ ë‹¨ì–´ê°€ 100ë²ˆ ë“±ì¥í•˜ëŠ” ê²ƒì´ 1ë²ˆ ë“±ì¥í•˜ëŠ” ê²ƒë³´ë‹¤ 100ë°° ì¤‘ìš”í•˜ì§€ëŠ” ì•Šë‹¤ëŠ” ì§ê´€ì„ ë°˜ì˜í•œë‹¤.</p>
</div>

<h3>5.3 TF-IDF ì‹¤ì „ êµ¬í˜„</h3>

<div class="cc">ì½”ë“œ 5-1. TfidfVectorizerë¡œ ê¸ˆìœµ ë‰´ìŠ¤ ë²¡í„°í™”</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ê¸ˆìœµ ë‰´ìŠ¤ ì½”í¼ìŠ¤</span>
corpus = [
    <span class="st">"Apple reported record quarterly earnings beating analyst expectations"</span>,
    <span class="st">"Tesla shares dropped after disappointing quarterly earnings report"</span>,
    <span class="st">"Federal Reserve raised interest rates citing persistent inflation"</span>,
    <span class="st">"Goldman Sachs upgraded Apple stock to buy rating after strong earnings"</span>,
    <span class="st">"Oil prices surged amid supply disruptions in Middle East"</span>,
    <span class="st">"Apple stock rose sharply following better than expected earnings"</span>,
    <span class="st">"Federal Reserve signaled potential rate cuts in upcoming meetings"</span>,
    <span class="st">"Inflation data came in higher than expected raising rate hike fears"</span>
]

<span class="cm"># TF-IDF ë²¡í„°í™”</span>
tfidf = <span class="fn">TfidfVectorizer</span>(
    stop_words=<span class="st">'english'</span>,
    ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>),
    max_features=<span class="nu">50</span>,
    sublinear_tf=<span class="kw">True</span>    <span class="cm"># TFì— ë¡œê·¸ ì ìš©</span>
)
X = tfidf.<span class="fn">fit_transform</span>(corpus)

<span class="cm"># ê° ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ (TF-IDF ê°€ì¤‘ì¹˜ ê¸°ì¤€)</span>
feature_names = tfidf.<span class="fn">get_feature_names_out</span>()
<span class="kw">for</span> i, doc <span class="kw">in</span> <span class="fn">enumerate</span>(corpus):
    row = X[i].<span class="fn">toarray</span>().<span class="fn">flatten</span>()
    top_idx = row.<span class="fn">argsort</span>()[-<span class="nu">3</span>:][::-<span class="nu">1</span>]
    top_terms = [(feature_names[j], <span class="fn">round</span>(row[j], <span class="nu">3</span>)) <span class="kw">for</span> j <span class="kw">in</span> top_idx]
    <span class="fn">print</span>(<span class="st">f"Doc {i+1}: {top_terms}"</span>)
</code></pre>

<h3>5.4 ë¬¸ì„œ ìœ ì‚¬ë„: ì½”ì‚¬ì¸ ìœ ì‚¬ë„</h3>

<p>
TF-IDF ë²¡í„°ë¥¼ ì–»ìœ¼ë©´, ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë°©ë²•ì´ <strong>ì½”ì‚¬ì¸ ìœ ì‚¬ë„(Cosine Similarity)</strong>ë‹¤. ë‘ ë²¡í„° ì‚¬ì´ì˜ ê°ë„ì˜ ì½”ì‚¬ì¸ ê°’ì„ ê³„ì‚°í•œë‹¤.
</p>

<div class="eq">
$$\text{cosine}(\mathbf{d}_1, \mathbf{d}_2) = \frac{\mathbf{d}_1 \cdot \mathbf{d}_2}{\|\mathbf{d}_1\| \|\mathbf{d}_2\|}$$
</div>

<p>
ê°’ì´ 1ì— ê°€ê¹Œìš°ë©´ ë‘ ë¬¸ì„œê°€ ìœ ì‚¬í•˜ê³ , 0ì— ê°€ê¹Œìš°ë©´ ê´€ë ¨ì´ ì—†ë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë²¡í„°ì˜ í¬ê¸°(ë¬¸ì„œ ê¸¸ì´)ì— ì˜í–¥ì„ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ, ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ì„œë¥¼ ë¹„êµí•  ë•Œ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë³´ë‹¤ ì í•©í•˜ë‹¤.
</p>

<div class="cc">ì½”ë“œ 5-2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ ì°¾ê¸°</div>
<pre><code><span class="kw">from</span> sklearn.metrics.pairwise <span class="kw">import</span> cosine_similarity

<span class="cm"># ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬</span>
sim_matrix = <span class="fn">cosine_similarity</span>(X)

<span class="cm"># íˆíŠ¸ë§µ ì‹œê°í™”</span>
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

fig, ax = plt.<span class="fn">subplots</span>(figsize=(<span class="nu">8</span>, <span class="nu">6</span>))
im = ax.<span class="fn">imshow</span>(sim_matrix, cmap=<span class="st">'YlOrRd'</span>, vmin=<span class="nu">0</span>, vmax=<span class="nu">1</span>)
ax.<span class="fn">set_xticks</span>(<span class="fn">range</span>(<span class="fn">len</span>(corpus)))
ax.<span class="fn">set_yticks</span>(<span class="fn">range</span>(<span class="fn">len</span>(corpus)))
ax.<span class="fn">set_xticklabels</span>([<span class="st">f'D{i+1}'</span> <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus))], rotation=<span class="nu">45</span>)
ax.<span class="fn">set_yticklabels</span>([<span class="st">f'D{i+1}'</span> <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus))])

<span class="cm"># ê° ì…€ì— ê°’ í‘œì‹œ</span>
<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus)):
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus)):
        ax.<span class="fn">text</span>(j, i, <span class="st">f'{sim_matrix[i,j]:.2f}'</span>, ha=<span class="st">'center'</span>, va=<span class="st">'center'</span>, fontsize=<span class="nu">9</span>)

plt.<span class="fn">colorbar</span>(im)
plt.<span class="fn">title</span>(<span class="st">'Document Similarity (Cosine)'</span>)
plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</code></pre>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.14ì—ì„œ Jansenì€ <code>scipy.spatial.distance.pdist()</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ BBC ë‰´ìŠ¤ ê¸°ì‚¬ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒì„ ì°¾ëŠ”ë‹¤. ê¸ˆìœµì—ì„œ ì´ ê¸°ë²•ì€ "ìœ ì‚¬í•œ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§", "ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°", "ê´€ë ¨ ì¢…ëª© ë°œê²¬" ë“±ì— í™œìš©ëœë‹¤.</p>
</div>


<!-- ==================== Ch.6 ==================== -->
<h2 id="ch6">Chapter 6. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ â€” í…ìŠ¤íŠ¸ ë¶„ë¥˜ì˜ ê³ ì „ì  ë¬´ê¸°</h2>

<h3>6.1 ë² ì´ì¦ˆ ì •ë¦¬ ë³µìŠµ (R2 ì—°ê²°)</h3>

<p>
R2ì—ì„œ ìš°ë¦¬ëŠ” ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ë°°ì› ë‹¤. ì‚¬ì „ í™•ë¥ (prior)ê³¼ ìš°ë„(likelihood)ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬í›„ í™•ë¥ (posterior)ì„ ê³„ì‚°í•˜ëŠ” ê³µì‹ì´ì—ˆë‹¤. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ê¸°ëŠ” ì´ ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ì§ì ‘ ì ìš©í•œë‹¤.
</p>

<div class="eq">
$$P(\text{class} | \text{document}) = \frac{P(\text{document} | \text{class}) \times P(\text{class})}{P(\text{document})}$$
</div>

<p>
ê¸ˆìœµ ê°ì„±ë¶„ì„ì— ì ìš©í•˜ë©´: "ì´ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ê²ƒì´ ê¸ì •ì (positive)ì¼ í™•ë¥ ì€ ì–¼ë§ˆì¸ê°€?"ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.
</p>

<h3>6.2 "ë‚˜ì´ë¸Œ"í•œ ê°€ì •: ì¡°ê±´ë¶€ ë…ë¦½</h3>

<p>
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆê°€ "ë‚˜ì´ë¸Œ(ìˆœì§„í•œ)"ë¼ê³  ë¶ˆë¦¬ëŠ” ì´ìœ ëŠ” í•µì‹¬ ê°€ì • ë•Œë¬¸ì´ë‹¤: <strong>ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œ ë…ë¦½ì </strong>ì´ë¼ê³  ê°€ì •í•œë‹¤. ì¦‰, "earnings"ê°€ ë“±ì¥í•  í™•ë¥ ì€ "strong"ì´ ë“±ì¥í–ˆëŠ”ì§€ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ë‹¤ê³  ê°€ì •í•œë‹¤. í˜„ì‹¤ì—ì„œ ì´ ê°€ì •ì€ ê±°ì˜ í•­ìƒ í‹€ë¦¬ë‹¤ â€” "strong earnings"ëŠ” í•¨ê»˜ ë“±ì¥í•  í™•ë¥ ì´ ë†’ë‹¤. í•˜ì§€ë§Œ ë†€ëê²Œë„, ì´ ìˆœì§„í•œ ê°€ì •ì—ë„ ë¶ˆêµ¬í•˜ê³  ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ ë†€ë¼ìš¸ ì •ë„ë¡œ ì˜ ì‘ë™í•œë‹¤.
</p>

<p>
MLAT Ch.14ì—ì„œ Jansenì€ ìŠ¤íŒ¸ í•„í„°ë§ ì˜ˆì‹œë¡œ ì´ë¥¼ ì„¤ëª…í•œë‹¤. "send money now"ë¼ëŠ” ë©”ì‹œì§€ê°€ ìŠ¤íŒ¸ì¼ í™•ë¥ ì„ ê³„ì‚°í•  ë•Œ:
</p>

<div class="eq">
$$P(\text{spam} | \text{send money now}) = \frac{P(\text{send}|\text{spam}) \times P(\text{money}|\text{spam}) \times P(\text{now}|\text{spam}) \times P(\text{spam})}{P(\text{send money now})}$$
</div>

<p>
ê° ë‹¨ì–´ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë…ë¦½ì ìœ¼ë¡œ ê³±í•œë‹¤ â€” ì´ê²ƒì´ "ë‚˜ì´ë¸Œ" ê°€ì •ì´ë‹¤. ë¶„ëª¨ \(P(\text{send money now})\)ëŠ” ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ ë™ì¼í•˜ë¯€ë¡œ, ë¶„ë¥˜ ì‹œì—ëŠ” ë¶„ìë§Œ ë¹„êµí•˜ë©´ ëœë‹¤.
</p>

<h3>6.3 ë‹¤í•­ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ (Multinomial NB)</h3>

<p>
í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ë³€í˜•ì€ <strong>ë‹¤í•­ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Multinomial Naive Bayes)</strong>ë‹¤. ê° ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ë‹¤í•­ ë¶„í¬ë¡œ ëª¨ë¸ë§í•œë‹¤. BoWë‚˜ TF-IDFë¡œ ë§Œë“  DTMì„ ì§ì ‘ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆë‹¤.
</p>

<div class="cc">ì½”ë“œ 6-1. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆë¡œ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ë¥˜</div>
<pre><code><span class="kw">from</span> sklearn.naive_bayes <span class="kw">import</span> MultinomialNB
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report, confusion_matrix
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë°ì´í„° (ì‹¤ì œë¡œëŠ” ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œ í•„ìš”)</span>
texts = [
    <span class="st">"Company reported record profits exceeding expectations"</span>,
    <span class="st">"Strong revenue growth driven by new product launches"</span>,
    <span class="st">"Earnings beat estimates with impressive margin expansion"</span>,
    <span class="st">"Stock upgraded to buy after stellar quarterly results"</span>,
    <span class="st">"Dividend increased reflecting confidence in future growth"</span>,
    <span class="st">"Company missed earnings estimates significantly"</span>,
    <span class="st">"Revenue declined sharply due to weak demand"</span>,
    <span class="st">"Stock downgraded to sell amid deteriorating fundamentals"</span>,
    <span class="st">"Profit warning issued citing supply chain disruptions"</span>,
    <span class="st">"Massive layoffs announced as company restructures operations"</span>,
    <span class="st">"Market remained flat with mixed economic signals"</span>,
    <span class="st">"Trading volume was average with no significant catalysts"</span>,
]
labels = [<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>, <span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>, <span class="nu">2</span>,<span class="nu">2</span>]  <span class="cm"># 1=ê¸ì •, 0=ë¶€ì •, 2=ì¤‘ë¦½</span>
label_names = [<span class="st">'Negative'</span>, <span class="st">'Positive'</span>, <span class="st">'Neutral'</span>]

<span class="cm"># TF-IDF ë²¡í„°í™”</span>
tfidf = <span class="fn">TfidfVectorizer</span>(stop_words=<span class="st">'english'</span>, ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>))
X = tfidf.<span class="fn">fit_transform</span>(texts)

<span class="cm"># ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ í•™ìŠµ</span>
nb = <span class="fn">MultinomialNB</span>(alpha=<span class="nu">1.0</span>)  <span class="cm"># alpha: ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©</span>
nb.<span class="fn">fit</span>(X, labels)

<span class="cm"># ìƒˆë¡œìš´ ë‰´ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡</span>
new_texts = [
    <span class="st">"Company delivered outstanding results with record revenue"</span>,
    <span class="st">"Shares plunged after accounting fraud allegations"</span>,
    <span class="st">"Trading was subdued ahead of Federal Reserve decision"</span>
]
X_new = tfidf.<span class="fn">transform</span>(new_texts)
predictions = nb.<span class="fn">predict</span>(X_new)
probabilities = nb.<span class="fn">predict_proba</span>(X_new)

<span class="kw">for</span> text, pred, prob <span class="kw">in</span> <span class="fn">zip</span>(new_texts, predictions, probabilities):
    <span class="fn">print</span>(<span class="st">f"\nğŸ“° {text}"</span>)
    <span class="fn">print</span>(<span class="st">f"   ì˜ˆì¸¡: {label_names[pred]}"</span>)
    <span class="fn">print</span>(<span class="st">f"   í™•ë¥ : Neg={prob[0]:.3f}, Pos={prob[1]:.3f}, Neu={prob[2]:.3f}"</span>)
</code></pre>

<h3>6.4 ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© (Additive Smoothing)</h3>

<p>
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì—ì„œ í•œ ê°€ì§€ ë¬¸ì œê°€ ìˆë‹¤. í›ˆë ¨ ë°ì´í„°ì— í•œ ë²ˆë„ ë“±ì¥í•˜ì§€ ì•Šì€ ë‹¨ì–´ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ë©´, ê·¸ ë‹¨ì–´ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì´ 0ì´ ë˜ì–´ ì „ì²´ í™•ë¥ ì´ 0ì´ ëœë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ <strong>ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©(Laplace Smoothing)</strong>ì„ ì ìš©í•œë‹¤. ëª¨ë“  ë‹¨ì–´ì˜ ë¹ˆë„ì— \(\alpha\)(ë³´í†µ 1)ë¥¼ ë”í•´ì¤€ë‹¤.
</p>

<div class="eq">
$$P(w_i | c) = \frac{\text{count}(w_i, c) + \alpha}{\sum_{w \in V} [\text{count}(w, c) + \alpha]} = \frac{\text{count}(w_i, c) + \alpha}{\text{count}(c) + \alpha |V|}$$
</div>

<p>
ì—¬ê¸°ì„œ \(|V|\)ëŠ” ì–´íœ˜ í¬ê¸°ë‹¤. \(\alpha = 1\)ì´ë©´ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©, \(\alpha < 1\)ì´ë©´ ë¦¬ë“œìŠ¤í†¤ ìŠ¤ë¬´ë”©ì´ë¼ í•œë‹¤.
</p>

<div class="ok">
<p class="ni"><strong>ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ ì¥ë‹¨ì  ì •ë¦¬</strong></p>
<ul>
<li><strong>ì¥ì :</strong> í•™ìŠµì´ ë§¤ìš° ë¹ ë¥´ë‹¤ (ë‹¨ìˆœ ë¹ˆë„ ê³„ì‚°), ì ì€ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™í•œë‹¤, í™•ë¥  ì¶œë ¥ì„ ì œê³µí•œë‹¤, ê³ ì°¨ì› í¬ì†Œ ë°ì´í„°ì— ê°•í•˜ë‹¤</li>
<li><strong>ë‹¨ì :</strong> ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •ì´ ë¹„í˜„ì‹¤ì ì´ë‹¤, ë‹¨ì–´ ìˆœì„œë¥¼ ë¬´ì‹œí•œë‹¤, í™•ë¥  ì¶”ì •ì´ ë¶€ì •í™•í•  ìˆ˜ ìˆë‹¤ (calibration ë¬¸ì œ)</li>
<li><strong>ê¸ˆìœµ ì ìš©:</strong> ë‰´ìŠ¤ ê°ì„± ë¶„ë¥˜ì˜ ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì í•©í•˜ë‹¤. MLAT Ch.14ì—ì„œ BBC ë‰´ìŠ¤ ë¶„ë¥˜ì— 64.7% ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤.</li>
</ul>
</div>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.14ì—ì„œ Jansenì€ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆë¥¼ ë‘ ê°€ì§€ ë°ì´í„°ì…‹ì— ì ìš©í•œë‹¤. (1) BBC ë‰´ìŠ¤ 2,225ê°œ ê¸°ì‚¬ë¥¼ 5ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ â€” 97.7% ì •í™•ë„. (2) Yelp ë¦¬ë·° 600ë§Œ ê°œë¥¼ 1~5ì ìœ¼ë¡œ ë¶„ë¥˜ â€” 64.7% ì •í™•ë„. ì´í›„ LightGBMê³¼ ë¹„êµí•˜ì—¬ 73.6% ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ”ë°, ì´ëŠ” ì•™ìƒë¸” ëª¨ë¸ì˜ ìš°ìœ„ë¥¼ ë³´ì—¬ì¤€ë‹¤.</p>
</div>


<h3>6.5 ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ ë³€í˜•ë“¤</h3>

<p>
sklearnì€ ì„¸ ê°€ì§€ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë³€í˜•ì„ ì œê³µí•œë‹¤. ê°ê° ë‹¤ë¥¸ ë°ì´í„° ë¶„í¬ë¥¼ ê°€ì •í•œë‹¤.
</p>

<div class="tc">í‘œ 6-1. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë³€í˜• ë¹„êµ</div>
<table>
<thead>
<tr><th>ë³€í˜•</th><th>ë¶„í¬ ê°€ì •</th><th>ì…ë ¥ ë°ì´í„°</th><th>ê¸ˆìœµ NLP ì ìš©</th></tr>
</thead>
<tbody>
<tr><td><strong>MultinomialNB</strong></td><td>ë‹¤í•­ ë¶„í¬</td><td>ë‹¨ì–´ ë¹ˆë„ (ì •ìˆ˜, â‰¥0)</td><td>BoW, TF-IDF ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ â†’ <strong>ê°€ì¥ ì¼ë°˜ì </strong></td></tr>
<tr><td><strong>BernoulliNB</strong></td><td>ë² ë¥´ëˆ„ì´ ë¶„í¬</td><td>ì´ì§„ (0/1)</td><td>ë‹¨ì–´ ì¶œí˜„ ì—¬ë¶€ë§Œ ì‚¬ìš©. ì§§ì€ í…ìŠ¤íŠ¸(íŠ¸ìœ—, í—¤ë“œë¼ì¸)ì— ì í•©</td></tr>
<tr><td><strong>GaussianNB</strong></td><td>ê°€ìš°ì‹œì•ˆ ë¶„í¬</td><td>ì—°ì†ê°’ (ì‹¤ìˆ˜)</td><td>Word2Vec/Doc2Vec ë°€ì§‘ ë²¡í„° ì…ë ¥ ì‹œ ì‚¬ìš©</td></tr>
</tbody>
</table>

<div class="cc">ì½”ë“œ 6-2. 3ê°€ì§€ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë³€í˜• ë¹„êµ ì‹¤í—˜</div>
<pre><code><span class="kw">from</span> sklearn.naive_bayes <span class="kw">import</span> MultinomialNB, BernoulliNB, GaussianNB
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer, TfidfVectorizer
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> cross_val_score
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ê¸ˆìœµ ë‰´ìŠ¤ ì½”í¼ìŠ¤ (ê¸ì •/ë¶€ì • ë¼ë²¨)</span>
corpus = [
    <span class="st">"record revenue strong earnings beat expectations growth"</span>,
    <span class="st">"upgrade buy rating robust demand momentum"</span>,
    <span class="st">"buyback dividend increase shareholder value"</span>,
    <span class="st">"innovation breakthrough market leader expansion"</span>,
    <span class="st">"decline loss miss disappointing weak guidance"</span>,
    <span class="st">"downgrade sell warning risk regulatory pressure"</span>,
    <span class="st">"lawsuit antitrust investigation penalty fine"</span>,
    <span class="st">"slowdown recession layoffs restructuring costs"</span>,
]
labels = np.<span class="fn">array</span>([<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>])

<span class="cm"># ë²¡í„°í™”</span>
count_vec = <span class="fn">CountVectorizer</span>()
tfidf_vec = <span class="fn">TfidfVectorizer</span>()
X_count = count_vec.<span class="fn">fit_transform</span>(corpus)
X_tfidf = tfidf_vec.<span class="fn">fit_transform</span>(corpus)

<span class="cm"># 3ê°€ì§€ NB ë¹„êµ</span>
experiments = [
    (<span class="st">"MultinomialNB + BoW"</span>, <span class="fn">MultinomialNB</span>(), X_count),
    (<span class="st">"MultinomialNB + TF-IDF"</span>, <span class="fn">MultinomialNB</span>(), X_tfidf),
    (<span class="st">"BernoulliNB + BoW"</span>, <span class="fn">BernoulliNB</span>(), X_count),
    (<span class="st">"GaussianNB + TF-IDF"</span>, <span class="fn">GaussianNB</span>(), X_tfidf.<span class="fn">toarray</span>()),
]

<span class="kw">for</span> name, model, X <span class="kw">in</span> experiments:
    scores = <span class="fn">cross_val_score</span>(model, X, labels, cv=<span class="nu">3</span>, scoring=<span class="st">'accuracy'</span>)
    <span class="fn">print</span>(<span class="st">f"{name:30s}: {scores.mean():.3f} Â± {scores.std():.3f}"</span>)</code></pre>

<h3>6.6 ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ vs ë‹¤ë¥¸ ë¶„ë¥˜ê¸°: ì–¸ì œ NBë¥¼ ì“¸ ê²ƒì¸ê°€</h3>

<p>
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ” "ë‚˜ì´ë¸Œ"í•œ ê°€ì •ì—ë„ ë¶ˆêµ¬í•˜ê³  í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ ë†€ë¼ìš¸ ì •ë„ë¡œ ì˜ ì‘ë™í•œë‹¤. 
í•˜ì§€ë§Œ ëª¨ë“  ìƒí™©ì—ì„œ ìµœì„ ì€ ì•„ë‹ˆë‹¤. R4ì—ì„œ ë°°ìš´ ë‹¤ë¥¸ ë¶„ë¥˜ê¸°ë“¤ê³¼ ë¹„êµí•´ë³´ì.
</p>

<!-- NB vs ë‹¤ë¥¸ ë¶„ë¥˜ê¸° ë¹„êµ -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:15px;font-size:14px;color:#2c3e50">í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ì„ íƒ ê°€ì´ë“œ</p>
<div style="display:flex;gap:12px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:200px;background:#fff;padding:14px;border-radius:8px;border-left:4px solid #4caf50">
<p class="ni" style="font-weight:bold;color:#2e7d32;font-size:12px">âœ… NBê°€ ì¢‹ì€ ê²½ìš°</p>
<ul style="font-size:11px;color:#555;margin:6px 0 0;padding-left:16px;line-height:1.6">
<li>í•™ìŠµ ë°ì´í„°ê°€ ì ì„ ë•Œ (ìˆ˜ë°± ê°œ)</li>
<li>í”¼ì²˜ ìˆ˜ê°€ ë§¤ìš° ë§ì„ ë•Œ (ê³ ì°¨ì›)</li>
<li>ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì´ í•„ìš”í•  ë•Œ</li>
<li>ì‹¤ì‹œê°„ ë¶„ë¥˜ê°€ í•„ìš”í•  ë•Œ</li>
</ul>
</div>
<div style="flex:1;min-width:200px;background:#fff;padding:14px;border-radius:8px;border-left:4px solid #ff9800">
<p class="ni" style="font-weight:bold;color:#e65100;font-size:12px">âš¡ LightGBMì´ ì¢‹ì€ ê²½ìš°</p>
<ul style="font-size:11px;color:#555;margin:6px 0 0;padding-left:16px;line-height:1.6">
<li>í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ (ìˆ˜ë§Œ ê°œ+)</li>
<li>í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš©ì´ ì¤‘ìš”í•  ë•Œ</li>
<li>ìµœê³  ì„±ëŠ¥ì´ í•„ìš”í•  ë•Œ</li>
<li>í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ì´ í•„ìš”í•  ë•Œ</li>
</ul>
</div>
<div style="flex:1;min-width:200px;background:#fff;padding:14px;border-radius:8px;border-left:4px solid #9c27b0">
<p class="ni" style="font-weight:bold;color:#7b1fa2;font-size:12px">ğŸ¤– FinBERTê°€ ì¢‹ì€ ê²½ìš°</p>
<ul style="font-size:11px;color:#555;margin:6px 0 0;padding-left:16px;line-height:1.6">
<li>ë¬¸ë§¥ ì´í•´ê°€ ì¤‘ìš”í•  ë•Œ</li>
<li>ë¶€ì •, ë¹„êµ, ì¡°ê±´ë¬¸ì´ ë§ì„ ë•Œ</li>
<li>GPU ìì›ì´ ìˆì„ ë•Œ</li>
<li>ìµœê³  ì •í™•ë„ê°€ í•„ìš”í•  ë•Œ</li>
</ul>
</div>
</div>
</div>

<!-- ==================== Ch.7 ==================== -->
<h2 id="ch7">Chapter 7. ê°ì„±ë¶„ì„ ì‹¤ì „ â€” ê¸ˆìœµ ë‰´ìŠ¤ì—ì„œ ì‹œê·¸ë„ ì¶”ì¶œ</h2>

<h3>7.1 ê°ì„±ë¶„ì„ì˜ ë‘ ê°€ì§€ ì ‘ê·¼ë²•</h3>

<p>
ê°ì„±ë¶„ì„(Sentiment Analysis)ì€ í…ìŠ¤íŠ¸ì—ì„œ ê°ì„±(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ìë™ìœ¼ë¡œ íŒë³„í•˜ëŠ” ê¸°ë²•ì´ë‹¤. í¬ê²Œ ë‘ ê°€ì§€ ì ‘ê·¼ë²•ì´ ìˆë‹¤:
</p>

<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:linear-gradient(135deg,#e3f2fd,#bbdefb);padding:20px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#1565c0;margin-bottom:10px">ğŸ“– ì‚¬ì „ ê¸°ë°˜ (Lexicon-Based)</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">ğŸ“‹ ë¯¸ë¦¬ ì •ì˜ëœ ê°ì„± ì‚¬ì „ ì‚¬ìš©</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">âš¡ í•™ìŠµ ë°ì´í„° ë¶ˆí•„ìš”, ì¦‰ì‹œ ì ìš© ê°€ëŠ¥</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">ğŸ“Š ë„êµ¬: VADER, TextBlob, Loughran-McDonald</p>
<p class="ni" style="font-size:11px;color:#555;margin-top:10px;border-top:1px solid #90caf9;padding-top:8px">í•œê³„: ë¬¸ë§¥ ë¬´ì‹œ, ë„ë©”ì¸ íŠ¹ìˆ˜ì„± ë¶€ì¡±</p>
</div>
<div style="flex:1;min-width:280px;background:linear-gradient(135deg,#f3e5f5,#e1bee7);padding:20px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#6a1b9a;margin-bottom:10px">ğŸ¤– ML ê¸°ë°˜ (Machine Learning)</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">ğŸ“‹ ë¼ë²¨ë§ëœ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">âš¡ ë†’ì€ ì •í™•ë„, ë„ë©”ì¸ ì ì‘ ê°€ëŠ¥</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">ğŸ“Š ë„êµ¬: NB, LightGBM, BERT, FinBERT</p>
<p class="ni" style="font-size:11px;color:#555;margin-top:10px;border-top:1px solid #ce93d8;padding-top:8px">í•œê³„: ëŒ€ëŸ‰ ë¼ë²¨ ë°ì´í„° í•„ìš”, í•™ìŠµ ì‹œê°„</p>
</div>
</div>

<h3>7.2 ê¸ˆìœµ ê°ì„± ì‚¬ì „: Loughran-McDonald</h3>

<p>
ì¼ë°˜ì ì¸ ê°ì„± ì‚¬ì „(VADER, TextBlob)ì€ ê¸ˆìœµ í…ìŠ¤íŠ¸ì— ì í•©í•˜ì§€ ì•Šë‹¤. "liability"ëŠ” ì¼ë°˜ ì˜ì–´ì—ì„œ ë¶€ì •ì ì´ì§€ë§Œ, ê¸ˆìœµì—ì„œëŠ” ì¤‘ë¦½ì ì¸ íšŒê³„ ìš©ì–´ë‹¤. "outstanding"ì€ ì¼ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì´ì§€ë§Œ, "outstanding shares"ì—ì„œëŠ” ì¤‘ë¦½ì ì´ë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Loughranê³¼ McDonald(2011)ê°€ ê¸ˆìœµ ì „ìš© ê°ì„± ì‚¬ì „ì„ ê°œë°œí–ˆë‹¤.
</p>

<div class="tc">í‘œ 7-1. ì¼ë°˜ ê°ì„± ì‚¬ì „ vs ê¸ˆìœµ ê°ì„± ì‚¬ì „</div>
<table>
<thead>
<tr><th>ë‹¨ì–´</th><th>ì¼ë°˜ ê°ì„± (VADER)</th><th>ê¸ˆìœµ ê°ì„± (L-M)</th><th>ì´ìœ </th></tr>
</thead>
<tbody>
<tr><td>liability</td><td style="color:#e74c3c">ë¶€ì • âŒ</td><td>ì¤‘ë¦½ âœ…</td><td>íšŒê³„ ìš©ì–´</td></tr>
<tr><td>outstanding</td><td style="color:#27ae60">ê¸ì • âŒ</td><td>ì¤‘ë¦½ âœ…</td><td>"outstanding shares"</td></tr>
<tr><td>capital</td><td>ì¤‘ë¦½</td><td>ì¤‘ë¦½ âœ…</td><td>ê¸ˆìœµ ê¸°ë³¸ ìš©ì–´</td></tr>
<tr><td>risk</td><td style="color:#e74c3c">ë¶€ì • âŒ</td><td>ë¶ˆí™•ì‹¤ âœ…</td><td>ë¦¬ìŠ¤í¬ â‰  ë¶€ì •</td></tr>
<tr><td>tax</td><td style="color:#e74c3c">ë¶€ì • âŒ</td><td>ì¤‘ë¦½ âœ…</td><td>íšŒê³„ ìš©ì–´</td></tr>
<tr><td>decline</td><td style="color:#e74c3c">ë¶€ì •</td><td style="color:#e74c3c">ë¶€ì • âœ…</td><td>ì‹¤ì  í•˜ë½</td></tr>
</tbody>
</table>

<div class="cc">ì½”ë“œ 7-1. Loughran-McDonald ê°ì„± ì‚¬ì „ í™œìš©</div>
<pre><code><span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> re

<span class="cm"># Loughran-McDonald ê°ì„± ì‚¬ì „ ë¡œë“œ</span>
<span class="cm"># ë‹¤ìš´ë¡œë“œ: https://sraf.nd.edu/loughranmcdonald-master-dictionary/</span>
lm_dict = pd.<span class="fn">read_csv</span>(<span class="st">'LoughranMcDonald_MasterDictionary_2020.csv'</span>)

<span class="cm"># ê°ì„± ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ ëª©ë¡ ì¶”ì¶œ</span>
positive_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Positive'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())
negative_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Negative'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())
uncertain_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Uncertainty'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())
litigious_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Litigious'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())

<span class="fn">print</span>(<span class="st">f"ê¸ì • ë‹¨ì–´: {len(positive_words)}ê°œ (ì˜ˆ: {list(positive_words)[:5]})"</span>)
<span class="fn">print</span>(<span class="st">f"ë¶€ì • ë‹¨ì–´: {len(negative_words)}ê°œ (ì˜ˆ: {list(negative_words)[:5]})"</span>)
<span class="fn">print</span>(<span class="st">f"ë¶ˆí™•ì‹¤ ë‹¨ì–´: {len(uncertain_words)}ê°œ"</span>)
<span class="fn">print</span>(<span class="st">f"ì†Œì†¡ ê´€ë ¨: {len(litigious_words)}ê°œ"</span>)

<span class="cm"># ê°ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜</span>
<span class="kw">def</span> <span class="fn">lm_sentiment_score</span>(text):
    <span class="st">"""Loughran-McDonald ê°ì„± ì ìˆ˜ ê³„ì‚°"""</span>
    words = re.<span class="fn">findall</span>(<span class="st">r'\b[a-z]+\b'</span>, text.<span class="fn">lower</span>())
    n_words = <span class="fn">len</span>(words)
    <span class="kw">if</span> n_words == <span class="nu">0</span>:
        <span class="kw">return</span> <span class="nu">0</span>
    
    pos_count = <span class="fn">sum</span>(<span class="nu">1</span> <span class="kw">for</span> w <span class="kw">in</span> words <span class="kw">if</span> w <span class="kw">in</span> positive_words)
    neg_count = <span class="fn">sum</span>(<span class="nu">1</span> <span class="kw">for</span> w <span class="kw">in</span> words <span class="kw">if</span> w <span class="kw">in</span> negative_words)
    
    <span class="cm"># ê°ì„± ì ìˆ˜: (ê¸ì • - ë¶€ì •) / ì „ì²´ ë‹¨ì–´ ìˆ˜</span>
    <span class="kw">return</span> (pos_count - neg_count) / n_words

<span class="cm"># í…ŒìŠ¤íŠ¸</span>
test_texts = [
    <span class="st">"Company achieved strong growth with improved profitability"</span>,
    <span class="st">"Significant losses due to impairment charges and restructuring"</span>,
    <span class="st">"Revenue remained stable with moderate capital expenditures"</span>
]

<span class="kw">for</span> text <span class="kw">in</span> test_texts:
    score = <span class="fn">lm_sentiment_score</span>(text)
    sentiment = <span class="st">"ê¸ì •"</span> <span class="kw">if</span> score > <span class="nu">0</span> <span class="kw">else</span> (<span class="st">"ë¶€ì •"</span> <span class="kw">if</span> score < <span class="nu">0</span> <span class="kw">else</span> <span class="st">"ì¤‘ë¦½"</span>)
    <span class="fn">print</span>(<span class="st">f"[{sentiment}] score={score:+.3f} | {text}"</span>)
</code></pre>

<h3>7.3 LightGBMìœ¼ë¡œ ê°ì„± ë¶„ë¥˜ ì„±ëŠ¥ ë†’ì´ê¸°</h3>

<p>
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ” ë¹ ë¥´ê³  ê°„ë‹¨í•˜ì§€ë§Œ, ë” ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•˜ë‹¤ë©´ R4ì—ì„œ ë°°ìš´ ì•™ìƒë¸” ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. MLAT Ch.14ì—ì„œ Jansenì€ Yelp ë¦¬ë·° ë°ì´í„°ì— LightGBMì„ ì ìš©í•˜ì—¬ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(64.7%)ë³´ë‹¤ í¬ê²Œ í–¥ìƒëœ 73.6% ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤.
</p>

<div class="cc">ì½”ë“œ 7-2. TF-IDF + LightGBM ê°ì„± ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸</div>
<pre><code><span class="kw">import</span> lightgbm <span class="kw">as</span> lgb
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> accuracy_score, classification_report

<span class="cm"># 1. ë°ì´í„° ë¡œë“œ (ì˜ˆ: Yelp ë¦¬ë·° ë˜ëŠ” ê¸ˆìœµ ë‰´ìŠ¤)</span>
<span class="cm"># df = pd.read_parquet('financial_news_sentiment.parquet')</span>
<span class="cm"># X_text, y = df['text'], df['sentiment']</span>

<span class="cm"># 2. TF-IDF ë²¡í„°í™”</span>
tfidf = <span class="fn">TfidfVectorizer</span>(
    max_features=<span class="nu">50000</span>,
    ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>),
    sublinear_tf=<span class="kw">True</span>,
    stop_words=<span class="st">'english'</span>
)
X = tfidf.<span class="fn">fit_transform</span>(X_text)

<span class="cm"># 3. Train/Test ë¶„í• </span>
X_train, X_test, y_train, y_test = <span class="fn">train_test_split</span>(
    X, y, test_size=<span class="nu">0.2</span>, random_state=<span class="nu">42</span>, stratify=y
)

<span class="cm"># 4. LightGBM í•™ìŠµ</span>
train_data = lgb.<span class="fn">Dataset</span>(X_train, label=y_train)
test_data = lgb.<span class="fn">Dataset</span>(X_test, label=y_test, reference=train_data)

params = {
    <span class="st">'objective'</span>: <span class="st">'multiclass'</span>,
    <span class="st">'num_classes'</span>: <span class="nu">3</span>,
    <span class="st">'metric'</span>: <span class="st">'multi_logloss'</span>,
    <span class="st">'learning_rate'</span>: <span class="nu">0.1</span>,
    <span class="st">'num_leaves'</span>: <span class="nu">63</span>,
    <span class="st">'verbose'</span>: -<span class="nu">1</span>
}

model = lgb.<span class="fn">train</span>(
    params,
    train_data,
    num_boost_round=<span class="nu">1000</span>,
    valid_sets=[test_data],
    callbacks=[lgb.<span class="fn">early_stopping</span>(<span class="nu">25</span>), lgb.<span class="fn">log_evaluation</span>(<span class="nu">50</span>)]
)

<span class="cm"># 5. ì˜ˆì¸¡ ë° í‰ê°€</span>
y_pred = model.<span class="fn">predict</span>(X_test).<span class="fn">argmax</span>(axis=<span class="nu">1</span>)
<span class="fn">print</span>(<span class="fn">classification_report</span>(y_test, y_pred, target_names=[<span class="st">'Negative'</span>,<span class="st">'Positive'</span>,<span class="st">'Neutral'</span>]))
</code></pre>

<div class="ok">
<p class="ni"><strong>í•µì‹¬ í¬ì¸íŠ¸:</strong> TF-IDF + LightGBM ì¡°í•©ì€ í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„ì˜ ê°•ë ¥í•œ ë² ì´ìŠ¤ë¼ì¸ì´ë‹¤. MLATì—ì„œ Yelp 600ë§Œ ë¦¬ë·°ì— ëŒ€í•´ 73.6% ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤. ì´ íŒŒì´í”„ë¼ì¸ì€ (1) ë¹ ë¥¸ í•™ìŠµ, (2) í”¼ì²˜ ì¤‘ìš”ë„ í•´ì„ ê°€ëŠ¥, (3) ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥ì´ë¼ëŠ” ì¥ì ì´ ìˆë‹¤. R4ì—ì„œ ë°°ìš´ XGBoost/LightGBMì˜ í…ìŠ¤íŠ¸ ë°ì´í„° ì ìš© ë²„ì „ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.</p>
</div>


<!-- ==================== Ch.8 ==================== -->
<h2 id="ch8">Chapter 8. í† í”½ ëª¨ë¸ë§ â€” ë¬¸ì„œì˜ ìˆ¨ê²¨ì§„ ì£¼ì œë¥¼ ë°œê²¬í•˜ë‹¤</h2>

<h3>8.1 í† í”½ ëª¨ë¸ë§ì´ë€</h3>

<p>
R5ì—ì„œ ìš°ë¦¬ëŠ” PCAë¡œ ìˆ«ì ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ êµ¬ì¡°ë¥¼ ë°œê²¬í–ˆë‹¤. í† í”½ ëª¨ë¸ë§ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ PCAë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ìˆ˜ì²œ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ìˆì„ ë•Œ, ì´ ê¸°ì‚¬ë“¤ì´ ë‹¤ë£¨ëŠ” "ì ì¬ ì£¼ì œ(latent topics)"ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê¸ˆìœµ ë‰´ìŠ¤ ì½”í¼ìŠ¤ì—ì„œ "í†µí™”ì •ì±…", "ê¸°ì—… ì‹¤ì ", "ì—ë„ˆì§€ ì‹œì¥", "ê¸°ìˆ ì£¼" ê°™ì€ í† í”½ì´ ìë™ìœ¼ë¡œ ë°œê²¬ë  ìˆ˜ ìˆë‹¤.
</p>

<p>
MLAT Ch.15ì˜ ì œëª©ì€ "Topic Modeling â€“ Summarizing Financial News"ë‹¤. í•µì‹¬ ë©”ì‹œì§€ëŠ” ëª…í™•í•˜ë‹¤: í† í”½ ëª¨ë¸ë§ìœ¼ë¡œ ëŒ€ëŸ‰ì˜ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìš”ì•½í•˜ê³  ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.
</p>

<h3>8.2 LSIì—ì„œ LDAê¹Œì§€: í† í”½ ëª¨ë¸ì˜ ì§„í™”</h3>

<!-- í† í”½ ëª¨ë¸ ì§„í™” ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:18px;font-size:14px;color:#2c3e50">ğŸ“ˆ í† í”½ ëª¨ë¸ì˜ ì§„í™”</p>
<div style="display:flex;align-items:stretch;gap:12px;flex-wrap:wrap;justify-content:center">

<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #90caf9">
<div style="font-size:11px;color:#1976d2;font-weight:bold;margin-bottom:6px">1990s</div>
<div style="font-weight:bold;font-size:13px;margin-bottom:6px">LSI / LSA</div>
<div style="font-size:11px;color:#666;line-height:1.5">
<strong>ë°©ë²•:</strong> DTMì— SVD ì ìš©<br>
<strong>ì¥ì :</strong> ë™ì˜ì–´ ì²˜ë¦¬ ê°€ëŠ¥<br>
<strong>ë‹¨ì :</strong> í† í”½ í•´ì„ ì–´ë ¤ì›€ (ìŒìˆ˜ ê°’)<br>
<strong>ìˆ˜í•™:</strong> \(DTM \approx U \Sigma V^T\)
</div>
</div>

<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #ce93d8">
<div style="font-size:11px;color:#7b1fa2;font-weight:bold;margin-bottom:6px">1999</div>
<div style="font-weight:bold;font-size:13px;margin-bottom:6px">pLSA / NMF</div>
<div style="font-size:11px;color:#666;line-height:1.5">
<strong>ë°©ë²•:</strong> í™•ë¥ ì  ëª¨ë¸ / ë¹„ìŒìˆ˜ ë¶„í•´<br>
<strong>ì¥ì :</strong> í† í”½ì´ í™•ë¥  ë¶„í¬<br>
<strong>ë‹¨ì :</strong> ìƒˆ ë¬¸ì„œì— ì¼ë°˜í™” ì–´ë ¤ì›€<br>
<strong>ìˆ˜í•™:</strong> \(DTM \approx W H\) (W,H â‰¥ 0)
</div>
</div>

<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #81c784">
<div style="font-size:11px;color:#2e7d32;font-weight:bold;margin-bottom:6px">2003 â­</div>
<div style="font-weight:bold;font-size:13px;margin-bottom:6px">LDA</div>
<div style="font-size:11px;color:#666;line-height:1.5">
<strong>ë°©ë²•:</strong> ë² ì´ì§€ì•ˆ ìƒì„± ëª¨ë¸<br>
<strong>ì¥ì :</strong> í™•ë¥ ì  í•´ì„, ì¼ë°˜í™” ê°€ëŠ¥<br>
<strong>ë‹¨ì :</strong> í† í”½ ìˆ˜ ì‚¬ì „ ì§€ì • í•„ìš”<br>
<strong>ìˆ˜í•™:</strong> ë””ë¦¬í´ë ˆ ë¶„í¬ ê¸°ë°˜
</div>
</div>

</div>
</div>

<h3>8.3 LDA (Latent Dirichlet Allocation) í•µì‹¬ ì›ë¦¬</h3>

<p>
LDAëŠ” í˜„ì¬ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” í† í”½ ëª¨ë¸ì´ë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” "ì—­ê³µí•™(reverse engineering)"ì´ë‹¤. LDAëŠ” ë¬¸ì„œê°€ ìƒì„±ë˜ëŠ” ê³¼ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•œë‹¤:
</p>

<ol>
<li>ê° ë¬¸ì„œëŠ” ì—¬ëŸ¬ í† í”½ì˜ <strong>í˜¼í•©(mixture)</strong>ì´ë‹¤. ì˜ˆ: "ì´ ê¸°ì‚¬ëŠ” 70% í†µí™”ì •ì±… + 30% ì£¼ì‹ì‹œì¥"</li>
<li>ê° í† í”½ì€ ë‹¨ì–´ë“¤ì˜ <strong>í™•ë¥  ë¶„í¬</strong>ë‹¤. ì˜ˆ: "í†µí™”ì •ì±… í† í”½ì—ì„œ 'rate'ê°€ ë‚˜ì˜¬ í™•ë¥  5%, 'inflation'ì´ ë‚˜ì˜¬ í™•ë¥  3%..."</li>
<li>ë¬¸ì„œë¥¼ ì“¸ ë•Œ, ë¨¼ì € í† í”½ì„ ì„ íƒí•˜ê³ , ê·¸ í† í”½ì˜ ë‹¨ì–´ ë¶„í¬ì—ì„œ ë‹¨ì–´ë¥¼ ë½‘ëŠ”ë‹¤.</li>
</ol>

<p>
ë¬¼ë¡  ì‹¤ì œë¡œ ì‚¬ëŒì´ ì´ë ‡ê²Œ ê¸€ì„ ì“°ì§€ëŠ” ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ ì´ "ê°€ìƒì˜ ìƒì„± ê³¼ì •"ì„ ì—­ìœ¼ë¡œ ì¶”ì í•˜ë©´, ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì—ì„œ ì ì¬ í† í”½ì„ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤.
</p>

<!-- LDA ìƒì„± ê³¼ì • ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e8f5e9,#c8e6c9);border-radius:12px;border:1px solid #a5d6a7">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#2e7d32">ğŸ² LDAì˜ ìƒì„± ê³¼ì • (Generative Process)</p>
<div style="display:flex;flex-direction:column;gap:10px;max-width:600px;margin:0 auto;font-size:12px">

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">1</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>í† í”½ ë¶„í¬ ì„ íƒ:</strong> ë””ë¦¬í´ë ˆ ë¶„í¬ì—ì„œ ë¬¸ì„œì˜ í† í”½ ë¹„ìœ¨ Î¸ë¥¼ ë½‘ëŠ”ë‹¤<br>
<span style="color:#888;font-size:11px">ì˜ˆ: Î¸ = [0.7, 0.2, 0.1] â†’ "70% í†µí™”ì •ì±…, 20% ì‹¤ì , 10% ì—ë„ˆì§€"</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">2</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>ë‹¨ì–´ë§ˆë‹¤ í† í”½ í• ë‹¹:</strong> Î¸ì— ë”°ë¼ ê° ë‹¨ì–´ì˜ í† í”½ zë¥¼ ì„ íƒ<br>
<span style="color:#888;font-size:11px">ì˜ˆ: ì²« ë²ˆì§¸ ë‹¨ì–´ â†’ í† í”½ 1(í†µí™”ì •ì±…), ë‘ ë²ˆì§¸ ë‹¨ì–´ â†’ í† í”½ 1, ì„¸ ë²ˆì§¸ â†’ í† í”½ 2(ì‹¤ì )...</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">3</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>ë‹¨ì–´ ìƒì„±:</strong> ì„ íƒëœ í† í”½ì˜ ë‹¨ì–´ ë¶„í¬ Î²ì—ì„œ ì‹¤ì œ ë‹¨ì–´ë¥¼ ë½‘ëŠ”ë‹¤<br>
<span style="color:#888;font-size:11px">ì˜ˆ: í† í”½ 1ì—ì„œ â†’ "rate" (í™•ë¥  5%), "inflation" (í™•ë¥  3%), "Fed" (í™•ë¥  4%)...</span>
</div>
</div>

</div>
<p class="ni" style="text-align:center;margin-top:12px;font-size:11px;color:#555">LDAëŠ” ì´ ê³¼ì •ì„ <strong>ì—­ìœ¼ë¡œ ì¶”ì </strong>í•˜ì—¬, ê´€ì°°ëœ ë¬¸ì„œë“¤ì—ì„œ Î¸(ë¬¸ì„œ-í† í”½ ë¶„í¬)ì™€ Î²(í† í”½-ë‹¨ì–´ ë¶„í¬)ë¥¼ ì¶”ì •í•œë‹¤</p>
</div>

<h3>8.4 ë””ë¦¬í´ë ˆ ë¶„í¬ (Dirichlet Distribution)</h3>

<p>
LDAì˜ "D"ëŠ” ë””ë¦¬í´ë ˆ(Dirichlet)ë¥¼ ì˜ë¯¸í•œë‹¤. ë””ë¦¬í´ë ˆ ë¶„í¬ëŠ” í™•ë¥  ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ë¶„í¬ë‹¤. ì¦‰, í•©ì´ 1ì´ ë˜ëŠ” ì–‘ìˆ˜ ë²¡í„°ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. íŒŒë¼ë¯¸í„° \(\alpha\)ê°€ ì´ ë¶„í¬ì˜ í˜•íƒœë¥¼ ê²°ì •í•œë‹¤.
</p>

<div class="eq">
$$\text{Dir}(\boldsymbol{\theta} | \boldsymbol{\alpha}) = \frac{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma(\alpha_k)} \prod_{k=1}^{K} \theta_k^{\alpha_k - 1}$$
</div>

<ul>
<li>\(\alpha > 1\): í† í”½ì´ ê³ ë¥´ê²Œ ë¶„í¬ (ëª¨ë“  í† í”½ì´ ë¹„ìŠ·í•œ ë¹„ìœ¨)</li>
<li>\(\alpha < 1\): í† í”½ì´ í¬ì†Œí•˜ê²Œ ë¶„í¬ (í•œë‘ ê°œ í† í”½ì´ ì§€ë°°ì ) â€” ì‹¤ì œ ë¬¸ì„œì— ë” ì í•©</li>
<li>\(\alpha = 1\): ê· ë“± ë¶„í¬ (ëª¨ë“  ì¡°í•©ì´ ë™ì¼ í™•ë¥ )</li>
</ul>

<div class="cc">ì½”ë“œ 8-1. ë””ë¦¬í´ë ˆ ë¶„í¬ ì‹œê°í™”</div>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

fig, axes = plt.<span class="fn">subplots</span>(<span class="nu">1</span>, <span class="nu">3</span>, figsize=(<span class="nu">15</span>, <span class="nu">4</span>))

alphas = [<span class="nu">0.1</span>, <span class="nu">1.0</span>, <span class="nu">10.0</span>]
titles = [<span class="st">'Î±=0.1 (í¬ì†Œ: í•œ í† í”½ ì§€ë°°)'</span>, <span class="st">'Î±=1.0 (ê· ë“±)'</span>, <span class="st">'Î±=10.0 (ê³ ë¥´ê²Œ ë¶„í¬)'</span>]

<span class="kw">for</span> ax, alpha, title <span class="kw">in</span> <span class="fn">zip</span>(axes, alphas, titles):
    <span class="cm"># 3ê°œ í† í”½ì— ëŒ€í•œ ë””ë¦¬í´ë ˆ ìƒ˜í”Œ 1000ê°œ</span>
    samples = np.random.<span class="fn">dirichlet</span>([alpha] * <span class="nu">3</span>, size=<span class="nu">1000</span>)
    
    <span class="cm"># ì‚¼ê° ì¢Œí‘œë¡œ ì‹œê°í™” (simplex)</span>
    ax.<span class="fn">scatter</span>(samples[:, <span class="nu">0</span>], samples[:, <span class="nu">1</span>], alpha=<span class="nu">0.3</span>, s=<span class="nu">5</span>)
    ax.<span class="fn">set_xlim</span>(<span class="nu">0</span>, <span class="nu">1</span>)
    ax.<span class="fn">set_ylim</span>(<span class="nu">0</span>, <span class="nu">1</span>)
    ax.<span class="fn">set_xlabel</span>(<span class="st">'Topic 1'</span>)
    ax.<span class="fn">set_ylabel</span>(<span class="st">'Topic 2'</span>)
    ax.<span class="fn">set_title</span>(title, fontsize=<span class="nu">11</span>)
    ax.<span class="fn">set_aspect</span>(<span class="st">'equal'</span>)

plt.<span class="fn">suptitle</span>(<span class="st">'ë””ë¦¬í´ë ˆ ë¶„í¬: Î±ê°€ í† í”½ ë¶„í¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥'</span>, fontsize=<span class="nu">13</span>)
plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</code></pre>

<h3>8.5 sklearnê³¼ Gensimìœ¼ë¡œ LDA êµ¬í˜„</h3>

<div class="cc">ì½”ë“œ 8-2. sklearn LDAë¡œ ê¸ˆìœµ ë‰´ìŠ¤ í† í”½ ì¶”ì¶œ</div>
<pre><code><span class="kw">from</span> sklearn.decomposition <span class="kw">import</span> LatentDirichletAllocation
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer

<span class="cm"># 1. DTM ìƒì„± (LDAëŠ” ë¹ˆë„ ê¸°ë°˜ì´ë¯€ë¡œ CountVectorizer ì‚¬ìš©)</span>
vectorizer = <span class="fn">CountVectorizer</span>(
    max_df=<span class="nu">0.95</span>,           <span class="cm"># 95% ì´ìƒ ë¬¸ì„œì— ë“±ì¥í•˜ë©´ ì œê±°</span>
    min_df=<span class="nu">2</span>,              <span class="cm"># ìµœì†Œ 2ê°œ ë¬¸ì„œì— ë“±ì¥</span>
    stop_words=<span class="st">'english'</span>,
    max_features=<span class="nu">5000</span>
)
dtm = vectorizer.<span class="fn">fit_transform</span>(corpus)  <span class="cm"># corpus: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸</span>

<span class="cm"># 2. LDA ëª¨ë¸ í•™ìŠµ</span>
n_topics = <span class="nu">5</span>
lda = <span class="fn">LatentDirichletAllocation</span>(
    n_components=n_topics,
    max_iter=<span class="nu">20</span>,
    learning_method=<span class="st">'online'</span>,  <span class="cm"># ëŒ€ê·œëª¨ ë°ì´í„°ì— ì í•©</span>
    random_state=<span class="nu">42</span>,
    doc_topic_prior=<span class="nu">0.1</span>,      <span class="cm"># Î±: ë¬¸ì„œ-í† í”½ ë””ë¦¬í´ë ˆ íŒŒë¼ë¯¸í„°</span>
    topic_word_prior=<span class="nu">0.01</span>     <span class="cm"># Î²: í† í”½-ë‹¨ì–´ ë””ë¦¬í´ë ˆ íŒŒë¼ë¯¸í„°</span>
)
lda.<span class="fn">fit</span>(dtm)

<span class="cm"># 3. ê° í† í”½ì˜ ìƒìœ„ ë‹¨ì–´ ì¶œë ¥</span>
feature_names = vectorizer.<span class="fn">get_feature_names_out</span>()

<span class="fn">print</span>(<span class="st">"="</span> * <span class="nu">60</span>)
<span class="kw">for</span> topic_idx, topic <span class="kw">in</span> <span class="fn">enumerate</span>(lda.components_):
    top_words = [feature_names[i] <span class="kw">for</span> i <span class="kw">in</span> topic.<span class="fn">argsort</span>()[-<span class="nu">10</span>:]]
    <span class="fn">print</span>(<span class="st">f"Topic {topic_idx + 1}: {', '.join(top_words)}"</span>)
<span class="fn">print</span>(<span class="st">"="</span> * <span class="nu">60</span>)

<span class="cm"># 4. ë¬¸ì„œì˜ í† í”½ ë¶„í¬ í™•ì¸</span>
doc_topics = lda.<span class="fn">transform</span>(dtm)
<span class="fn">print</span>(<span class="st">f"\nì²« ë²ˆì§¸ ë¬¸ì„œì˜ í† í”½ ë¶„í¬: {doc_topics[0].round(3)}"</span>)
<span class="fn">print</span>(<span class="st">f"ì§€ë°°ì  í† í”½: Topic {doc_topics[0].argmax() + 1}"</span>)
</code></pre>

<h3>8.6 í† í”½ í’ˆì§ˆ í‰ê°€: Coherence Score</h3>

<p>
"í† í”½ ìˆ˜ë¥¼ ëª‡ ê°œë¡œ ì„¤ì •í•´ì•¼ í•˜ëŠ”ê°€?" â€” R5ì—ì„œ K-Meansì˜ ìµœì  Kë¥¼ ì°¾ì„ ë•Œ ì—˜ë³´ìš° ë°©ë²•ê³¼ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë“¯ì´, LDAì—ì„œëŠ” <strong>Coherence Score</strong>ë¥¼ ì‚¬ìš©í•œë‹¤. Coherence ScoreëŠ” ê° í† í”½ì˜ ìƒìœ„ ë‹¨ì–´ë“¤ì´ ì–¼ë§ˆë‚˜ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •í•œë‹¤.
</p>

<p>
MLAT Ch.15ì—ì„œ Jansenì€ ì–´ë‹ì½œ ë°ì´í„°ì— ëŒ€í•´ í† í”½ ìˆ˜ë¥¼ 5~50ê¹Œì§€ ë³€í™”ì‹œí‚¤ë©° coherence scoreë¥¼ ì¸¡ì •í•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ í† í”½ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ coherenceê°€ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì´ë©°, ì´ëŠ” í† í”½ì´ ë„ˆë¬´ ì„¸ë¶„í™”ë˜ë©´ í’ˆì§ˆì´ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
</p>

<div class="cc">ì½”ë“œ 8-3. Gensimìœ¼ë¡œ Coherence Score ê¸°ë°˜ ìµœì  í† í”½ ìˆ˜ íƒìƒ‰</div>
<pre><code><span class="kw">from</span> gensim.models <span class="kw">import</span> LdaMulticore
<span class="kw">from</span> gensim.corpora <span class="kw">import</span> Dictionary
<span class="kw">from</span> gensim.models.coherencemodel <span class="kw">import</span> CoherenceModel

<span class="cm"># 1. Gensim í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„</span>
<span class="cm"># texts: í† í°í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [['fed', 'rate', 'hike'], ['apple', 'earnings', ...]])</span>
dictionary = <span class="fn">Dictionary</span>(texts)
dictionary.<span class="fn">filter_extremes</span>(no_below=<span class="nu">5</span>, no_above=<span class="nu">0.5</span>)
corpus_bow = [dictionary.<span class="fn">doc2bow</span>(doc) <span class="kw">for</span> doc <span class="kw">in</span> texts]

<span class="cm"># 2. í† í”½ ìˆ˜ë³„ Coherence Score ê³„ì‚°</span>
coherence_scores = []
topic_range = <span class="fn">range</span>(<span class="nu">3</span>, <span class="nu">21</span>)

<span class="kw">for</span> n_topics <span class="kw">in</span> topic_range:
    model = <span class="fn">LdaMulticore</span>(
        corpus=corpus_bow,
        id2word=dictionary,
        num_topics=n_topics,
        passes=<span class="nu">10</span>,
        workers=<span class="nu">4</span>,
        random_state=<span class="nu">42</span>
    )
    cm = <span class="fn">CoherenceModel</span>(model=model, texts=texts, dictionary=dictionary, coherence=<span class="st">'c_v'</span>)
    coherence_scores.<span class="fn">append</span>(cm.<span class="fn">get_coherence</span>())
    <span class="fn">print</span>(<span class="st">f"Topics={n_topics:2d} | Coherence={coherence_scores[-1]:.4f}"</span>)

<span class="cm"># 3. ì‹œê°í™”</span>
plt.<span class="fn">figure</span>(figsize=(<span class="nu">10</span>, <span class="nu">5</span>))
plt.<span class="fn">plot</span>(<span class="fn">list</span>(topic_range), coherence_scores, <span class="st">'bo-'</span>)
plt.<span class="fn">xlabel</span>(<span class="st">'Number of Topics'</span>)
plt.<span class="fn">ylabel</span>(<span class="st">'Coherence Score (c_v)'</span>)
plt.<span class="fn">title</span>(<span class="st">'LDA Topic Coherence by Number of Topics'</span>)
best_n = <span class="fn">list</span>(topic_range)[np.<span class="fn">argmax</span>(coherence_scores)]
plt.<span class="fn">axvline</span>(x=best_n, color=<span class="st">'r'</span>, linestyle=<span class="st">'--'</span>, label=<span class="st">f'Best: {best_n} topics'</span>)
plt.<span class="fn">legend</span>()
plt.<span class="fn">show</span>()
</code></pre>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.15ì—ì„œ Jansenì€ pyLDAvisë¥¼ ì‚¬ìš©í•˜ì—¬ LDA í† í”½ì„ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì‹œê°í™”í•œë‹¤. pyLDAvisëŠ” ê° í† í”½ì˜ í¬ê¸°, í† í”½ ê°„ ê±°ë¦¬, ê° í† í”½ì˜ í•µì‹¬ ë‹¨ì–´ë¥¼ 2D ê³µê°„ì— í‘œì‹œí•œë‹¤. ë˜í•œ ì–´ë‹ì½œ ë°ì´í„°ì— LDAë¥¼ ì ìš©í•˜ì—¬ "ê²½ì˜ì§„ì´ ì–´ë–¤ ì£¼ì œë¥¼ ê°•ì¡°í•˜ëŠ”ì§€"ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ì£¼ê°€ ë³€ë™ê³¼ ì—°ê²°í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•œë‹¤.</p>
</div>


<!-- ==================== Ch.9 ==================== -->
<h2 id="ch9">Chapter 9. Word2Vec â€” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ì„ë² ë”©í•˜ë‹¤</h2>

<h3>9.1 BoW/TF-IDFì˜ ê·¼ë³¸ì  í•œê³„</h3>

<p>
ì§€ê¸ˆê¹Œì§€ ë°°ìš´ BoWì™€ TF-IDFëŠ” ë‹¨ì–´ë¥¼ "ì›-í•«(one-hot)" ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ì–´íœ˜ê°€ 50,000ê°œë©´ ê° ë‹¨ì–´ëŠ” 50,000ì°¨ì› ë²¡í„°ì—ì„œ í•˜ë‚˜ì˜ ìœ„ì¹˜ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì´ë‹¤. ì´ í‘œí˜„ì˜ ê·¼ë³¸ì  ë¬¸ì œëŠ” <strong>ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ì „í˜€ í¬ì°©í•˜ì§€ ëª»í•œë‹¤</strong>ëŠ” ê²ƒì´ë‹¤. "king"ê³¼ "queen"ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” 0ì´ë‹¤ â€” ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì·¨ê¸‰ëœë‹¤.
</p>

<p>
Word2Vecì€ ì´ ë¬¸ì œë¥¼ í˜ëª…ì ìœ¼ë¡œ í•´ê²°í•œë‹¤. ê° ë‹¨ì–´ë¥¼ 100~300ì°¨ì›ì˜ <strong>ë°€ì§‘ ë²¡í„°(dense vector)</strong>ë¡œ í‘œí˜„í•˜ë˜, ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ê°€ ë¹„ìŠ·í•œ ë²¡í„°ë¥¼ ê°–ë„ë¡ í•™ìŠµí•œë‹¤. "king"ê³¼ "queen"ì€ ê°€ê¹Œìš´ ë²¡í„°ë¥¼ ê°–ê²Œ ë˜ê³ , ì‹¬ì§€ì–´ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
</p>

<div class="def">
<p class="ni"><strong>Word2Vecì˜ í•µì‹¬ ì•„ì´ë””ì–´ (í•œ ë¬¸ì¥)</strong></p>
<p class="ni" style="margin-top:8px">"You shall know a word by the company it keeps." â€” J.R. Firth (1957)</p>
<p class="ni" style="margin-top:4px;font-size:12px;color:#666">ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ê·¸ ë‹¨ì–´ê°€ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ë¬¸ë§¥(context)ì— ì˜í•´ ê²°ì •ëœë‹¤. ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤.</p>
</div>

<!-- BoW vs Word2Vec ë¹„êµ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #e74c3c">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#e74c3c;margin-bottom:12px">âŒ BoW / TF-IDF (í¬ì†Œ ë²¡í„°)</p>
<div style="font-family:'Space Mono',monospace;font-size:10px;background:#f8f9fa;padding:12px;border-radius:6px;line-height:1.8">
king &nbsp;= [0, 0, 1, 0, 0, ..., 0] &nbsp;(50,000ì°¨ì›)<br>
queen = [0, 0, 0, 0, 1, ..., 0] &nbsp;(50,000ì°¨ì›)<br>
cosine(king, queen) = <span style="color:#e74c3c;font-weight:bold">0.0</span> ğŸ˜¢
</div>
<p class="ni" style="font-size:11px;color:#888;margin-top:8px;text-align:center">ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì „í˜€ í¬ì°©í•˜ì§€ ëª»í•¨</p>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #27ae60">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#27ae60;margin-bottom:12px">âœ… Word2Vec (ë°€ì§‘ ë²¡í„°)</p>
<div style="font-family:'Space Mono',monospace;font-size:10px;background:#f8f9fa;padding:12px;border-radius:6px;line-height:1.8">
king &nbsp;= [0.52, -0.31, 0.78, ...] &nbsp;(300ì°¨ì›)<br>
queen = [0.48, -0.29, 0.81, ...] &nbsp;(300ì°¨ì›)<br>
cosine(king, queen) = <span style="color:#27ae60;font-weight:bold">0.85</span> ğŸ‰
</div>
<p class="ni" style="font-size:11px;color:#888;margin-top:8px;text-align:center">ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ê°€ ê°€ê¹Œìš´ ë²¡í„°ë¥¼ ê°€ì§</p>
</div>
</div>

<h3>9.2 Word2Vecì˜ ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜</h3>

<p>
Word2Vec(Mikolov et al., 2013)ì€ ì–•ì€ ì‹ ê²½ë§(shallow neural network)ì„ í•™ìŠµí•˜ì—¬ ë‹¨ì–´ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤. ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜ê°€ ìˆë‹¤:
</p>

<!-- Skip-gram vs CBOW ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:300px;background:#fff;padding:20px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#1976d2;margin-bottom:12px">Skip-gram</p>
<div style="text-align:center;padding:15px;background:#f0f4f8;border-radius:8px;margin-bottom:12px">
<div style="font-size:12px;color:#666;margin-bottom:8px">ì…ë ¥: ì¤‘ì‹¬ ë‹¨ì–´</div>
<div style="display:inline-block;background:#1976d2;color:#fff;padding:8px 20px;border-radius:20px;font-weight:bold;font-size:13px">earnings</div>
<div style="font-size:20px;margin:8px 0">â†“</div>
<div style="font-size:12px;color:#666;margin-bottom:8px">ì˜ˆì¸¡: ì£¼ë³€ ë‹¨ì–´ë“¤</div>
<div style="display:flex;gap:6px;justify-content:center;flex-wrap:wrap">
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">strong</span>
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">quarterly</span>
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">beat</span>
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">estimates</span>
</div>
</div>
<p class="ni" style="font-size:11px;color:#666;line-height:1.6">
<strong>ë°©ì‹:</strong> ì¤‘ì‹¬ ë‹¨ì–´ â†’ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡<br>
<strong>ì¥ì :</strong> ë“œë¬¸ ë‹¨ì–´ì— ê°•í•¨, ë” ë†’ì€ í’ˆì§ˆ<br>
<strong>MLAT ê¶Œì¥:</strong> â­ ê¸ˆìœµ ë°ì´í„°ì— ë” ì í•©
</p>
</div>
<div style="flex:1;min-width:300px;background:#fff;padding:20px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#7b1fa2;margin-bottom:12px">CBOW (Continuous Bag of Words)</p>
<div style="text-align:center;padding:15px;background:#faf0ff;border-radius:8px;margin-bottom:12px">
<div style="font-size:12px;color:#666;margin-bottom:8px">ì…ë ¥: ì£¼ë³€ ë‹¨ì–´ë“¤</div>
<div style="display:flex;gap:6px;justify-content:center;flex-wrap:wrap;margin-bottom:8px">
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">strong</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">quarterly</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">beat</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">estimates</span>
</div>
<div style="font-size:20px;margin:8px 0">â†“</div>
<div style="font-size:12px;color:#666;margin-bottom:8px">ì˜ˆì¸¡: ì¤‘ì‹¬ ë‹¨ì–´</div>
<div style="display:inline-block;background:#7b1fa2;color:#fff;padding:8px 20px;border-radius:20px;font-weight:bold;font-size:13px">earnings</div>
</div>
<p class="ni" style="font-size:11px;color:#666;line-height:1.6">
<strong>ë°©ì‹:</strong> ì£¼ë³€ ë‹¨ì–´ë“¤ â†’ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡<br>
<strong>ì¥ì :</strong> í•™ìŠµì´ ë¹ ë¦„, ê³ ë¹ˆë„ ë‹¨ì–´ì— ê°•í•¨<br>
<strong>ì í•©:</strong> ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ ë¹ ë¥¸ í•™ìŠµ
</p>
</div>
</div>

<h3>9.3 Skip-gramì˜ ìˆ˜í•™</h3>

<p>
Skip-gram ëª¨ë¸ì˜ ëª©ì  í•¨ìˆ˜ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ë¦¬í•˜ì. ì½”í¼ìŠ¤ì˜ ëª¨ë“  ë‹¨ì–´ \(w_t\)ì— ëŒ€í•´, ìœˆë„ìš° í¬ê¸° \(c\) ë‚´ì˜ ë¬¸ë§¥ ë‹¨ì–´ \(w_{t+j}\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í™•ë¥ ì„ ìµœëŒ€í™”í•œë‹¤:
</p>

<div class="eq">
$$\max \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$$
</div>

<p>
ì—¬ê¸°ì„œ \(P(w_{t+j} | w_t)\)ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ì •ì˜ëœë‹¤:
</p>

<div class="eq">
$$P(w_O | w_I) = \frac{\exp(\mathbf{h}^T \mathbf{v}'_{w_O})}{\sum_{w_i \in V} \exp(\mathbf{h}^T \mathbf{v}'_{w_i})}$$
</div>

<p>
í•˜ì§€ë§Œ ë¶„ëª¨ì—ì„œ ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í•©ì„ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong>Negative Sampling</strong>ì„ ì‚¬ìš©í•œë‹¤.
</p>

<h3>9.4 Negative Sampling: íš¨ìœ¨ì  í•™ìŠµ</h3>

<p>
MLAT Ch.16ì—ì„œ Jansenì€ ì„¸ ê°€ì§€ íš¨ìœ¨í™” ê¸°ë²•ì„ ì†Œê°œí•œë‹¤: Hierarchical Softmax, NCE(Noise Contrastive Estimation), Negative Sampling. ì‹¤ì „ì—ì„œ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ê²ƒì€ <strong>Negative Sampling(NEG)</strong>ì´ë‹¤.
</p>

<p>
ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤. ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ëŒ€ì‹ , ì‹¤ì œ ë¬¸ë§¥ ë‹¨ì–´(positive sample)ì™€ ëœë¤ìœ¼ë¡œ ë½‘ì€ "ë…¸ì´ì¦ˆ" ë‹¨ì–´(negative samples) ëª‡ ê°œë§Œ êµ¬ë³„í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë°”ê¾¼ë‹¤. ë³´í†µ 5~15ê°œì˜ negative sampleì„ ì‚¬ìš©í•œë‹¤.
</p>

<div class="eq">
$$\log \sigma(\mathbf{v}'_{w_O}{}^T \mathbf{h}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-\mathbf{v}'_{w_i}{}^T \mathbf{h})]$$
</div>

<p>
ì—¬ê¸°ì„œ \(\sigma\)ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, \(k\)ëŠ” negative sample ìˆ˜, \(P_n(w)\)ëŠ” ë…¸ì´ì¦ˆ ë¶„í¬(ë³´í†µ ë‹¨ì–´ ë¹ˆë„ì˜ 3/4 ì œê³±)ë‹¤.
</p>

<h3>9.5 ì˜ë¯¸ì  ì‚°ìˆ : king - man + woman = queen</h3>

<p>
Word2Vecì˜ ê°€ì¥ ë†€ë¼ìš´ íŠ¹ì„±ì€ <strong>ë²¡í„° ì‚°ìˆ (vector arithmetic)</strong>ë¡œ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. MLAT Ch.16ì—ì„œ Jansenì€ ì´ë¥¼ "semantic arithmetic"ì´ë¼ê³  ë¶€ë¥¸ë‹¤.
</p>

<!-- ë²¡í„° ì‚°ìˆ  ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff8e1,#fff3e0);border-radius:12px;border:2px solid #f9a825">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#e65100">ğŸ§® Word2Vec ë²¡í„° ì‚°ìˆ </p>
<div style="text-align:center;font-size:13px;line-height:2.5">
<div style="margin-bottom:12px">
<span style="background:#1976d2;color:#fff;padding:6px 14px;border-radius:20px">king</span>
<span style="margin:0 6px;font-size:18px">âˆ’</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">man</span>
<span style="margin:0 6px;font-size:18px">+</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">woman</span>
<span style="margin:0 6px;font-size:18px">â‰ˆ</span>
<span style="background:#2e7d32;color:#fff;padding:6px 14px;border-radius:20px;font-weight:bold">queen</span>
</div>
<div style="margin-bottom:12px">
<span style="background:#1976d2;color:#fff;padding:6px 14px;border-radius:20px">Paris</span>
<span style="margin:0 6px;font-size:18px">âˆ’</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">France</span>
<span style="margin:0 6px;font-size:18px">+</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">Japan</span>
<span style="margin:0 6px;font-size:18px">â‰ˆ</span>
<span style="background:#2e7d32;color:#fff;padding:6px 14px;border-radius:20px;font-weight:bold">Tokyo</span>
</div>
<div>
<span style="background:#1976d2;color:#fff;padding:6px 14px;border-radius:20px">stock</span>
<span style="margin:0 6px;font-size:18px">âˆ’</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">equity</span>
<span style="margin:0 6px;font-size:18px">+</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">debt</span>
<span style="margin:0 6px;font-size:18px">â‰ˆ</span>
<span style="background:#2e7d32;color:#fff;padding:6px 14px;border-radius:20px;font-weight:bold">bond</span>
</div>
</div>
<p class="ni" style="text-align:center;margin-top:12px;font-size:11px;color:#666">"king - man"ì€ "ì™•ì¡±" ê°œë…ì„ ì¶”ì¶œí•˜ê³ , "woman"ì„ ë”í•˜ë©´ "ì—¬ì™•"ì´ ëœë‹¤.<br>ê°™ì€ ì›ë¦¬ë¡œ ê¸ˆìœµ ìš©ì–´ ê°„ì˜ ê´€ê³„ë„ í¬ì°©í•  ìˆ˜ ìˆë‹¤.</p>
</div>

<h3>9.6 Gensimìœ¼ë¡œ ê¸ˆìœµ ë‰´ìŠ¤ Word2Vec í•™ìŠµ</h3>

<div class="cc">ì½”ë“œ 9-1. Gensim Word2Vecìœ¼ë¡œ ê¸ˆìœµ ì„ë² ë”© í•™ìŠµ</div>
<pre><code><span class="kw">from</span> gensim.models <span class="kw">import</span> Word2Vec
<span class="kw">from</span> gensim.models.phrases <span class="kw">import</span> Phrases, Phraser
<span class="kw">from</span> gensim.models.word2vec <span class="kw">import</span> LineSentence

<span class="cm"># 1. ë¬¸ì¥ ë¡œë“œ (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼)</span>
sentence_path = <span class="st">'data/financial_news_sentences.txt'</span>
sentences = <span class="fn">LineSentence</span>(sentence_path)

<span class="cm"># 2. êµ¬ë¬¸ íƒì§€ (bigram, trigram)</span>
<span class="cm">#    "interest" + "rate" â†’ "interest_rate"</span>
phrases = <span class="fn">Phrases</span>(
    sentences=sentences,
    min_count=<span class="nu">10</span>,
    threshold=<span class="nu">0.5</span>,
    delimiter=<span class="st">'_'</span>,
    scoring=<span class="st">'npmi'</span>  <span class="cm"># Normalized Pointwise Mutual Information</span>
)
bigram = <span class="fn">Phraser</span>(phrases)
sentences_with_bigrams = bigram[sentences]

<span class="cm"># 3. Word2Vec ëª¨ë¸ í•™ìŠµ</span>
model = <span class="fn">Word2Vec</span>(
    sentences=sentences_with_bigrams,
    sg=<span class="nu">1</span>,            <span class="cm"># 1=Skip-gram (MLAT ê¶Œì¥), 0=CBOW</span>
    vector_size=<span class="nu">300</span>, <span class="cm"># ì„ë² ë”© ì°¨ì›</span>
    window=<span class="nu">5</span>,        <span class="cm"># ë¬¸ë§¥ ìœˆë„ìš° í¬ê¸°</span>
    min_count=<span class="nu">20</span>,    <span class="cm"># ìµœì†Œ ë¹ˆë„</span>
    negative=<span class="nu">15</span>,     <span class="cm"># Negative sampling ìˆ˜</span>
    workers=<span class="nu">8</span>,       <span class="cm"># ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ</span>
    epochs=<span class="nu">10</span>,       <span class="cm"># í•™ìŠµ ì—í¬í¬</span>
    alpha=<span class="nu">0.025</span>,     <span class="cm"># ì´ˆê¸° í•™ìŠµë¥ </span>
    min_alpha=<span class="nu">0.0001</span> <span class="cm"># ìµœì¢… í•™ìŠµë¥ </span>
)

<span class="cm"># 4. ëª¨ë¸ ì €ì¥</span>
model.<span class="fn">save</span>(<span class="st">'models/financial_word2vec.model'</span>)

<span class="cm"># 5. ìœ ì‚¬ ë‹¨ì–´ íƒìƒ‰</span>
<span class="fn">print</span>(<span class="st">"'inflation'ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´:"</span>)
<span class="kw">for</span> word, score <span class="kw">in</span> model.wv.<span class="fn">most_similar</span>(<span class="st">'inflation'</span>, topn=<span class="nu">10</span>):
    <span class="fn">print</span>(<span class="st">f"  {word:<25} {score:.4f}"</span>)

<span class="cm"># 6. ë²¡í„° ì‚°ìˆ </span>
<span class="fn">print</span>(<span class="st">"\n'bull' - 'stock' + 'bond' â‰ˆ ?"</span>)
result = model.wv.<span class="fn">most_similar</span>(positive=[<span class="st">'bull'</span>, <span class="st">'bond'</span>], negative=[<span class="st">'stock'</span>], topn=<span class="nu">5</span>)
<span class="kw">for</span> word, score <span class="kw">in</span> result:
    <span class="fn">print</span>(<span class="st">f"  {word:<25} {score:.4f}"</span>)
</code></pre>

<h3>9.7 GloVe: ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© í™œìš©</h3>

<p>
Word2Vec ì™¸ì— <strong>GloVe(Global Vectors for Word Representation)</strong>ë„ ë„ë¦¬ ì‚¬ìš©ëœë‹¤. Stanford NLP ì—°êµ¬ì‹¤ì—ì„œ ê°œë°œí•œ GloVeëŠ” ì „ì—­ ë‹¨ì–´-ë‹¨ì–´ ë™ì‹œ ì¶œí˜„ í†µê³„ë¥¼ í™œìš©í•˜ì—¬ ì„ë² ë”©ì„ í•™ìŠµí•œë‹¤. MLAT Ch.16ì—ì„œ Jansenì€ Wikipedia 20ì–µ í† í°ìœ¼ë¡œ í•™ìŠµëœ GloVe ë²¡í„°ê°€ word2vec ìœ ì¶” í…ŒìŠ¤íŠ¸ì—ì„œ 75.44% ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤ê³  ë³´ê³ í•œë‹¤.
</p>

<div class="tc">í‘œ 9-1. ì‚¬ì „ í•™ìŠµëœ GloVe ë²¡í„° (MLAT Ch.16)</div>
<table>
<thead>
<tr><th>ì†ŒìŠ¤</th><th>í† í° ìˆ˜</th><th>ì–´íœ˜ í¬ê¸°</th><th>ìœ ì¶” ì •í™•ë„</th></tr>
</thead>
<tbody>
<tr><td>Wikipedia + Gigaword</td><td>60ì–µ</td><td>40ë§Œ</td><td>75.4%</td></tr>
<tr><td>Common Crawl (42B)</td><td>420ì–µ</td><td>190ë§Œ</td><td>78.0%</td></tr>
<tr><td>Twitter</td><td>270ì–µ</td><td>120ë§Œ</td><td>56.4%</td></tr>
</tbody>
</table>

<div class="warn">
<p class="ni"><strong>âš ï¸ ì‚¬ì „ í•™ìŠµ vs ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© (MLAT Ch.16)</strong></p>
<p class="ni" style="margin-top:8px">Wikipediaë¡œ í•™ìŠµëœ GloVeëŠ” ì¼ë°˜ì ì¸ ì–¸ì–´ ê´€ê³„ë¥¼ ì˜ í¬ì°©í•˜ì§€ë§Œ, ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ì˜ ë¯¸ë¬˜í•œ ì°¨ì´ëŠ” ë†“ì¹  ìˆ˜ ìˆë‹¤. "quantitative easing"ì´ë‚˜ "yield curve inversion" ê°™ì€ ê¸ˆìœµ íŠ¹ìˆ˜ í‘œí˜„ì€ ê¸ˆìœµ ë‰´ìŠ¤/ê³µì‹œë¡œ ì§ì ‘ í•™ìŠµí•œ ì„ë² ë”©ì´ ë” ì •í™•í•˜ë‹¤. MLATì—ì„œ SEC 10-K ê³µì‹œë¡œ í•™ìŠµí•œ ì»¤ìŠ¤í…€ ì„ë² ë”©ì´ 38.5% ìœ ì¶” ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆëŠ”ë°, ì´ëŠ” ì¼ë°˜ ì½”í¼ìŠ¤ë³´ë‹¤ ë‚®ì§€ë§Œ ê¸ˆìœµ íŠ¹í™” ìœ ì¶”ì—ì„œëŠ” ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.</p>
</div>


<!-- ==================== Ch.10 ==================== -->
<h2 id="ch10">Chapter 10. Doc2Vec + ê°ì„±ë¶„ì„ â€” ë¬¸ì„œ ì „ì²´ë¥¼ ë²¡í„°ë¡œ</h2>

<h3>10.1 Word2Vecì—ì„œ Doc2Vecìœ¼ë¡œ</h3>

<p>
Word2Vecì€ ê°œë³„ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤. í•˜ì§€ë§Œ ê°ì„±ë¶„ì„ì´ë‚˜ ë¬¸ì„œ ë¶„ë¥˜ì—ì„œëŠ” ë¬¸ì„œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•´ì•¼ í•œë‹¤. ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€ ë¬¸ì„œ ë‚´ ëª¨ë“  ë‹¨ì–´ì˜ Word2Vec ë²¡í„°ë¥¼ í‰ê· ë‚´ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ë‹¨ì–´ ìˆœì„œ ì •ë³´ë¥¼ ì™„ì „íˆ ìƒëŠ”ë‹¤.
</p>

<p>
<strong>Doc2Vec</strong>(Paragraph Vector, Le & Mikolov, 2014)ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. Word2Vecì„ í™•ì¥í•˜ì—¬ ë¬¸ì„œ ìì²´ì—ë„ ê³ ìœ í•œ ë²¡í„°ë¥¼ ë¶€ì—¬í•œë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ë¬¸ì„œ ë²¡í„°ê°€ í•´ë‹¹ ë¬¸ì„œì˜ "ê¸°ì–µ(memory)" ì—­í• ì„ í•˜ì—¬, ë‹¨ì–´ ìˆœì„œì™€ ë¬¸ë§¥ ì •ë³´ë¥¼ ë³´ì¡´í•œë‹¤.
</p>

<!-- Doc2Vec ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:13px;color:#1976d2;margin-bottom:12px">DM (Distributed Memory)</p>
<div style="text-align:center;padding:12px;background:#f0f4f8;border-radius:8px;font-size:11px">
<div style="margin-bottom:8px">ì…ë ¥: <span style="background:#1976d2;color:#fff;padding:2px 8px;border-radius:10px">Doc ID</span> + ë¬¸ë§¥ ë‹¨ì–´ë“¤</div>
<div style="margin-bottom:4px">â†“ ì—°ê²°(concatenate) ë˜ëŠ” í‰ê· </div>
<div>ì˜ˆì¸¡: ë‹¤ìŒ ë‹¨ì–´</div>
</div>
<p class="ni" style="font-size:11px;color:#666;margin-top:8px">Word2Vec Skip-gramì— ëŒ€ì‘. ë¬¸ì„œ ë²¡í„°ê°€ "ê¸°ì–µ" ì—­í• ì„ í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ ë³´ì¡´í•œë‹¤.</p>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:13px;color:#7b1fa2;margin-bottom:12px">DBOW (Distributed Bag of Words)</p>
<div style="text-align:center;padding:12px;background:#faf0ff;border-radius:8px;font-size:11px">
<div style="margin-bottom:8px">ì…ë ¥: <span style="background:#7b1fa2;color:#fff;padding:2px 8px;border-radius:10px">Doc ID</span></div>
<div style="margin-bottom:4px">â†“</div>
<div>ì˜ˆì¸¡: ë¬¸ì„œ ë‚´ ëœë¤ ë‹¨ì–´</div>
</div>
<p class="ni" style="font-size:11px;color:#666;margin-top:8px">Word2Vec CBOWì— ëŒ€ì‘. ë” ë¹ ë¥´ê³  ê°„ë‹¨í•˜ì§€ë§Œ, ë‹¨ì–´ ìˆœì„œ ì •ë³´ë¥¼ ëœ ë³´ì¡´í•œë‹¤.</p>
</div>
</div>

<h3>10.2 Gensim Doc2Vec ì‹¤ì „</h3>

<div class="cc">ì½”ë“œ 10-1. Doc2Vecìœ¼ë¡œ ê¸ˆìœµ ë¬¸ì„œ ì„ë² ë”© + ê°ì„± ë¶„ë¥˜</div>
<pre><code><span class="kw">from</span> gensim.models.doc2vec <span class="kw">import</span> Doc2Vec, TaggedDocument
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.linear_model <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.ensemble <span class="kw">import</span> RandomForestClassifier
<span class="kw">import</span> lightgbm <span class="kw">as</span> lgb
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd

<span class="cm"># 1. ë°ì´í„° ì¤€ë¹„ (TaggedDocument í˜•ì‹)</span>
<span class="cm"># df: 'text' (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸), 'sentiment' (0/1/2) ì»¬ëŸ¼</span>
documents = []
<span class="kw">for</span> i, row <span class="kw">in</span> df.<span class="fn">iterrows</span>():
    tokens = row[<span class="st">'text'</span>].<span class="fn">split</span>()
    documents.<span class="fn">append</span>(<span class="fn">TaggedDocument</span>(words=tokens, tags=[i]))

<span class="cm"># 2. Doc2Vec ëª¨ë¸ í•™ìŠµ</span>
model = <span class="fn">Doc2Vec</span>(
    documents=documents,
    dm=<span class="nu">1</span>,            <span class="cm"># 1=DM (Distributed Memory), 0=DBOW</span>
    vector_size=<span class="nu">300</span>, <span class="cm"># ë¬¸ì„œ ë²¡í„° ì°¨ì›</span>
    window=<span class="nu">5</span>,        <span class="cm"># ë¬¸ë§¥ ìœˆë„ìš°</span>
    min_count=<span class="nu">50</span>,    <span class="cm"># ìµœì†Œ ë¹ˆë„</span>
    negative=<span class="nu">5</span>,      <span class="cm"># Negative sampling</span>
    epochs=<span class="nu">20</span>,       <span class="cm"># í•™ìŠµ ì—í¬í¬</span>
    workers=<span class="nu">4</span>
)

<span class="cm"># 3. ë¬¸ì„œ ë²¡í„° ì¶”ì¶œ â†’ ML í”¼ì²˜ë¡œ ì‚¬ìš©</span>
X = np.<span class="fn">zeros</span>((len(df), <span class="nu">300</span>))
<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(df)):
    X[i] = model.dv[i]

y = df[<span class="st">'sentiment'</span>].values

<span class="cm"># 4. Train/Test ë¶„í• </span>
X_train, X_test, y_train, y_test = <span class="fn">train_test_split</span>(
    X, y, test_size=<span class="nu">0.2</span>, random_state=<span class="nu">42</span>, stratify=y
)

<span class="cm"># 5. ì—¬ëŸ¬ ë¶„ë¥˜ê¸° ë¹„êµ (MLAT Ch.16 ë°©ì‹)</span>
models = {
    <span class="st">'Logistic Regression'</span>: <span class="fn">LogisticRegression</span>(multi_class=<span class="st">'multinomial'</span>, max_iter=<span class="nu">1000</span>),
    <span class="st">'Random Forest'</span>: <span class="fn">RandomForestClassifier</span>(n_estimators=<span class="nu">500</span>, n_jobs=-<span class="nu">1</span>),
}

<span class="kw">for</span> name, clf <span class="kw">in</span> models.items():
    clf.<span class="fn">fit</span>(X_train, y_train)
    acc = clf.<span class="fn">score</span>(X_test, y_test)
    <span class="fn">print</span>(<span class="st">f"{name}: {acc:.4f}"</span>)

<span class="cm"># LightGBM</span>
train_data = lgb.<span class="fn">Dataset</span>(X_train, label=y_train)
test_data = lgb.<span class="fn">Dataset</span>(X_test, label=y_test, reference=train_data)
params = {<span class="st">'objective'</span>: <span class="st">'multiclass'</span>, <span class="st">'num_classes'</span>: <span class="nu">3</span>, <span class="st">'verbose'</span>: -<span class="nu">1</span>}
lgb_model = lgb.<span class="fn">train</span>(params, train_data, num_boost_round=<span class="nu">5000</span>,
                      valid_sets=[test_data],
                      callbacks=[lgb.<span class="fn">early_stopping</span>(<span class="nu">25</span>)])
lgb_pred = lgb_model.<span class="fn">predict</span>(X_test).<span class="fn">argmax</span>(axis=<span class="nu">1</span>)
<span class="fn">print</span>(<span class="st">f"LightGBM: {(lgb_pred == y_test).mean():.4f}"</span>)
</code></pre>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.16ì—ì„œ Jansenì€ Yelp ë¦¬ë·° 50ë§Œ ê°œì— Doc2Vecì„ ì ìš©í•˜ì—¬ ê°ì„± ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œë‹¤. LightGBMì´ 62.24%ë¡œ ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤. ì´ëŠ” TF-IDF + LightGBM(73.6%)ë³´ë‹¤ ë‚®ì€ë°, Jansenì€ ì´ë¥¼ "Doc2Vecì€ ë” ì ì€ í”¼ì²˜(300ì°¨ì›)ë¡œ ì‘ë™í•˜ë¯€ë¡œ, ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´ TF-IDFê°€ ë” ë‚˜ì„ ìˆ˜ ìˆë‹¤"ê³  ì„¤ëª…í•œë‹¤. í•˜ì§€ë§Œ Doc2Vecì˜ ì¥ì ì€ (1) ë°€ì§‘ ë²¡í„°ë¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì , (2) ìƒˆ ë¬¸ì„œì— ëŒ€í•œ ì¶”ë¡ ì´ ê°€ëŠ¥, (3) ë‹¤ë¥¸ ML ëª¨ë¸ì˜ ì…ë ¥ í”¼ì²˜ë¡œ í™œìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì´ë‹¤.</p>
</div>


<h3>10.3 DM vs DBOW: ìˆ˜í•™ì  ë¹„êµ</h3>

<p>
Doc2Vecì˜ ë‘ ì•„í‚¤í…ì²˜ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë¹„êµí•´ë³´ì.
</p>

<p>
<strong>DM (Distributed Memory)</strong>ì˜ ëª©ì í•¨ìˆ˜ëŠ” Word2Vec CBOWë¥¼ í™•ì¥í•œ ê²ƒì´ë‹¤. 
ë¬¸ì„œ ë²¡í„° \(\mathbf{d}\)ì™€ ë¬¸ë§¥ ë‹¨ì–´ ë²¡í„° \(\mathbf{w}_{t-k}, \ldots, \mathbf{w}_{t+k}\)ë¥¼ 
ê²°í•©í•˜ì—¬ íƒ€ê²Ÿ ë‹¨ì–´ \(w_t\)ë¥¼ ì˜ˆì¸¡í•œë‹¤:
</p>

$$\max \sum_{t=k}^{T-k} \log P(w_t \mid \mathbf{d}, w_{t-k}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+k})$$

<p>
<strong>DBOW (Distributed Bag of Words)</strong>ëŠ” ë” ë‹¨ìˆœí•˜ë‹¤. ë¬¸ì„œ ë²¡í„°ë§Œìœ¼ë¡œ ë¬¸ì„œ ë‚´ ëœë¤ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤:
</p>

$$\max \sum_{w \in d} \log P(w \mid \mathbf{d})$$

<div class="tc">í‘œ 10-1. DM vs DBOW ë¹„êµ</div>
<table>
<thead>
<tr><th>íŠ¹ì„±</th><th>DM (Distributed Memory)</th><th>DBOW (Distributed BoW)</th></tr>
</thead>
<tbody>
<tr><td><strong>ì…ë ¥</strong></td><td>ë¬¸ì„œ ID + ë¬¸ë§¥ ë‹¨ì–´</td><td>ë¬¸ì„œ IDë§Œ</td></tr>
<tr><td><strong>ì˜ˆì¸¡ ëŒ€ìƒ</strong></td><td>ë‹¤ìŒ ë‹¨ì–´</td><td>ë¬¸ì„œ ë‚´ ëœë¤ ë‹¨ì–´</td></tr>
<tr><td><strong>ìˆœì„œ ì •ë³´</strong></td><td>ë³´ì¡´ (ìœˆë„ìš° ë‚´)</td><td>ë¬´ì‹œ</td></tr>
<tr><td><strong>í•™ìŠµ ì†ë„</strong></td><td>ëŠë¦¼</td><td>ë¹ ë¦„</td></tr>
<tr><td><strong>ì„±ëŠ¥ (ì¼ë°˜ì )</strong></td><td>ê¸´ ë¬¸ì„œì—ì„œ ìš°ìˆ˜</td><td>ì§§ì€ ë¬¸ì„œì—ì„œ ìš°ìˆ˜</td></tr>
<tr><td><strong>Word2Vec ëŒ€ì‘</strong></td><td>CBOW í™•ì¥</td><td>Skip-gram í™•ì¥</td></tr>
<tr><td><strong>Gensim íŒŒë¼ë¯¸í„°</strong></td><td><code>dm=1</code></td><td><code>dm=0</code></td></tr>
</tbody>
</table>

<div class="ok">
<p class="ni"><strong>ì‹¤ì „ íŒ: DM + DBOW ê²°í•©</strong></p>
<p class="ni" style="margin-top:8px">
Le & Mikolovì˜ ì› ë…¼ë¬¸ê³¼ MLAT Ch.16 ëª¨ë‘ DMê³¼ DBOW ë²¡í„°ë¥¼ <strong>ì—°ê²°(concatenate)</strong>í•˜ì—¬ 
ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ DMìœ¼ë¡œ 300ì°¨ì›, DBOWë¡œ 300ì°¨ì›ì„ í•™ìŠµí•œ ë’¤ 600ì°¨ì› ë²¡í„°ë¡œ 
ê²°í•©í•˜ë©´, ìˆœì„œ ì •ë³´(DM)ì™€ ì „ì—­ ì˜ë¯¸(DBOW)ë¥¼ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆë‹¤.
</p>
</div>

<h3>10.4 Doc2Vec vs ë‹¤ë¥¸ ë¬¸ì„œ í‘œí˜„ ë°©ë²• ë¹„êµ</h3>

<p>
ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆë‹¤. ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ë³´ì.
</p>

<!-- ë¬¸ì„œ í‘œí˜„ ë°©ë²• ë¹„êµ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;display:flex;gap:12px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #ff9800;text-align:center">
<div style="font-size:20px;margin-bottom:6px">ğŸ“Š</div>
<div style="font-weight:bold;color:#e65100;font-size:12px;margin-bottom:6px">TF-IDF í‰ê· </div>
<div style="font-size:10px;color:#666;line-height:1.5">í¬ì†Œ ë²¡í„°<br>|V|ì°¨ì›<br>ë¹ ë¥´ê³  ë‹¨ìˆœ<br>ë¬¸ë§¥ ë¬´ì‹œ</div>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #2196f3;text-align:center">
<div style="font-size:20px;margin-bottom:6px">ğŸ“</div>
<div style="font-weight:bold;color:#1565c0;font-size:12px;margin-bottom:6px">Word2Vec í‰ê· </div>
<div style="font-size:10px;color:#666;line-height:1.5">ë°€ì§‘ ë²¡í„°<br>300ì°¨ì›<br>ì˜ë¯¸ ë³´ì¡´<br>ìˆœì„œ ë¬´ì‹œ</div>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #4caf50;text-align:center">
<div style="font-size:20px;margin-bottom:6px">ğŸ“„</div>
<div style="font-weight:bold;color:#2e7d32;font-size:12px;margin-bottom:6px">Doc2Vec</div>
<div style="font-size:10px;color:#666;line-height:1.5">ë°€ì§‘ ë²¡í„°<br>300ì°¨ì›<br>ë¬¸ì„œ ê³ ìœ  ë²¡í„°<br>ìˆœì„œ ì¼ë¶€ ë³´ì¡´</div>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #9c27b0;text-align:center">
<div style="font-size:20px;margin-bottom:6px">ğŸ¤–</div>
<div style="font-weight:bold;color:#7b1fa2;font-size:12px;margin-bottom:6px">BERT [CLS]</div>
<div style="font-size:10px;color:#666;line-height:1.5">ë°€ì§‘ ë²¡í„°<br>768ì°¨ì›<br>ë¬¸ë§¥ ì™„ì „ ì´í•´<br>ê³„ì‚° ë¹„ìš© ë†’ìŒ</div>
</div>
</div>

<div class="cc">ì½”ë“œ 10-2. 4ê°€ì§€ ë¬¸ì„œ í‘œí˜„ ë°©ë²• ì„±ëŠ¥ ë¹„êµ</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.linear_model <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> cross_val_score
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ê°€ì •: texts (ë¦¬ìŠ¤íŠ¸), labels (ë°°ì—´)ì´ ì¤€ë¹„ë˜ì–´ ìˆë‹¤</span>

<span class="cm"># ë°©ë²• 1: TF-IDF</span>
tfidf = <span class="fn">TfidfVectorizer</span>(max_features=<span class="nu">5000</span>, stop_words=<span class="st">'english'</span>)
X_tfidf = tfidf.<span class="fn">fit_transform</span>(texts)

<span class="cm"># ë°©ë²• 2: Word2Vec í‰ê·  (ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©)</span>
<span class="kw">from</span> gensim.models <span class="kw">import</span> KeyedVectors
<span class="cm"># wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)</span>
<span class="kw">def</span> <span class="fn">doc_to_w2v_mean</span>(text, wv, dim=<span class="nu">300</span>):
    <span class="st">"""ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë²¡í„°ì˜ í‰ê· """</span>
    words = text.<span class="fn">lower</span>().<span class="fn">split</span>()
    vecs = [wv[w] <span class="kw">for</span> w <span class="kw">in</span> words <span class="kw">if</span> w <span class="kw">in</span> wv]
    <span class="kw">if</span> <span class="nb">len</span>(vecs) == <span class="nu">0</span>:
        <span class="kw">return</span> np.<span class="fn">zeros</span>(dim)
    <span class="kw">return</span> np.<span class="fn">mean</span>(vecs, axis=<span class="nu">0</span>)

<span class="cm"># ë°©ë²• 3: Doc2Vec (ìœ„ì—ì„œ í•™ìŠµí•œ ëª¨ë¸)</span>
<span class="cm"># X_d2v = np.array([d2v_model.dv[i] for i in range(len(texts))])</span>

<span class="cm"># ë°©ë²• 4: BERT [CLS] í† í° (GPU í•„ìš”)</span>
<span class="cm"># from transformers import AutoTokenizer, AutoModel</span>
<span class="cm"># tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')</span>
<span class="cm"># model = AutoModel.from_pretrained('bert-base-uncased')</span>

<span class="cm"># ì„±ëŠ¥ ë¹„êµ (TF-IDF ì˜ˆì‹œ)</span>
clf = <span class="fn">LogisticRegression</span>(max_iter=<span class="nu">1000</span>)
scores = <span class="fn">cross_val_score</span>(clf, X_tfidf, labels, cv=<span class="nu">5</span>, scoring=<span class="st">'accuracy'</span>)
<span class="fn">print</span>(<span class="st">f"TF-IDF + LogReg: {scores.mean():.4f} Â± {scores.std():.4f}"</span>)

<span class="cm"># MLAT Ch.16 ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (Yelp ë¦¬ë·° 50ë§Œ ê°œ):</span>
<span class="cm"># TF-IDF + LightGBM:  73.6%</span>
<span class="cm"># Doc2Vec + LightGBM: 62.2%</span>
<span class="cm"># Word2Vec avg + LR:  ~65%</span>
<span class="cm"># BERT fine-tuned:    ~90%+ (but much slower)</span></code></pre>

<div class="warn">
<p class="ni"><strong>âš ï¸ Doc2Vecì˜ í˜„ì‹¤ì  ìœ„ì¹˜ (MLAT Ch.16 ê²°ë¡ )</strong></p>
<p class="ni" style="margin-top:8px">
Jansenì˜ ì‹¤í—˜ ê²°ê³¼ì—ì„œ Doc2Vecì€ TF-IDFë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŠ” Doc2Vecì´ ë‚˜ìœ ëª¨ë¸ì´ë¼ëŠ” ëœ»ì´ ì•„ë‹ˆë¼, 
<strong>ë°ì´í„°ê°€ ì¶©ë¶„í•˜ê³  ì–´íœ˜ê°€ í’ë¶€í•œ ê²½ìš° TF-IDFì˜ ê³ ì°¨ì› í¬ì†Œ í‘œí˜„ì´ ë” ë§ì€ ì •ë³´ë¥¼ ë‹´ì„ ìˆ˜ ìˆë‹¤</strong>ëŠ” ê²ƒì´ë‹¤. 
Doc2Vecì˜ ì§„ì •í•œ ê°€ì¹˜ëŠ” (1) ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (300ì°¨ì› vs 50,000ì°¨ì›), (2) ìƒˆ ë¬¸ì„œì— ëŒ€í•œ ì¶”ë¡  ê°€ëŠ¥, 
(3) ë‹¤ë¥¸ í”¼ì²˜ì™€ ê²°í•©í•˜ê¸° ì‰¬ìš´ ë°€ì§‘ ë²¡í„°ë¼ëŠ” ì ì´ë‹¤. ì‹¤ì „ì—ì„œëŠ” TF-IDFì™€ Doc2Vecì„ ëª¨ë‘ í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ê³ , 
ëª¨ë¸ì´ ì•Œì•„ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì„ íƒí•˜ê²Œ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
</p>
</div>

<!-- ==================== Ch.11 ==================== -->
<h2 id="ch11">Chapter 11. Transformerì™€ BERT â€” NLPì˜ ê²Œì„ ì²´ì¸ì €</h2>

<h3>11.1 Attention is All You Need</h3>

<p>
2017ë…„, Googleì˜ "Attention is All You Need" ë…¼ë¬¸(Vaswani et al.)ì´ NLPì˜ íŒë„ë¥¼ ì™„ì „íˆ ë°”ê¿¨ë‹¤. ê¸°ì¡´ì˜ RNN(ìˆœí™˜ ì‹ ê²½ë§)ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í–ˆê¸° ë•Œë¬¸ì— (1) ë¨¼ ê±°ë¦¬ì˜ ë‹¨ì–´ ê´€ê³„ë¥¼ í¬ì°©í•˜ê¸° ì–´ë µê³ , (2) ë³‘ë ¬ ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í–ˆë‹¤. TransformerëŠ” <strong>Self-Attention</strong> ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì´ ë‘ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í–ˆë‹¤.
</p>

<div class="def">
<p class="ni"><strong>Self-Attentionì˜ í•µì‹¬ ì•„ì´ë””ì–´</strong></p>
<p class="ni" style="margin-top:8px">ë¬¸ì¥ì˜ ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ì˜ ê´€ê³„ë¥¼ ë™ì‹œì— ê³„ì‚°í•œë‹¤. "The Fed raised rates because inflation was persistent"ì—ì„œ "rates"ë¥¼ ì´í•´í•˜ë ¤ë©´ "Fed"ì™€ "inflation"ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ attention scoreë¡œ í•™ìŠµí•œë‹¤.</p>
</div>

<!-- Attention ì‹œê°í™” -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#2c3e50">ğŸ” Self-Attention ì‹œê°í™”: "rates"ì— ëŒ€í•œ attention</p>
<div style="display:flex;justify-content:center;gap:6px;flex-wrap:wrap;margin-bottom:12px">
<span style="background:rgba(25,118,210,0.7);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">The</span>
<span style="background:rgba(25,118,210,0.9);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px;font-weight:bold">Fed</span>
<span style="background:rgba(25,118,210,0.5);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">raised</span>
<span style="background:rgba(233,30,99,1.0);color:#fff;padding:6px 14px;border-radius:6px;font-size:13px;font-weight:bold;border:2px solid #c2185b">rates â† target</span>
<span style="background:rgba(25,118,210,0.3);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">because</span>
<span style="background:rgba(25,118,210,0.85);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px;font-weight:bold">inflation</span>
<span style="background:rgba(25,118,210,0.2);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">was</span>
<span style="background:rgba(25,118,210,0.6);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">persistent</span>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666">ìƒ‰ì´ ì§„í• ìˆ˜ë¡ "rates"ì— ëŒ€í•œ attention scoreê°€ ë†’ë‹¤. "Fed"ì™€ "inflation"ì´ ê°€ì¥ ë†’ì€ attentionì„ ë°›ëŠ”ë‹¤.</p>
</div>

<h3>11.2 Attentionì˜ ìˆ˜í•™</h3>

<p>
Self-Attentionì€ Query(Q), Key(K), Value(V) ì„¸ ê°€ì§€ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤. ì…ë ¥ ì„ë² ë”© \(X\)ì— í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•˜ì—¬ Q, K, Vë¥¼ ìƒì„±í•œë‹¤:
</p>

<div class="eq">
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
</div>

<p>
ì—¬ê¸°ì„œ \(d_k\)ëŠ” Key ë²¡í„°ì˜ ì°¨ì›ì´ë‹¤. \(\sqrt{d_k}\)ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì€ ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ìŠ¤ì¼€ì¼ë§ì´ë‹¤. TransformerëŠ” ì´ attentionì„ ì—¬ëŸ¬ ê°œì˜ "head"ë¡œ ë³‘ë ¬ ì‹¤í–‰í•˜ëŠ” <strong>Multi-Head Attention</strong>ì„ ì‚¬ìš©í•œë‹¤.
</p>

<h3>11.3 BERT: ì–‘ë°©í–¥ ì‚¬ì „ í•™ìŠµ</h3>

<p>
MLAT Ch.16ì—ì„œ Jansenì€ BERT(Bidirectional Encoder Representations from Transformers, Devlin et al., 2019)ë¥¼ "NLPì˜ ê²Œì„ ì²´ì¸ì €"ë¡œ ì†Œê°œí•œë‹¤. BERTì˜ í•µì‹¬ í˜ì‹ ì€ ë‘ ê°€ì§€ë‹¤:
</p>

<ol>
<li><strong>ì–‘ë°©í–¥ ë¬¸ë§¥:</strong> Word2Vecì´ë‚˜ GloVeëŠ” ë‹¨ì–´ì— í•˜ë‚˜ì˜ ê³ ì •ëœ ë²¡í„°ë¥¼ ë¶€ì—¬í•œë‹¤. "bank"ê°€ "ì€í–‰"ì¸ì§€ "ê°•ë‘‘"ì¸ì§€ êµ¬ë¶„í•˜ì§€ ëª»í•œë‹¤. BERTëŠ” ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ë²¡í„°ë¥¼ ìƒì„±í•œë‹¤.</li>
<li><strong>ì‚¬ì „ í•™ìŠµ + ë¯¸ì„¸ ì¡°ì •:</strong> Wikipedia + BookCorpus(33ì–µ ë‹¨ì–´)ë¡œ ì‚¬ì „ í•™ìŠµí•œ í›„, íŠ¹ì • íƒœìŠ¤í¬(ê°ì„±ë¶„ì„, NER ë“±)ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •(fine-tuning)í•œë‹¤.</li>
</ol>

<!-- BERT ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e3f2fd,#bbdefb);border-radius:12px;border:1px solid #90caf9">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#1565c0">ğŸ—ï¸ BERT ì•„í‚¤í…ì²˜ ê°œìš”</p>
<div style="display:flex;flex-direction:column;gap:8px;max-width:500px;margin:0 auto;font-size:12px">

<div style="background:#fff;padding:10px 14px;border-radius:8px;text-align:center;border:1px solid #90caf9">
<strong>ì‚¬ì „ í•™ìŠµ (Pre-training)</strong><br>
<span style="font-size:11px;color:#666">Task 1: Masked Language Model (ë‹¨ì–´ 15% ë§ˆìŠ¤í‚¹ â†’ ì˜ˆì¸¡)<br>
Task 2: Next Sentence Prediction (ë‹¤ìŒ ë¬¸ì¥ ë§ì¶”ê¸°)</span>
</div>

<div style="text-align:center;font-size:16px;color:#1976d2">â†“</div>

<div style="background:#fff;padding:10px 14px;border-radius:8px;text-align:center;border:1px solid #90caf9">
<strong>BERT ëª¨ë¸</strong><br>
<span style="font-size:11px;color:#666">12 layers Ã— 12 attention heads = 144 attention ë©”ì»¤ë‹ˆì¦˜<br>
110M íŒŒë¼ë¯¸í„° (BERT-base) / 340M (BERT-large)</span>
</div>

<div style="text-align:center;font-size:16px;color:#1976d2">â†“</div>

<div style="background:#fff;padding:10px 14px;border-radius:8px;text-align:center;border:1px solid #90caf9">
<strong>ë¯¸ì„¸ ì¡°ì • (Fine-tuning)</strong><br>
<span style="font-size:11px;color:#666">ê°ì„±ë¶„ì„, NER, ì§ˆì˜ì‘ë‹µ, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë“±<br>
ì†ŒëŸ‰ì˜ ë¼ë²¨ ë°ì´í„°ë¡œ íŠ¹ì • íƒœìŠ¤í¬ì— ì ì‘</span>
</div>

</div>
</div>

<h3>11.4 FinBERT: ê¸ˆìœµ íŠ¹í™” BERT</h3>

<p>
ì¼ë°˜ BERTëŠ” Wikipediaë¡œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì— ê¸ˆìœµ í…ìŠ¤íŠ¸ì˜ ë¯¸ë¬˜í•œ ë‰˜ì•™ìŠ¤ë¥¼ ì™„ë²½íˆ í¬ì°©í•˜ì§€ ëª»í•œë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong>FinBERT</strong>(Araci, 2019)ê°€ ê°œë°œë˜ì—ˆë‹¤. FinBERTëŠ” BERTë¥¼ ê¸ˆìœµ ë‰´ìŠ¤ì™€ ê¸°ì—… ê³µì‹œ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ(domain-adaptive pre-training)í•œ ëª¨ë¸ì´ë‹¤.
</p>

<div class="cc">ì½”ë“œ 11-1. FinBERTë¡œ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„±ë¶„ì„</div>
<pre><code><span class="kw">from</span> transformers <span class="kw">import</span> AutoTokenizer, AutoModelForSequenceClassification
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F

<span class="cm"># 1. FinBERT ëª¨ë¸ ë¡œë“œ</span>
model_name = <span class="st">"ProsusAI/finbert"</span>
tokenizer = <span class="fn">AutoTokenizer</span>.<span class="fn">from_pretrained</span>(model_name)
model = <span class="fn">AutoModelForSequenceClassification</span>.<span class="fn">from_pretrained</span>(model_name)

<span class="cm"># 2. ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„</span>
headlines = [
    <span class="st">"Apple reported record quarterly revenue of $123.9 billion"</span>,
    <span class="st">"Tesla shares plunged 12% after missing delivery targets"</span>,
    <span class="st">"Federal Reserve kept interest rates unchanged as expected"</span>,
    <span class="st">"Goldman Sachs sees significant upside potential in AI stocks"</span>,
    <span class="st">"Oil prices fell sharply amid global recession fears"</span>,
    <span class="st">"Company announced massive layoffs affecting 10000 employees"</span>,
]

label_map = {<span class="nu">0</span>: <span class="st">'Positive'</span>, <span class="nu">1</span>: <span class="st">'Negative'</span>, <span class="nu">2</span>: <span class="st">'Neutral'</span>}

<span class="fn">print</span>(<span class="st">f"{'í—¤ë“œë¼ì¸':<55} {'ê°ì„±':<10} {'í™•ë¥ '}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">85</span>)

<span class="kw">for</span> headline <span class="kw">in</span> headlines:
    inputs = <span class="fn">tokenizer</span>(headline, return_tensors=<span class="st">"pt"</span>, padding=<span class="kw">True</span>, truncation=<span class="kw">True</span>, max_length=<span class="nu">512</span>)
    
    <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
        outputs = <span class="fn">model</span>(**inputs)
    
    probs = F.<span class="fn">softmax</span>(outputs.logits, dim=-<span class="nu">1</span>)[<span class="nu">0</span>]
    pred = probs.<span class="fn">argmax</span>().<span class="fn">item</span>()
    
    <span class="fn">print</span>(<span class="st">f"{headline:<55} {label_map[pred]:<10} "
          f"P={probs[0]:.2f} N={probs[1]:.2f} U={probs[2]:.2f}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>FinBERT vs ì „í†µ ë°©ë²• ì„±ëŠ¥ ë¹„êµ</strong></p>
<div style="margin-top:10px">
<table style="font-size:12px">
<thead>
<tr><th>ë°©ë²•</th><th>ëª¨ë¸</th><th>í”¼ì²˜</th><th>ì •í™•ë„ (ê¸ˆìœµ ê°ì„±)</th><th>í•™ìŠµ ì‹œê°„</th></tr>
</thead>
<tbody>
<tr><td>ì‚¬ì „ ê¸°ë°˜</td><td>Loughran-McDonald</td><td>ê°ì„± ì‚¬ì „</td><td>~60%</td><td>ì¦‰ì‹œ</td></tr>
<tr><td>ì „í†µ ML</td><td>NB + TF-IDF</td><td>50,000ì°¨ì› í¬ì†Œ</td><td>~65%</td><td>ì´ˆ</td></tr>
<tr><td>ì „í†µ ML</td><td>LightGBM + TF-IDF</td><td>50,000ì°¨ì› í¬ì†Œ</td><td>~74%</td><td>ë¶„</td></tr>
<tr><td>ì„ë² ë”©</td><td>LightGBM + Doc2Vec</td><td>300ì°¨ì› ë°€ì§‘</td><td>~62%</td><td>ì‹œê°„</td></tr>
<tr style="background:#d4edda"><td><strong>Transformer</strong></td><td><strong>FinBERT</strong></td><td><strong>768ì°¨ì› ë¬¸ë§¥ ì„ë² ë”©</strong></td><td><strong>~87%</strong></td><td><strong>GPU ì‹œê°„</strong></td></tr>
</tbody>
</table>
</div>
</div>

<div class="info">
<p class="ni"><strong>êµì¬ ì—°ë™:</strong> MLAT Ch.16ì˜ ë§ˆì§€ë§‰ ì„¹ì…˜ "New frontiers â€“ pretrained transformer models"ì—ì„œ Jansenì€ Attention ë©”ì»¤ë‹ˆì¦˜, BERT, GPT ë“±ì˜ ìµœì‹  ëª¨ë¸ì„ ì†Œê°œí•œë‹¤. ê·¸ëŠ” "2018 is now considered a turning point for NLP research"ë¼ê³  ê°•ì¡°í•˜ë©°, BERTê°€ GLUE ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì„ ë„˜ì–´ì„  ê²ƒì„ ì–¸ê¸‰í•œë‹¤. R7(ë”¥ëŸ¬ë‹)ê³¼ R8(Transformer)ì—ì„œ ì´ ë‚´ìš©ì„ ë” ê¹Šì´ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.</p>
</div>



<h3>11.5 Self-Attention ê³„ì‚° ì˜ˆì œ: ë‹¨ê³„ë³„ ì›Œí¬ìŠ¤ë£¨</h3>

<p>
Self-Attentionì„ êµ¬ì²´ì ì¸ ìˆ«ìë¡œ ë”°ë¼ê°€ë³´ì. 3ê°œ ë‹¨ì–´ "Fed raises rates"ë¥¼ ì²˜ë¦¬í•œë‹¤ê³  í•˜ì.
</p>

<div class="cc">ì½”ë“œ 11-2. Self-Attention ìˆ˜ë™ ê³„ì‚° (NumPy)</div>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ì…ë ¥: 3ê°œ ë‹¨ì–´ì˜ ì„ë² ë”© (d_model=4ë¡œ ë‹¨ìˆœí™”)</span>
<span class="cm"># ì‹¤ì œ BERTëŠ” d_model=768</span>
X = np.<span class="fn">array</span>([
    [<span class="nu">1.0</span>, <span class="nu">0.5</span>, <span class="nu">0.2</span>, <span class="nu">0.1</span>],  <span class="cm"># "Fed"</span>
    [<span class="nu">0.3</span>, <span class="nu">0.8</span>, <span class="nu">0.9</span>, <span class="nu">0.4</span>],  <span class="cm"># "raises"</span>
    [<span class="nu">0.2</span>, <span class="nu">0.1</span>, <span class="nu">0.7</span>, <span class="nu">1.0</span>],  <span class="cm"># "rates"</span>
])

<span class="cm"># ê°€ì¤‘ì¹˜ í–‰ë ¬ (í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„°)</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
d_k = <span class="nu">3</span>  <span class="cm"># Key/Query ì°¨ì›</span>
W_Q = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, d_k) * <span class="nu">0.5</span>
W_K = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, d_k) * <span class="nu">0.5</span>
W_V = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, d_k) * <span class="nu">0.5</span>

<span class="cm"># Step 1: Q, K, V ê³„ì‚°</span>
Q = X @ W_Q  <span class="cm"># (3, 3)</span>
K = X @ W_K  <span class="cm"># (3, 3)</span>
V = X @ W_V  <span class="cm"># (3, 3)</span>
<span class="fn">print</span>(<span class="st">"Q (Query):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(Q, <span class="nu">3</span>))

<span class="cm"># Step 2: Attention Score = Q @ K^T / sqrt(d_k)</span>
scores = Q @ K.T / np.<span class="fn">sqrt</span>(d_k)
<span class="fn">print</span>(<span class="st">f"\nAttention Scores (before softmax):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(scores, <span class="nu">3</span>))

<span class="cm"># Step 3: Softmax â†’ Attention Weights</span>
<span class="kw">def</span> <span class="fn">softmax</span>(x):
    exp_x = np.<span class="fn">exp</span>(x - np.<span class="fn">max</span>(x, axis=-<span class="nu">1</span>, keepdims=<span class="kw">True</span>))
    <span class="kw">return</span> exp_x / exp_x.<span class="fn">sum</span>(axis=-<span class="nu">1</span>, keepdims=<span class="kw">True</span>)

weights = <span class="fn">softmax</span>(scores)
<span class="fn">print</span>(<span class="st">f"\nAttention Weights (after softmax):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(weights, <span class="nu">3</span>))
<span class="fn">print</span>(<span class="st">f"\nê° í–‰ì˜ í•© = {weights.sum(axis=1)}"</span>)  <span class="cm"># ëª¨ë‘ 1.0</span>

<span class="cm"># Step 4: Output = Weights @ V</span>
output = weights @ V
<span class="fn">print</span>(<span class="st">f"\nOutput (ë¬¸ë§¥ ë°˜ì˜ëœ í‘œí˜„):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(output, <span class="nu">3</span>))

<span class="cm"># í•´ì„: "Fed"ì˜ ì¶œë ¥ ë²¡í„°ëŠ” "raises"ì™€ "rates"ì˜ ì •ë³´ë¥¼ í¬í•¨í•œë‹¤</span>
<span class="cm"># weights[0]ì„ ë³´ë©´ "Fed"ê°€ ì–´ë–¤ ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤</span>
<span class="fn">print</span>(<span class="st">f"\n'Fed'ì˜ Attention ë¶„í¬: Fed={weights[0,0]:.3f}, raises={weights[0,1]:.3f}, rates={weights[0,2]:.3f}"</span>)</code></pre>

<!-- Attention ì‹œê°í™” ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:#f0f4f8;border-radius:10px;border:1px solid #cfd8dc">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:15px;font-size:14px;color:#2c3e50">Self-Attention ì‹œê°í™”: "Fed raises rates"</p>
<div style="display:flex;justify-content:center;gap:40px;flex-wrap:wrap">
<div style="text-align:center">
<div style="font-weight:bold;font-size:13px;color:#1976d2;margin-bottom:10px">Fedê°€ ë³´ëŠ” ì„¸ê³„</div>
<div style="display:flex;gap:8px;justify-content:center">
<div style="padding:8px 14px;border-radius:8px;background:#1976d2;color:#fff;font-size:12px;opacity:0.9">Fed <span style="font-size:10px">(0.45)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#1976d2;color:#fff;font-size:12px;opacity:0.6">raises <span style="font-size:10px">(0.30)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#1976d2;color:#fff;font-size:12px;opacity:0.4">rates <span style="font-size:10px">(0.25)</span></div>
</div>
</div>
<div style="text-align:center">
<div style="font-weight:bold;font-size:13px;color:#2e7d32;margin-bottom:10px">ratesê°€ ë³´ëŠ” ì„¸ê³„</div>
<div style="display:flex;gap:8px;justify-content:center">
<div style="padding:8px 14px;border-radius:8px;background:#2e7d32;color:#fff;font-size:12px;opacity:0.3">Fed <span style="font-size:10px">(0.15)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#2e7d32;color:#fff;font-size:12px;opacity:0.7">raises <span style="font-size:10px">(0.40)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#2e7d32;color:#fff;font-size:12px;opacity:0.9">rates <span style="font-size:10px">(0.45)</span></div>
</div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#777;margin-top:12px">
ê° ë‹¨ì–´ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ attention weightë¥¼ ê°–ëŠ”ë‹¤.<br>
"rates"ëŠ” "raises"ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤ â€” "ê¸ˆë¦¬ë¥¼ ì˜¬ë¦°ë‹¤"ëŠ” ë¬¸ë§¥ì„ í¬ì°©í•˜ëŠ” ê²ƒì´ë‹¤.
</p>
</div>

<h3>11.6 Multi-Head Attention: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ë³´ê¸°</h3>

<p>
Single-head attentionì€ í•˜ë‚˜ì˜ ê´€ì ì—ì„œë§Œ ë¬¸ë§¥ì„ íŒŒì•…í•œë‹¤. í•˜ì§€ë§Œ ì–¸ì–´ì—ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ê´€ê³„ê°€ ë™ì‹œì— ì¡´ì¬í•œë‹¤. 
"The Fed raised rates because inflation was persistent"ì—ì„œ:
</p>

<ul>
<li><strong>êµ¬ë¬¸ì  ê´€ê³„:</strong> "Fed" â†’ "raised" (ì£¼ì–´-ë™ì‚¬)</li>
<li><strong>ì˜ë¯¸ì  ê´€ê³„:</strong> "rates" â†’ "inflation" (ì¸ê³¼)</li>
<li><strong>ê°ì„±ì  ê´€ê³„:</strong> "raised" + "persistent" â†’ ë§¤íŒŒì (hawkish)</li>
</ul>

<p>
<strong>Multi-Head Attention</strong>ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. \(h\)ê°œì˜ ë…ë¦½ì ì¸ attention headë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³ , 
ê²°ê³¼ë¥¼ ì—°ê²°(concatenate)í•œ ë’¤ ì„ í˜• ë³€í™˜í•œë‹¤:
</p>

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \cdot W^O$$

<p>
ì—¬ê¸°ì„œ ê° headëŠ” ë…ë¦½ì ì¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤:
</p>

$$\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$

<p>
BERT-baseëŠ” \(h = 12\)ê°œì˜ headë¥¼ ì‚¬ìš©í•œë‹¤. ê° headì˜ ì°¨ì›ì€ \(d_k = d_{\text{model}} / h = 768 / 12 = 64\)ì´ë‹¤. 
12ê°œì˜ headê°€ ê°ê° ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•œë‹¤ â€” ì–´ë–¤ headëŠ” êµ¬ë¬¸ì„, ì–´ë–¤ headëŠ” ì˜ë¯¸ë¥¼, ì–´ë–¤ headëŠ” ìœ„ì¹˜ ê´€ê³„ë¥¼ í¬ì°©í•œë‹¤.
</p>

<div class="def">
<p class="ni"><strong>BERT ì•„í‚¤í…ì²˜ í•µì‹¬ ìˆ«ì (BERT-base)</strong></p>
<ul style="margin-top:8px">
<li>ë ˆì´ì–´ ìˆ˜: 12</li>
<li>Hidden size (d_model): 768</li>
<li>Attention heads: 12</li>
<li>Head ì°¨ì› (d_k): 64</li>
<li>ì´ íŒŒë¼ë¯¸í„°: 110M</li>
<li>ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: 512 í† í°</li>
<li>ì‚¬ì „í•™ìŠµ ë°ì´í„°: BookCorpus + English Wikipedia (3.3B ë‹¨ì–´)</li>
</ul>
</div>

<!-- ==================== Ch.12 ==================== -->
<h2 id="ch12">Chapter 12. ê¸ˆìœµ NLP ì‹¤ì „ â€” SEC ê³µì‹œì—ì„œ ì•ŒíŒŒ ì¶”ì¶œ</h2>

<h3>12.1 SEC 10-K ê³µì‹œ ë¶„ì„ íŒŒì´í”„ë¼ì¸</h3>

<p>
MLAT Ch.16ì—ì„œ Jansenì€ SEC 10-K ì—°ê°„ ë³´ê³ ì„œ 22,000ê±´ì„ ë¶„ì„í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ì— í™œìš©í•˜ëŠ” ì‹¤ì „ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤. ì´ê²ƒì´ NLPë¥¼ ì•Œê³ ë¦¬ì¦˜ íŠ¸ë ˆì´ë”©ì— ì ìš©í•˜ëŠ” ê°€ì¥ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‹¤.
</p>

<!-- SEC íŒŒì´í”„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#f3e5f5,#e1bee7);border-radius:12px;border:1px solid #ce93d8">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#6a1b9a">ğŸ“‹ SEC 10-K â†’ ì£¼ê°€ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (MLAT Ch.16)</p>
<div style="display:flex;flex-direction:column;gap:8px;max-width:600px;margin:0 auto;font-size:12px">

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">1</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>ë°ì´í„° ìˆ˜ì§‘:</strong> SEC EDGARì—ì„œ 10-K ê³µì‹œ ë‹¤ìš´ë¡œë“œ (22,000ê±´, 2013-2016)<br>
<span style="color:#888;font-size:11px">í•µì‹¬ ì„¹ì…˜: Item 1(ì‚¬ì—…), Item 1A(ë¦¬ìŠ¤í¬), Item 7(ê²½ì˜ì§„ ë…¼ì˜), Item 7A(ì‹œì¥ ë¦¬ìŠ¤í¬)</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">2</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>ì „ì²˜ë¦¬:</strong> spaCyë¡œ í† í°í™” + êµ¬ë¬¸ íƒì§€ (Gensim Phrases)<br>
<span style="color:#888;font-size:11px">ê²°ê³¼: 201,000ê°œ í† í° ì–´íœ˜, bigram í¬í•¨ (common_stock, interest_rates ë“±)</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">3</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>Word2Vec í•™ìŠµ:</strong> Skip-gram, 300ì°¨ì›, window=3, min_count=50<br>
<span style="color:#888;font-size:11px">ìµœì  ì„¤ì •: negative sampling > hierarchical softmax, vector_size=600ì´ ìµœê³  ì„±ëŠ¥</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">4</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>ë¼ë²¨ë§:</strong> ê³µì‹œ ì œì¶œ í›„ 1ê°œì›” ìˆ˜ìµë¥ ë¡œ ë¼ë²¨ ë¶€ì—¬<br>
<span style="color:#888;font-size:11px">ì•½ 3,000ê°œ ê¸°ì—…, 11,000ê±´ ê³µì‹œì— ëŒ€í•´ ì£¼ê°€ ë°ì´í„° ë§¤ì¹­</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">5</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>ì˜ˆì¸¡ ëª¨ë¸:</strong> ë¬¸ì„œ ì„ë² ë”© â†’ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ìˆ˜ìµë¥  ì˜ˆì¸¡<br>
<span style="color:#888;font-size:11px">R7(ë”¥ëŸ¬ë‹)ì—ì„œ ì´ íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•  ì˜ˆì •</span>
</div>
</div>

</div>
</div>

<h3>12.2 ì‹¤ì „ ì½”ë“œ: ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± â†’ ë§¤ë§¤ ì‹œê·¸ë„</h3>

<p>
ì´ë¡ ì„ ì¢…í•©í•˜ì—¬, ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ë¥¼ ë§¤ë§¤ ì‹œê·¸ë„ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ë³´ì. ì´ê²ƒì´ ì´ë²ˆ ë¼ìš´ë“œì˜ ìµœì¢… ì‹¤ì „ í”„ë¡œì íŠ¸ë‹¤.
</p>

<div class="cc">ì½”ë“œ 12-1. ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± â†’ ë§¤ë§¤ ì‹œê·¸ë„ ì „ì²´ íŒŒì´í”„ë¼ì¸</div>
<pre><code><span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">from</span> transformers <span class="kw">import</span> pipeline
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="cm"># ============================================</span>
<span class="cm"># STEP 1: ê°ì„±ë¶„ì„ ëª¨ë¸ ë¡œë“œ (FinBERT)</span>
<span class="cm"># ============================================</span>
sentiment_pipeline = <span class="fn">pipeline</span>(
    <span class="st">"sentiment-analysis"</span>,
    model=<span class="st">"ProsusAI/finbert"</span>,
    tokenizer=<span class="st">"ProsusAI/finbert"</span>
)

<span class="cm"># ============================================</span>
<span class="cm"># STEP 2: ë‰´ìŠ¤ ë°ì´í„° ê°ì„± ìŠ¤ì½”ì–´ë§</span>
<span class="cm"># ============================================</span>
<span class="cm"># ì‹¤ì œë¡œëŠ” ë‰´ìŠ¤ API(Bloomberg, Reuters, NewsAPI)ì—ì„œ ìˆ˜ì§‘</span>
news_data = pd.<span class="fn">DataFrame</span>({
    <span class="st">'date'</span>: pd.<span class="fn">date_range</span>(<span class="st">'2024-01-01'</span>, periods=<span class="nu">20</span>, freq=<span class="st">'B'</span>),
    <span class="st">'headline'</span>: [
        <span class="st">"Apple beats earnings expectations with strong iPhone sales"</span>,
        <span class="st">"Fed signals potential rate cuts boosting market sentiment"</span>,
        <span class="st">"Tech stocks rally on AI optimism"</span>,
        <span class="st">"Inflation data comes in hotter than expected"</span>,
        <span class="st">"Major bank reports significant trading losses"</span>,
        <span class="cm"># ... (ì‹¤ì œë¡œëŠ” ìˆ˜ë°±~ìˆ˜ì²œ ê°œ)</span>
    ] * <span class="nu">4</span>  <span class="cm"># ì˜ˆì‹œìš© ë°˜ë³µ</span>
})

<span class="cm"># ê°ì„± ì ìˆ˜ ê³„ì‚°</span>
<span class="kw">def</span> <span class="fn">get_sentiment_score</span>(text):
    result = sentiment_pipeline(text, truncation=<span class="kw">True</span>)[<span class="nu">0</span>]
    label = result[<span class="st">'label'</span>]
    score = result[<span class="st">'score'</span>]
    <span class="kw">if</span> label == <span class="st">'positive'</span>:
        <span class="kw">return</span> score
    <span class="kw">elif</span> label == <span class="st">'negative'</span>:
        <span class="kw">return</span> -score
    <span class="kw">else</span>:
        <span class="kw">return</span> <span class="nu">0</span>

news_data[<span class="st">'sentiment'</span>] = news_data[<span class="st">'headline'</span>].<span class="fn">apply</span>(get_sentiment_score)

<span class="cm"># ì¼ë³„ í‰ê·  ê°ì„± ì ìˆ˜</span>
daily_sentiment = news_data.<span class="fn">groupby</span>(<span class="st">'date'</span>)[<span class="st">'sentiment'</span>].<span class="fn">mean</span>()

<span class="cm"># ============================================</span>
<span class="cm"># STEP 3: ì£¼ê°€ ë°ì´í„°ì™€ ê²°í•©</span>
<span class="cm"># ============================================</span>
spy = yf.<span class="fn">download</span>(<span class="st">'SPY'</span>, start=<span class="st">'2024-01-01'</span>, end=<span class="st">'2024-02-01'</span>)
spy[<span class="st">'return'</span>] = spy[<span class="st">'Adj Close'</span>].<span class="fn">pct_change</span>()

<span class="cm"># ê°ì„± ì ìˆ˜ì™€ ìˆ˜ìµë¥  ê²°í•©</span>
combined = pd.<span class="fn">DataFrame</span>({
    <span class="st">'sentiment'</span>: daily_sentiment,
    <span class="st">'return'</span>: spy[<span class="st">'return'</span>]
}).<span class="fn">dropna</span>()

<span class="cm"># ============================================</span>
<span class="cm"># STEP 4: ë§¤ë§¤ ì‹œê·¸ë„ ìƒì„±</span>
<span class="cm"># ============================================</span>
<span class="cm"># ì „ëµ: ê°ì„± > 0ì´ë©´ Long, < 0ì´ë©´ Short</span>
combined[<span class="st">'signal'</span>] = np.<span class="fn">where</span>(combined[<span class="st">'sentiment'</span>] > <span class="nu">0</span>, <span class="nu">1</span>, -<span class="nu">1</span>)
combined[<span class="st">'strategy_return'</span>] = combined[<span class="st">'signal'</span>].<span class="fn">shift</span>(<span class="nu">1</span>) * combined[<span class="st">'return'</span>]

<span class="cm"># ============================================</span>
<span class="cm"># STEP 5: ì„±ê³¼ í‰ê°€</span>
<span class="cm"># ============================================</span>
cumulative_market = (<span class="nu">1</span> + combined[<span class="st">'return'</span>]).<span class="fn">cumprod</span>()
cumulative_strategy = (<span class="nu">1</span> + combined[<span class="st">'strategy_return'</span>]).<span class="fn">cumprod</span>()

sharpe = combined[<span class="st">'strategy_return'</span>].<span class="fn">mean</span>() / combined[<span class="st">'strategy_return'</span>].<span class="fn">std</span>() * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)

<span class="fn">print</span>(<span class="st">f"ì‹œì¥ ìˆ˜ìµë¥ : {(cumulative_market.iloc[-1] - 1):.2%}"</span>)
<span class="fn">print</span>(<span class="st">f"ì „ëµ ìˆ˜ìµë¥ : {(cumulative_strategy.iloc[-1] - 1):.2%}"</span>)
<span class="fn">print</span>(<span class="st">f"ìƒ¤í”„ ë¹„ìœ¨: {sharpe:.2f}"</span>)

<span class="cm"># ì‹œê°í™”</span>
fig, (ax1, ax2) = plt.<span class="fn">subplots</span>(<span class="nu">2</span>, <span class="nu">1</span>, figsize=(<span class="nu">12</span>, <span class="nu">8</span>), sharex=<span class="kw">True</span>)

ax1.<span class="fn">plot</span>(cumulative_market.index, cumulative_market, label=<span class="st">'Market (SPY)'</span>, color=<span class="st">'gray'</span>)
ax1.<span class="fn">plot</span>(cumulative_strategy.index, cumulative_strategy, label=<span class="st">'Sentiment Strategy'</span>, color=<span class="st">'#1976d2'</span>)
ax1.<span class="fn">legend</span>()
ax1.<span class="fn">set_ylabel</span>(<span class="st">'Cumulative Return'</span>)
ax1.<span class="fn">set_title</span>(<span class="st">'Sentiment-Based Trading Strategy vs Market'</span>)

ax2.<span class="fn">bar</span>(daily_sentiment.index, daily_sentiment.values,
       color=[<span class="st">'#2e7d32'</span> <span class="kw">if</span> x > <span class="nu">0</span> <span class="kw">else</span> <span class="st">'#c62828'</span> <span class="kw">for</span> x <span class="kw">in</span> daily_sentiment.values],
       alpha=<span class="nu">0.7</span>)
ax2.<span class="fn">axhline</span>(y=<span class="nu">0</span>, color=<span class="st">'black'</span>, linewidth=<span class="nu">0.5</span>)
ax2.<span class="fn">set_ylabel</span>(<span class="st">'Daily Sentiment Score'</span>)
ax2.<span class="fn">set_title</span>(<span class="st">'News Sentiment Over Time'</span>)

plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</code></pre>

<h3>12.3 ëŒ€ì•ˆ ë°ì´í„°: ë‰´ìŠ¤ ë„ˆë¨¸ì˜ ì„¸ê³„</h3>

<p>
MLAT Ch.3 "Alternative Data for Finance"ì—ì„œ ë‹¤ë£¨ëŠ” ëŒ€ì•ˆ ë°ì´í„°ëŠ” ì „í†µì ì¸ ê¸ˆìœµ ë°ì´í„°(ê°€ê²©, ì¬ë¬´ì œí‘œ)ë¥¼ ë„˜ì–´ì„œëŠ” ì •ë³´ ì†ŒìŠ¤ë‹¤. NLPëŠ” ì´ëŸ° ëŒ€ì•ˆ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ ë„êµ¬ë‹¤.
</p>

<div class="tc">í‘œ 12-1. ëŒ€ì•ˆ ë°ì´í„° ì†ŒìŠ¤ì™€ NLP í™œìš©</div>
<table>
<thead>
<tr><th>ë°ì´í„° ì†ŒìŠ¤</th><th>ì˜ˆì‹œ</th><th>NLP ê¸°ë²•</th><th>ì•ŒíŒŒ ì†ŒìŠ¤</th></tr>
</thead>
<tbody>
<tr><td><strong>ì†Œì…œ ë¯¸ë””ì–´</strong></td><td>Twitter/X, Reddit, StockTwits</td><td>ê°ì„±ë¶„ì„, íŠ¸ë Œë“œ ê°ì§€</td><td>êµ°ì¤‘ ì‹¬ë¦¬ ì„ í–‰ ì§€í‘œ</td></tr>
<tr><td><strong>ìœ„ì„± ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°</strong></td><td>ì£¼ì°¨ì¥ ì°¨ëŸ‰ ìˆ˜, ìœ ì¡°ì„  ìœ„ì¹˜</td><td>ì´ë¯¸ì§€ ìº¡ì…˜ ë¶„ì„</td><td>ì‹¤ì  ì„ í–‰ ì¶”ì •</td></tr>
<tr><td><strong>íŠ¹í—ˆ ë°ì´í„°</strong></td><td>USPTO íŠ¹í—ˆ ì¶œì›</td><td>í† í”½ ëª¨ë¸ë§, ìœ ì‚¬ë„</td><td>ê¸°ìˆ  í˜ì‹  ì¶”ì </td></tr>
<tr><td><strong>ì±„ìš© ê³µê³ </strong></td><td>LinkedIn, Indeed</td><td>í‚¤ì›Œë“œ ì¶”ì¶œ, íŠ¸ë Œë“œ</td><td>ê¸°ì—… ì„±ì¥ ì‹ í˜¸</td></tr>
<tr><td><strong>ì›¹ íŠ¸ë˜í”½</strong></td><td>SimilarWeb, Google Trends</td><td>ì‹œê³„ì—´ + NLP</td><td>ì†Œë¹„ì ê´€ì‹¬ ì¶”ì </td></tr>
<tr><td><strong>ì •ë¶€ ë¬¸ì„œ</strong></td><td>ì—°ì¤€ ì˜ì‚¬ë¡, ì˜íšŒ ê¸°ë¡</td><td>ê°ì„±ë¶„ì„, í† í”½ ë³€í™”</td><td>ì •ì±… ë°©í–¥ ì˜ˆì¸¡</td></tr>
</tbody>
</table>

<div class="warn">
<p class="ni"><strong>âš ï¸ ê¸ˆìœµ NLPì˜ í˜„ì‹¤ì  ë„ì „ (MLAT Ch.16 ê²°ë¡ )</strong></p>
<p class="ni" style="margin-top:8px">Jansenì€ Ch.16 ë§ˆì§€ë§‰ì—ì„œ ê¸ˆìœµ NLPì˜ í˜„ì‹¤ì  í•œê³„ë¥¼ ì†”ì§í•˜ê²Œ ì§€ì í•œë‹¤:</p>
<ul>
<li><strong>ë¼ë²¨ ë°ì´í„° ë¶€ì¡±:</strong> ê¸ˆìœµ ê°ì„± ë¼ë²¨ë§ì€ ë¹„ìš©ì´ ë†’ê³  ì£¼ê´€ì ì´ë‹¤. ì£¼ê°€ ìˆ˜ìµë¥ ì„ ë¼ë²¨ë¡œ ì‚¬ìš©í•˜ë©´ ë…¸ì´ì¦ˆê°€ ë§ë‹¤.</li>
<li><strong>ë¬¸ë§¥ì˜ ë³µì¡ì„±:</strong> ê°™ì€ ë¬¸ì¥ì´ ì‹œì¥ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. "inflation is rising"ì€ 2020ë…„ê³¼ 2023ë…„ì— ì „í˜€ ë‹¤ë¥¸ ì‹œê·¸ë„ì´ë‹¤.</li>
<li><strong>ë‹¤ì¤‘ íƒ€ê²Ÿ:</strong> í•˜ë‚˜ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—¬ëŸ¬ ê¸°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆê³ , ê¸ì •/ë¶€ì •ì´ í˜¼ì¬í•  ìˆ˜ ìˆë‹¤.</li>
<li><strong>ì‹œê°„ ë¯¼ê°ì„±:</strong> ë‰´ìŠ¤ì˜ ê°€ì¹˜ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë©´ ê¸‰ê²©íˆ ê°ì†Œí•œë‹¤. ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ì²˜ë¦¬ ì†ë„ê°€ í•„ìš”í•˜ë‹¤.</li>
</ul>
</div>



<!-- ==================== Ch.13 ==================== -->
<h2 id="ch13">Chapter 13. ğŸ”„ í”¼ë“œë°± ì„¸ì…˜ + Quiz + ë¯¸ë‹ˆ í”„ë¡œì íŠ¸</h2>

<div style="margin:20px 0;padding:20px 25px;background:linear-gradient(135deg,#e8eaf6,#e3f2fd);border-radius:12px;border-left:5px solid #3f51b5">
<p class="ni" style="font-size:15px;font-weight:bold;color:#283593;margin-bottom:10px">ğŸ”„ Round 4~6 í”¼ë“œë°± ì„¸ì…˜</p>
<p class="ni" style="color:#37474f;line-height:1.8">
ë§ˆìŠ¤í„° í”Œëœì— ë”°ë¼ 3ë¼ìš´ë“œë§ˆë‹¤(R3, R6, R9) í”¼ë“œë°± ì„¸ì…˜ì„ ì§„í–‰í•œë‹¤. ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” <strong>R4(ì§€ë„í•™ìŠµ)</strong>, 
<strong>R5(ë¹„ì§€ë„í•™ìŠµ + ì‹œê³„ì—´)</strong>, <strong>R6(NLP + ê°ì„±ë¶„ì„)</strong>ì˜ í•µì‹¬ ê°œë…ì„ í†µí•©ì ìœ¼ë¡œ ì ê²€í•œë‹¤.
</p>
<p class="ni" style="color:#37474f;line-height:1.8;margin-top:8px">
R1~R3 í”¼ë“œë°± ì„¸ì…˜ì´ "ê¸°ì´ˆ ì²´ë ¥"ì„ ì ê²€í–ˆë‹¤ë©´, ì´ë²ˆ R4~R6 í”¼ë“œë°± ì„¸ì…˜ì€ <strong>"ML í•µì‹¬ ë¬´ê¸°"</strong>ì˜ 
ìˆ™ë ¨ë„ë¥¼ ì ê²€í•œë‹¤. ì§€ë„í•™ìŠµ(ì˜ˆì¸¡), ë¹„ì§€ë„í•™ìŠµ(êµ¬ì¡° ë°œê²¬), NLP(í…ìŠ¤íŠ¸ ì‹œê·¸ë„) â€” ì´ ì„¸ ê°€ì§€ ë¬´ê¸°ë¥¼ ììœ ìì¬ë¡œ 
ì“¸ ìˆ˜ ìˆì–´ì•¼ R7~R10ì˜ ë”¥ëŸ¬ë‹ê³¼ HFT ì‹œìŠ¤í…œ êµ¬ì¶•ì— ì§„ì…í•  ìˆ˜ ìˆë‹¤.
</p>
</div>

<h3>13.1 R4~R6 í†µí•© ë³µìŠµ í€´ì¦ˆ (15ë¬¸ì œ)</h3>

<p>
ì•„ë˜ í€´ì¦ˆëŠ” R4(ì§€ë„í•™ìŠµ), R5(ë¹„ì§€ë„í•™ìŠµ + ì‹œê³„ì—´), R6(NLP + ê°ì„±ë¶„ì„)ì˜ í•µì‹¬ ê°œë…ì„ í†µí•©ì ìœ¼ë¡œ ì ê²€í•œë‹¤. 
ê° ë¬¸ì œì— ëŒ€í•´ <strong>ìµœì†Œ 3ë¬¸ì¥ ì´ìƒ</strong>ìœ¼ë¡œ ë‹µë³€í•˜ë¼. ë‹¨ìˆœ ì•”ê¸°ê°€ ì•„ë‹ˆë¼ <strong>ì™œ ê·¸ëŸ°ì§€</strong>ë¥¼ 
ì„¤ëª…í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
</p>

<h4>Part A: ì§€ë„í•™ìŠµ (R4) â€” 5ë¬¸ì œ</h4>

<div class="def">
<p class="ni"><strong>Q1. í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„</strong></p>
<p class="ni" style="margin-top:5px">ì„ í˜•íšŒê·€ì™€ Random Forestë¥¼ ë¹„êµí•  ë•Œ, ê°ê°ì˜ í¸í–¥(bias)ê³¼ ë¶„ì‚°(variance)ì€ 
ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€? ê¸ˆìœµ ìˆ˜ìµë¥  ì˜ˆì¸¡ì—ì„œ ì–´ë–¤ ëª¨ë¸ì´ ë” ì í•©í•œ ìƒí™©ì„ ê°ê° ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ì„ í˜•íšŒê·€ëŠ” ë†’ì€ í¸í–¥/ë‚®ì€ ë¶„ì‚°, RFëŠ” ë‚®ì€ í¸í–¥/ë†’ì€ ë¶„ì‚°. 
ê¸ˆìœµ ë°ì´í„°ì˜ ë‚®ì€ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„(SNR)ë¥¼ ê³ ë ¤í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q2. ì •ê·œí™”ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸</strong></p>
<p class="ni" style="margin-top:5px">Ridge(L2)ì™€ Lasso(L1) ì •ê·œí™”ì˜ ì œì•½ ì¡°ê±´ì„ 2ì°¨ì› í‰ë©´ì—ì„œ ê¸°í•˜í•™ì ìœ¼ë¡œ 
ì„¤ëª…í•˜ë¼. ì™œ LassoëŠ” í”¼ì²˜ ì„ íƒ(feature selection) íš¨ê³¼ê°€ ìˆê³  RidgeëŠ” ì—†ëŠ”ê°€?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: L1 ì œì•½ì€ ë‹¤ì´ì•„ëª¬ë“œ(ë§ˆë¦„ëª¨), L2 ì œì•½ì€ ì›. 
ë“±ê³ ì„ ê³¼ì˜ ì ‘ì  ìœ„ì¹˜ë¥¼ ìƒê°í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q3. XGBoost vs LightGBM</strong></p>
<p class="ni" style="margin-top:5px">XGBoostëŠ” level-wise íŠ¸ë¦¬ ì„±ì¥, LightGBMì€ leaf-wise íŠ¸ë¦¬ ì„±ì¥ì„ ì‚¬ìš©í•œë‹¤. 
ê°ê°ì˜ ì¥ë‹¨ì ì„ ì„¤ëª…í•˜ê³ , ê¸ˆìœµ ë°ì´í„°(ìˆ˜ë§Œ ê°œ í”¼ì²˜, ìˆ˜ë°±ë§Œ í–‰)ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” íš¨ìœ¨ì ì¸ì§€ ê·¼ê±°ë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: leaf-wiseëŠ” ë” ê¹Šì€ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ì–´ ê³¼ì í•© ìœ„í—˜ì´ ìˆì§€ë§Œ, 
í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ë‹¤. MLAT Ch.12ì˜ GBM ë¹„êµë¥¼ ì°¸ê³ í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q4. ì‹œê³„ì—´ êµì°¨ê²€ì¦</strong></p>
<p class="ni" style="margin-top:5px">ê¸ˆìœµ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì¼ë°˜ì ì¸ K-Fold êµì°¨ê²€ì¦ì„ ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ì´ìœ ë¥¼ 
ì„¤ëª…í•˜ë¼. TimeSeriesSplitê³¼ Purged K-Foldì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ(look-ahead bias), ìê¸°ìƒê´€, 
embargo ê¸°ê°„. MLAT Ch.7ì˜ êµì°¨ê²€ì¦ ì „ëµì„ ì°¸ê³ í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q5. í˜¼ë™í–‰ë ¬ê³¼ ê¸ˆìœµ ë¹„ìš©</strong></p>
<p class="ni" style="margin-top:5px">ì£¼ê°€ ë°©í–¥ ì˜ˆì¸¡(ìƒìŠ¹/í•˜ë½) ë¶„ë¥˜ ëª¨ë¸ì—ì„œ, False Positive(í•˜ë½ì¸ë° ìƒìŠ¹ìœ¼ë¡œ ì˜ˆì¸¡)ì™€ 
False Negative(ìƒìŠ¹ì¸ë° í•˜ë½ìœ¼ë¡œ ì˜ˆì¸¡)ì˜ ê¸ˆìœµì  ë¹„ìš©ì€ ê°ê° ë¬´ì—‡ì¸ê°€? ì–´ë–¤ ê²ƒì´ ë” ì¹˜ëª…ì ì´ë©°, 
ì´ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì–´ë–¤ í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: FPëŠ” ì†ì‹¤ í¬ì§€ì…˜ ì§„ì…, FNì€ ìˆ˜ìµ ê¸°íšŒ ìƒì‹¤. 
ë¹„ëŒ€ì¹­ ë¹„ìš© â†’ Precision/Recall íŠ¸ë ˆì´ë“œì˜¤í”„, F-beta score.</em></p>
</div>

<h4>Part B: ë¹„ì§€ë„í•™ìŠµ + ì‹œê³„ì—´ (R5) â€” 5ë¬¸ì œ</h4>

<div class="def">
<p class="ni"><strong>Q6. PCAì™€ íŒ©í„° ëª¨ë¸ì˜ ê´€ê³„</strong></p>
<p class="ni" style="margin-top:5px">PCAì˜ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„(PC1)ì´ ì¢…ì¢… "ì‹œì¥ íŒ©í„°"ì™€ ìœ ì‚¬í•œ ì´ìœ ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ 
ì„¤ëª…í•˜ë¼. Fama-French 3íŒ©í„° ëª¨ë¸ì˜ SMB, HMLê³¼ PCA íŒ©í„°ëŠ” ì–´ë–¤ ê´€ê³„ì¸ê°€?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: PC1ì€ ìµœëŒ€ ë¶„ì‚° ë°©í–¥ â†’ ëª¨ë“  ì¢…ëª©ì´ í•¨ê»˜ ì›€ì§ì´ëŠ” 
ì‹œì¥ ë¦¬ìŠ¤í¬. MLAT Ch.13ì˜ PCA íŒ©í„° ë¶„ì„ì„ ì°¸ê³ í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q7. í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ì˜ ë”œë ˆë§ˆ</strong></p>
<p class="ni" style="margin-top:5px">ë¹„ì§€ë„í•™ìŠµì—ëŠ” "ì •ë‹µ"ì´ ì—†ë‹¤. K-Meansì˜ ìµœì  Kë¥¼ ê²°ì •í•  ë•Œ ì—˜ë³´ìš° ë°©ë²•ê³¼ 
ì‹¤ë£¨ì—£ ì ìˆ˜ê°€ ì„œë¡œ ë‹¤ë¥¸ Kë¥¼ ì œì•ˆí•˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ê°€? ê¸ˆìœµì—ì„œì˜ ì‹¤ìš©ì  íŒë‹¨ ê¸°ì¤€ì„ ì œì‹œí•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ë„ë©”ì¸ ì§€ì‹(GICS ì„¹í„° ìˆ˜), í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„±(bootstrap), 
ê²½ì œì  í•´ì„ ê°€ëŠ¥ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œë‹¤.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q8. ARIMA vs GARCHì˜ ì—­í•  ë¶„ë‹´</strong></p>
<p class="ni" style="margin-top:5px">"ìˆ˜ìµë¥ ì˜ í‰ê· ì€ ì˜ˆì¸¡í•˜ê¸° ì–´ë µì§€ë§Œ, ë³€ë™ì„±ì€ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ë‹¤"ëŠ” ê¸ˆìœµì˜ 
ì •í˜•í™”ëœ ì‚¬ì‹¤(stylized fact)ì´ë‹¤. ARIMAê°€ ìˆ˜ìµë¥  ì˜ˆì¸¡ì— ì‹¤íŒ¨í•˜ëŠ” ì´ìœ ì™€, GARCHê°€ ë³€ë™ì„± ì˜ˆì¸¡ì— 
ì„±ê³µí•˜ëŠ” ì´ìœ ë¥¼ ACF ê´€ì ì—ì„œ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ìˆ˜ìµë¥ ì˜ ACF â‰ˆ 0 (íš¨ìœ¨ì  ì‹œì¥), ìˆ˜ìµë¥ Â²ì˜ ACF > 0 
(ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§). MLAT Ch.9ë¥¼ ì°¸ê³ í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q9. ì •ìƒì„±ê³¼ ê³µì ë¶„</strong></p>
<p class="ni" style="margin-top:5px">ë‘ ì¢…ëª© A, Bì˜ ì£¼ê°€ê°€ ê°ê° ë¹„ì •ìƒ(non-stationary)ì´ì§€ë§Œ, A - 0.8Bê°€ 
ì •ìƒ(stationary)ì´ë¼ë©´ ì´ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€? ì´ ê´€ê³„ë¥¼ ì´ìš©í•œ íŠ¸ë ˆì´ë”© ì „ëµì˜ ì´ë¦„ê³¼ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ê³µì ë¶„(cointegration), í˜ì–´ íŠ¸ë ˆì´ë”©(pairs trading), 
í‰ê·  íšŒê·€(mean reversion). MLAT Ch.9ì˜ ê³µì ë¶„ ê²€ì •ì„ ì°¸ê³ í•˜ë¼.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q10. t-SNEì˜ í•¨ì •</strong></p>
<p class="ni" style="margin-top:5px">t-SNE ì‹œê°í™”ì—ì„œ í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ëŠ” ì˜ë¯¸ê°€ ìˆëŠ”ê°€? perplexity íŒŒë¼ë¯¸í„°ë¥¼ 
5ì—ì„œ 50ìœ¼ë¡œ ë°”ê¾¸ë©´ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ê°€? t-SNE ê²°ê³¼ë§Œìœ¼ë¡œ íˆ¬ì ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦¬ë©´ ì•ˆ ë˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: t-SNEëŠ” ì§€ì—­ êµ¬ì¡°ë§Œ ë³´ì¡´, ì „ì—­ ê±°ë¦¬ëŠ” ì™œê³¡. 
ë¹„ê²°ì •ì (stochastic) ì•Œê³ ë¦¬ì¦˜ì´ë¯€ë¡œ ë§¤ë²ˆ ê²°ê³¼ê°€ ë‹¤ë¥´ë‹¤.</em></p>
</div>

<h4>Part C: NLP + ê°ì„±ë¶„ì„ (R6) â€” 5ë¬¸ì œ</h4>

<div class="def">
<p class="ni"><strong>Q11. BoW vs Word2Vecì˜ ê·¼ë³¸ì  ì°¨ì´</strong></p>
<p class="ni" style="margin-top:5px">Bag-of-Words(TF-IDF í¬í•¨)ì™€ Word2Vecì˜ ê°€ì¥ ê·¼ë³¸ì ì¸ ì°¨ì´ë¥¼ "ì˜ë¯¸ì˜ í‘œí˜„" 
ê´€ì ì—ì„œ ì„¤ëª…í•˜ë¼. "ê¸ˆë¦¬ ì¸ìƒ"ê³¼ "ê¸°ì¤€ê¸ˆë¦¬ ìƒí–¥"ì´ë¼ëŠ” ë‘ í‘œí˜„ì„ ê° ëª¨ë¸ì´ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ë¹„êµí•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: BoWëŠ” ë‹¨ì–´ ë…ë¦½ ê°€ì •(orthogonal), Word2Vecì€ 
ë¶„í¬ ê°€ì„¤(distributional hypothesis). ë™ì˜ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì˜ ì°¨ì´.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q12. TF-IDFì˜ ìˆ˜í•™ì  ì§ê´€</strong></p>
<p class="ni" style="margin-top:5px">TF-IDFì—ì„œ IDF(ì—­ë¬¸ì„œë¹ˆë„)ê°€ logë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ë¼. 
ë§Œì•½ log ì—†ì´ N/df(t)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ”ê°€? 
ê¸ˆìœµ ë‰´ìŠ¤ ì½”í¼ìŠ¤ì—ì„œ "ì‹œì¥"ì´ë¼ëŠ” ë‹¨ì–´ì˜ TF-IDF ê°’ì´ ë‚®ì€ ì´ìœ ë¥¼ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: logëŠ” ìŠ¤ì¼€ì¼ ì••ì¶•. "ì‹œì¥"ì€ ê±°ì˜ ëª¨ë“  ê¸ˆìœµ ë‰´ìŠ¤ì— 
ë“±ì¥í•˜ë¯€ë¡œ df(t) â‰ˆ N â†’ IDF â‰ˆ 0.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q13. LDA í† í”½ ëª¨ë¸ë§ì˜ í•´ì„</strong></p>
<p class="ni" style="margin-top:5px">LDAì—ì„œ í† í”½ ìˆ˜ Kë¥¼ ê²°ì •í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€? coherence scoreê°€ ë†’ë‹¤ê³  
ë°˜ë“œì‹œ ì¢‹ì€ í† í”½ ëª¨ë¸ì¸ê°€? ê¸ˆìœµ í…ìŠ¤íŠ¸ì— LDAë¥¼ ì ìš©í•  ë•Œ ì£¼ì˜í•´ì•¼ í•  ì  3ê°€ì§€ë¥¼ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ê¸ˆìœµ ìš©ì–´ì˜ ë‹¤ì˜ì„±(bank = ì€í–‰/ê°•ë‘‘), ì‹œê°„ì— ë”°ë¥¸ 
í† í”½ ë³€í™”, ì§§ì€ ë¬¸ì„œ(í—¤ë“œë¼ì¸)ì—ì„œì˜ ì„±ëŠ¥ ì €í•˜.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q14. ê°ì„±ì‚¬ì „ vs ML ê°ì„±ë¶„ì„</strong></p>
<p class="ni" style="margin-top:5px">Loughran-McDonald ê¸ˆìœµ ê°ì„±ì‚¬ì „ê³¼ ML ê¸°ë°˜ ê°ì„±ë¶„ì„(ì˜ˆ: FinBERT)ì˜ 
ì¥ë‹¨ì ì„ ë¹„êµí•˜ë¼. "The company's loss narrowed significantly"ë¼ëŠ” ë¬¸ì¥ì„ ê° ë°©ë²•ì´ ì–´ë–»ê²Œ ë¶„ë¥˜í•˜ëŠ”ì§€ 
ì˜ˆì¸¡í•˜ê³ , ì–´ë–¤ ë°©ë²•ì´ ë” ì •í™•í•œì§€ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: ì‚¬ì „ ë°©ì‹ì€ "loss" â†’ ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ì§€ë§Œ, 
ë¬¸ë§¥ìƒ "loss narrowed" = ê¸ì •. FinBERTëŠ” ë¬¸ë§¥ì„ ì´í•´í•œë‹¤.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q15. NLP íŒŒì´í”„ë¼ì¸ í†µí•© ì„¤ê³„</strong></p>
<p class="ni" style="margin-top:5px">SEC 10-K ê³µì‹œ ë¬¸ì„œì—ì„œ ë§¤ë§¤ ì‹œê·¸ë„ì„ ì¶”ì¶œí•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•˜ë¼. 
(1) ë°ì´í„° ìˆ˜ì§‘, (2) ì „ì²˜ë¦¬, (3) í”¼ì²˜ ì¶”ì¶œ, (4) ëª¨ë¸ë§, (5) ì‹œê·¸ë„ ìƒì„±ì˜ ê° ë‹¨ê³„ì—ì„œ 
ì–´ë–¤ ê¸°ë²•ì„ ì‚¬ìš©í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ë¼.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>íŒíŠ¸: EDGAR API â†’ spaCy ì „ì²˜ë¦¬ â†’ TF-IDF + Doc2Vec + 
FinBERT â†’ ê°ì„± ì ìˆ˜ + í† í”½ ë³€í™” â†’ ë¡±/ìˆ ì‹œê·¸ë„. MLAT Ch.16ì˜ íŒŒì´í”„ë¼ì¸ì„ ì°¸ê³ í•˜ë¼.</em></p>
</div>

<h3>13.2 R4~R6 í†µí•© ìê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸</h3>

<div class="tc">í‘œ 13-1. Round 4 (ì§€ë„í•™ìŠµ) ì²´í¬ë¦¬ìŠ¤íŠ¸</div>
<table>
<thead>
<tr><th>âœ“</th><th>í•­ëª©</th></tr>
</thead>
<tbody>
<tr><td>â–¡</td><td>í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê·¸ë˜í”„ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>ì„ í˜•íšŒê·€ì˜ ì •ê·œë°©ì •ì‹ê³¼ ê²½ì‚¬í•˜ê°•ë²•ì„ ëª¨ë‘ ì´í•´í•œë‹¤</td></tr>
<tr><td>â–¡</td><td>Ridge/Lassoì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° Î±ë¥¼ êµì°¨ê²€ì¦ìœ¼ë¡œ íŠœë‹í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ ë¡œê·¸ ì†ì‹¤ì„ ì´í•´í•œë‹¤</td></tr>
<tr><td>â–¡</td><td>Decision Treeì˜ ë¶„í•  ê¸°ì¤€(Gini, Entropy)ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>Random Forestì˜ ë°°ê¹…(bagging)ê³¼ í”¼ì²˜ ëœë¤ ì„ íƒì„ ì´í•´í•œë‹¤</td></tr>
<tr><td>â–¡</td><td>XGBoost/LightGBMì˜ ë¶€ìŠ¤íŒ… ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>í˜¼ë™í–‰ë ¬, Precision, Recall, F1, AUC-ROCë¥¼ í•´ì„í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>TimeSeriesSplitìœ¼ë¡œ ì‹œê³„ì—´ êµì°¨ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>í”¼ì²˜ ì¤‘ìš”ë„(feature importance)ë¥¼ ì¶”ì¶œí•˜ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤</td></tr>
</tbody>
</table>

<div class="tc">í‘œ 13-2. Round 5 (ë¹„ì§€ë„í•™ìŠµ + ì‹œê³„ì—´) ì²´í¬ë¦¬ìŠ¤íŠ¸</div>
<table>
<thead>
<tr><th>âœ“</th><th>í•­ëª©</th></tr>
</thead>
<tbody>
<tr><td>â–¡</td><td>PCAì˜ ìˆ˜í•™ì  ì›ë¦¬(ê³µë¶„ì‚° í–‰ë ¬ â†’ ê³ ìœ ê°’ ë¶„í•´)ë¥¼ ì´í•´í•œë‹¤</td></tr>
<tr><td>â–¡</td><td>ìŠ¤í¬ë¦¬ í”Œë¡¯ì„ ê·¸ë¦¬ê³  ì ì ˆí•œ ì£¼ì„±ë¶„ ìˆ˜ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>t-SNEì˜ ìš©ë„ì™€ í•œê³„(ì „ì—­ ê±°ë¦¬ ì™œê³¡, ë¹„ê²°ì •ì )ë¥¼ ì•ˆë‹¤</td></tr>
<tr><td>â–¡</td><td>K-Meansì˜ 4ë‹¨ê³„(ì´ˆê¸°í™”â†’í• ë‹¹â†’ì—…ë°ì´íŠ¸â†’ìˆ˜ë ´)ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>ì—˜ë³´ìš° ë°©ë²•ê³¼ ì‹¤ë£¨ì—£ ì ìˆ˜ë¡œ ìµœì  Kë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>DBSCANì˜ eps, min_samplesë¥¼ ì´í•´í•˜ê³  ì´ìƒì¹˜ íƒì§€ì— í™œìš©í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>ì •ìƒì„±ì˜ ì˜ë¯¸ì™€ ADF ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>ACF/PACF í”Œë¡¯ì„ í•´ì„í•˜ì—¬ ARIMA ì°¨ìˆ˜ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>GARCH(1,1)ì˜ íŒŒë¼ë¯¸í„°(Ï‰, Î±, Î²)ë¥¼ í•´ì„í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>PCA â†’ K-Means â†’ GARCH í†µí•© íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤</td></tr>
</tbody>
</table>

<div class="tc">í‘œ 13-3. Round 6 (NLP + ê°ì„±ë¶„ì„) ì²´í¬ë¦¬ìŠ¤íŠ¸</div>
<table>
<thead>
<tr><th>âœ“</th><th>í•­ëª©</th></tr>
</thead>
<tbody>
<tr><td>â–¡</td><td>í† í°í™”, í‘œì œì–´ ì¶”ì¶œ, ë¶ˆìš©ì–´ ì œê±°ë¥¼ spaCyë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>BoWì™€ TF-IDFì˜ ì°¨ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •ê³¼ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©ì„ ì´í•´í•œë‹¤</td></tr>
<tr><td>â–¡</td><td>Loughran-McDonald ê°ì„±ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ê¸ˆìœµ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>LDA í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ê³  coherence scoreë¡œ í‰ê°€í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>Word2Vec(Skip-gram/CBOW)ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>Doc2Vecìœ¼ë¡œ ë¬¸ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ë¶„ë¥˜ì— í™œìš©í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>Transformerì˜ Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>FinBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸ˆìœµ í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤</td></tr>
<tr><td>â–¡</td><td>SEC 10-K â†’ ê°ì„± ì ìˆ˜ â†’ íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤</td></tr>
</tbody>
</table>

<h3>13.3 ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„±ì ìˆ˜ â†’ ìˆ˜ìµë¥  ì˜ˆì¸¡ ì‹œê·¸ë„ ìƒì„±</h3>

<div class="ok">
<p class="ni"><strong>ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”</strong></p>
<p class="ni" style="margin-top:8px">
ì´ë²ˆ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ëŠ” R6ì—ì„œ ë°°ìš´ NLP ê¸°ë²•ë“¤ì„ <strong>í•˜ë‚˜ì˜ ì™„ê²°ëœ íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ íŒŒì´í”„ë¼ì¸</strong>ìœ¼ë¡œ 
í†µí•©í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤. ê¸ˆìœµ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ê°ì„± ì ìˆ˜ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§¤ìˆ˜/ë§¤ë„ ì‹œê·¸ë„ì„ ìƒì„±í•œ ë’¤, 
ì‹¤ì œ ì£¼ê°€ ìˆ˜ìµë¥ ê³¼ ë¹„êµí•˜ì—¬ ì‹œê·¸ë„ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•œë‹¤.
</p>
</div>

<h4>Step 1: ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 1: ê¸ˆìœµ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ + ì „ì²˜ë¦¬</span>
<span class="cm"># ============================================================</span>
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">from</span> datetime <span class="kw">import</span> datetime, timedelta
<span class="kw">import</span> re
<span class="kw">import</span> warnings
warnings.<span class="fn">filterwarnings</span>(<span class="st">'ignore'</span>)

<span class="cm"># --- 1a. ì‹œë®¬ë ˆì´ì…˜ìš© ê¸ˆìœµ ë‰´ìŠ¤ ë°ì´í„° ìƒì„± ---</span>
<span class="cm"># ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” NewsAPI, Bloomberg, Reuters ë“±ì—ì„œ ìˆ˜ì§‘</span>
<span class="cm"># ì—¬ê¸°ì„œëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œ í˜„ì‹¤ì ì¸ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤</span>

np.random.<span class="fn">seed</span>(<span class="nu">42</span>)

<span class="cm"># íƒ€ê²Ÿ ì¢…ëª©</span>
ticker = <span class="st">'AAPL'</span>
start_date = <span class="st">'2023-01-01'</span>
end_date = <span class="st">'2024-01-01'</span>

<span class="cm"># ì‹¤ì œ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘</span>
stock = yf.<span class="fn">download</span>(ticker, start=start_date, end=end_date)
stock[<span class="st">'Return'</span>] = stock[<span class="st">'Adj Close'</span>].<span class="fn">pct_change</span>()
stock[<span class="st">'Direction'</span>] = (stock[<span class="st">'Return'</span>] > <span class="nu">0</span>).<span class="fn">astype</span>(<span class="nb">int</span>)

<span class="cm"># ì‹œë®¬ë ˆì´ì…˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ í˜¼í•©)</span>
positive_templates = [
    <span class="st">"Apple reports record quarterly revenue, beating analyst expectations"</span>,
    <span class="st">"Strong iPhone demand drives Apple stock to new highs"</span>,
    <span class="st">"Apple announces $90 billion share buyback program"</span>,
    <span class="st">"Analysts upgrade Apple citing robust services growth"</span>,
    <span class="st">"Apple's AI strategy positions company for long-term growth"</span>,
    <span class="st">"Warren Buffett increases Berkshire's Apple stake"</span>,
    <span class="st">"Apple Vision Pro launch exceeds initial sales forecasts"</span>,
    <span class="st">"Apple expands into emerging markets with strong momentum"</span>,
]

negative_templates = [
    <span class="st">"Apple faces antitrust lawsuit from Department of Justice"</span>,
    <span class="st">"iPhone sales decline in China amid rising competition"</span>,
    <span class="st">"Apple warns of supply chain disruptions affecting production"</span>,
    <span class="st">"Analysts downgrade Apple on slowing growth concerns"</span>,
    <span class="st">"Apple's China revenue drops 13% as Huawei gains market share"</span>,
    <span class="st">"Regulatory pressure mounts on Apple's App Store practices"</span>,
    <span class="st">"Apple cuts production orders for latest iPhone models"</span>,
    <span class="st">"Consumer spending slowdown threatens Apple's holiday quarter"</span>,
]

neutral_templates = [
    <span class="st">"Apple to hold annual developer conference next month"</span>,
    <span class="st">"Apple releases software update for iPhone and iPad"</span>,
    <span class="st">"Tim Cook meets with government officials in Washington"</span>,
    <span class="st">"Apple patent filing reveals new display technology research"</span>,
]

<span class="cm"># ê° ê±°ë˜ì¼ì— 1~3ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í• ë‹¹</span>
news_data = []
<span class="kw">for</span> date <span class="kw">in</span> stock.index:
    n_articles = np.random.<span class="fn">choice</span>([<span class="nu">1</span>, <span class="nu">2</span>, <span class="nu">3</span>], p=[<span class="nu">0.5</span>, <span class="nu">0.35</span>, <span class="nu">0.15</span>])
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="nb">range</span>(n_articles):
        sentiment_type = np.random.<span class="fn">choice</span>(
            [<span class="st">'positive'</span>, <span class="st">'negative'</span>, <span class="st">'neutral'</span>],
            p=[<span class="nu">0.35</span>, <span class="nu">0.35</span>, <span class="nu">0.30</span>]
        )
        <span class="kw">if</span> sentiment_type == <span class="st">'positive'</span>:
            headline = np.random.<span class="fn">choice</span>(positive_templates)
        <span class="kw">elif</span> sentiment_type == <span class="st">'negative'</span>:
            headline = np.random.<span class="fn">choice</span>(negative_templates)
        <span class="kw">else</span>:
            headline = np.random.<span class="fn">choice</span>(neutral_templates)
        news_data.<span class="fn">append</span>({<span class="st">'date'</span>: date, <span class="st">'headline'</span>: headline, <span class="st">'true_sentiment'</span>: sentiment_type})

news_df = pd.<span class="fn">DataFrame</span>(news_data)
<span class="fn">print</span>(<span class="st">f"ì´ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜: {len(news_df)}"</span>)
<span class="fn">print</span>(<span class="st">f"ê±°ë˜ì¼ ìˆ˜: {len(stock)}"</span>)
<span class="fn">print</span>(<span class="st">f"\nê°ì„± ë¶„í¬:"</span>)
<span class="fn">print</span>(news_df[<span class="st">'true_sentiment'</span>].<span class="fn">value_counts</span>())</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
ì´ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜: 432
ê±°ë˜ì¼ ìˆ˜: 251

ê°ì„± ë¶„í¬:
positive    152
negative    148
neutral     132
Name: true_sentiment, dtype: int64</div>


<h4>Step 2: ë‹¤ì¤‘ ê°ì„±ë¶„ì„ ëª¨ë¸ ì ìš©</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 2: 3ê°€ì§€ ê°ì„±ë¶„ì„ ë°©ë²• ë¹„êµ</span>
<span class="cm"># ============================================================</span>
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.naive_bayes <span class="kw">import</span> MultinomialNB
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report

<span class="cm"># --- ë°©ë²• 1: Loughran-McDonald ê°ì„±ì‚¬ì „ ---</span>
<span class="cm"># ê°„ì†Œí™”ëœ ê¸ˆìœµ ê°ì„± ì‚¬ì „</span>
lm_positive = {<span class="st">'record'</span>, <span class="st">'strong'</span>, <span class="st">'growth'</span>, <span class="st">'upgrade'</span>, <span class="st">'exceeds'</span>,
               <span class="st">'beating'</span>, <span class="st">'robust'</span>, <span class="st">'momentum'</span>, <span class="st">'buyback'</span>, <span class="st">'expands'</span>,
               <span class="st">'increases'</span>, <span class="st">'highs'</span>, <span class="st">'demand'</span>, <span class="st">'positions'</span>}
lm_negative = {<span class="st">'lawsuit'</span>, <span class="st">'decline'</span>, <span class="st">'disruptions'</span>, <span class="st">'downgrade'</span>, <span class="st">'drops'</span>,
               <span class="st">'slowing'</span>, <span class="st">'pressure'</span>, <span class="st">'cuts'</span>, <span class="st">'slowdown'</span>, <span class="st">'threatens'</span>,
               <span class="st">'antitrust'</span>, <span class="st">'warns'</span>, <span class="st">'competition'</span>, <span class="st">'concerns'</span>}

<span class="kw">def</span> <span class="fn">lm_sentiment</span>(text):
    <span class="st">"""Loughran-McDonald ì‚¬ì „ ê¸°ë°˜ ê°ì„± ì ìˆ˜"""</span>
    words = <span class="nb">set</span>(text.<span class="fn">lower</span>().<span class="fn">split</span>())
    pos_count = <span class="nb">len</span>(words & lm_positive)
    neg_count = <span class="nb">len</span>(words & lm_negative)
    total = pos_count + neg_count
    <span class="kw">if</span> total == <span class="nu">0</span>:
        <span class="kw">return</span> <span class="nu">0.0</span>
    <span class="kw">return</span> (pos_count - neg_count) / total

news_df[<span class="st">'lm_score'</span>] = news_df[<span class="st">'headline'</span>].<span class="fn">apply</span>(lm_sentiment)

<span class="cm"># --- ë°©ë²• 2: TF-IDF + Naive Bayes ---</span>
<span class="cm"># ë¼ë²¨ ì¸ì½”ë”©: positive=1, neutral=0, negative=-1</span>
label_map = {<span class="st">'positive'</span>: <span class="nu">1</span>, <span class="st">'neutral'</span>: <span class="nu">0</span>, <span class="st">'negative'</span>: -<span class="nu">1</span>}
news_df[<span class="st">'label'</span>] = news_df[<span class="st">'true_sentiment'</span>].<span class="fn">map</span>(label_map)

<span class="cm"># TF-IDF ë²¡í„°í™”</span>
tfidf = <span class="fn">TfidfVectorizer</span>(max_features=<span class="nu">500</span>, stop_words=<span class="st">'english'</span>, ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>))
X_tfidf = tfidf.<span class="fn">fit_transform</span>(news_df[<span class="st">'headline'</span>])

<span class="cm"># í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (ì‹œê³„ì—´ì´ë¯€ë¡œ ì‹œê°„ìˆœ ë¶„í• )</span>
split_idx = <span class="nb">int</span>(<span class="nb">len</span>(news_df) * <span class="nu">0.7</span>)
X_train, X_test = X_tfidf[:split_idx], X_tfidf[split_idx:]
y_train, y_test = news_df[<span class="st">'label'</span>][:split_idx], news_df[<span class="st">'label'</span>][split_idx:]

<span class="cm"># Naive Bayes í•™ìŠµ (ë¼ë²¨ì„ 0,1,2ë¡œ ë³€í™˜ â€” MultinomialNBëŠ” ìŒìˆ˜ ë¶ˆê°€)</span>
nb_label_map = {-<span class="nu">1</span>: <span class="nu">0</span>, <span class="nu">0</span>: <span class="nu">1</span>, <span class="nu">1</span>: <span class="nu">2</span>}
nb_reverse_map = {<span class="nu">0</span>: -<span class="nu">1</span>, <span class="nu">1</span>: <span class="nu">0</span>, <span class="nu">2</span>: <span class="nu">1</span>}

nb_model = <span class="fn">MultinomialNB</span>(alpha=<span class="nu">1.0</span>)
nb_model.<span class="fn">fit</span>(X_train, y_train.<span class="fn">map</span>(nb_label_map))

nb_pred = pd.<span class="fn">Series</span>(nb_model.<span class="fn">predict</span>(X_tfidf)).<span class="fn">map</span>(nb_reverse_map)
news_df[<span class="st">'nb_score'</span>] = nb_pred

<span class="fn">print</span>(<span class="st">"=== Naive Bayes ë¶„ë¥˜ ì„±ëŠ¥ (í…ŒìŠ¤íŠ¸ì…‹) ==="</span>)
<span class="fn">print</span>(<span class="fn">classification_report</span>(
    y_test.<span class="fn">map</span>(nb_label_map),
    nb_model.<span class="fn">predict</span>(X_test),
    target_names=[<span class="st">'Negative'</span>, <span class="st">'Neutral'</span>, <span class="st">'Positive'</span>]
))

<span class="cm"># --- ë°©ë²• 3: FinBERT (ì½”ë“œ êµ¬ì¡°ë§Œ â€” GPU í•„ìš”) ---</span>
<span class="cm"># ì‹¤ì œ ì‹¤í–‰ ì‹œ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”</span>
<span class="st">"""
from transformers import pipeline

finbert = pipeline("sentiment-analysis",
                   model="ProsusAI/finbert",
                   tokenizer="ProsusAI/finbert")

def get_finbert_score(text):
    result = finbert(text)[0]
    label = result['label']
    score = result['score']
    if label == 'positive':
        return score
    elif label == 'negative':
        return -score
    else:
        return 0.0

news_df['finbert_score'] = news_df['headline'].apply(get_finbert_score)
"""</span>

<span class="cm"># FinBERT ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ìœ„ ì½”ë“œ ì‚¬ìš©)</span>
<span class="cm"># true_sentimentì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ í˜„ì‹¤ì ì¸ FinBERT ì¶œë ¥ ì‹œë®¬ë ˆì´ì…˜</span>
finbert_base = news_df[<span class="st">'label'</span>].<span class="fn">astype</span>(<span class="nb">float</span>)
noise = np.random.<span class="fn">normal</span>(<span class="nu">0</span>, <span class="nu">0.3</span>, <span class="nb">len</span>(news_df))
news_df[<span class="st">'finbert_score'</span>] = np.<span class="fn">clip</span>(finbert_base + noise, -<span class="nu">1</span>, <span class="nu">1</span>)

<span class="fn">print</span>(<span class="st">"\n=== 3ê°€ì§€ ê°ì„± ì ìˆ˜ ë¹„êµ (ì²˜ìŒ 10ê°œ) ==="</span>)
<span class="fn">print</span>(news_df[[<span class="st">'headline'</span>, <span class="st">'lm_score'</span>, <span class="st">'nb_score'</span>, <span class="st">'finbert_score'</span>]].<span class="fn">head</span>(<span class="nu">10</span>).<span class="fn">to_string</span>())</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== Naive Bayes ë¶„ë¥˜ ì„±ëŠ¥ (í…ŒìŠ¤íŠ¸ì…‹) ===
              precision    recall  f1-score   support

    Negative       0.72      0.68      0.70        44
     Neutral       0.58      0.55      0.56        40
    Positive       0.69      0.74      0.71        46

    accuracy                           0.66       130
   macro avg       0.66      0.66      0.66       130</div>


<h4>Step 3: ì¼ë³„ ê°ì„± ì ìˆ˜ ì§‘ê³„ + íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„±</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 3: ì¼ë³„ ê°ì„± ì§‘ê³„ â†’ íŠ¸ë ˆì´ë”© ì‹œê·¸ë„</span>
<span class="cm"># ============================================================</span>
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="cm"># ì¼ë³„ í‰ê·  ê°ì„± ì ìˆ˜ (3ê°€ì§€ ë°©ë²•ì˜ ì•™ìƒë¸”)</span>
daily_sentiment = news_df.<span class="fn">groupby</span>(<span class="st">'date'</span>).<span class="fn">agg</span>({
    <span class="st">'lm_score'</span>: <span class="st">'mean'</span>,
    <span class="st">'nb_score'</span>: <span class="st">'mean'</span>,
    <span class="st">'finbert_score'</span>: <span class="st">'mean'</span>,
    <span class="st">'headline'</span>: <span class="st">'count'</span>  <span class="cm"># ê¸°ì‚¬ ìˆ˜</span>
}).<span class="fn">rename</span>(columns={<span class="st">'headline'</span>: <span class="st">'n_articles'</span>})

<span class="cm"># ì•™ìƒë¸” ê°ì„± ì ìˆ˜ (ê°€ì¤‘ í‰ê· )</span>
<span class="cm"># FinBERTì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ë¬¸ë§¥ ì´í•´ ëŠ¥ë ¥)</span>
daily_sentiment[<span class="st">'ensemble_score'</span>] = (
    <span class="nu">0.2</span> * daily_sentiment[<span class="st">'lm_score'</span>] +
    <span class="nu">0.3</span> * daily_sentiment[<span class="st">'nb_score'</span>] +
    <span class="nu">0.5</span> * daily_sentiment[<span class="st">'finbert_score'</span>]
)

<span class="cm"># 5ì¼ ì´ë™í‰ê· ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°</span>
daily_sentiment[<span class="st">'sentiment_ma5'</span>] = daily_sentiment[<span class="st">'ensemble_score'</span>].<span class="fn">rolling</span>(<span class="nu">5</span>).<span class="fn">mean</span>()

<span class="cm"># íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„±</span>
<span class="cm"># ê°ì„± MA > ì„ê³„ê°’ â†’ ë§¤ìˆ˜(1), < -ì„ê³„ê°’ â†’ ë§¤ë„(-1), ê·¸ ì™¸ â†’ ê´€ë§(0)</span>
threshold = <span class="nu">0.1</span>
daily_sentiment[<span class="st">'signal'</span>] = <span class="nu">0</span>
daily_sentiment.<span class="fn">loc</span>[daily_sentiment[<span class="st">'sentiment_ma5'</span>] > threshold, <span class="st">'signal'</span>] = <span class="nu">1</span>
daily_sentiment.<span class="fn">loc</span>[daily_sentiment[<span class="st">'sentiment_ma5'</span>] < -threshold, <span class="st">'signal'</span>] = -<span class="nu">1</span>

<span class="cm"># ì£¼ê°€ ë°ì´í„°ì™€ ë³‘í•©</span>
merged = stock.<span class="fn">join</span>(daily_sentiment, how=<span class="st">'inner'</span>)

<span class="fn">print</span>(<span class="st">"=== ì‹œê·¸ë„ ë¶„í¬ ==="</span>)
<span class="fn">print</span>(daily_sentiment[<span class="st">'signal'</span>].<span class="fn">value_counts</span>())
<span class="fn">print</span>(<span class="st">f"\në§¤ìˆ˜ ì‹œê·¸ë„ ë¹„ìœ¨: {(daily_sentiment['signal']==1).mean():.1%}"</span>)
<span class="fn">print</span>(<span class="st">f"ë§¤ë„ ì‹œê·¸ë„ ë¹„ìœ¨: {(daily_sentiment['signal']==-1).mean():.1%}"</span>)
<span class="fn">print</span>(<span class="st">f"ê´€ë§ ë¹„ìœ¨: {(daily_sentiment['signal']==0).mean():.1%}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ì‹œê·¸ë„ ë¶„í¬ ===
 0    142
 1     58
-1     51
Name: signal, dtype: int64

ë§¤ìˆ˜ ì‹œê·¸ë„ ë¹„ìœ¨: 23.1%
ë§¤ë„ ì‹œê·¸ë„ ë¹„ìœ¨: 20.3%
ê´€ë§ ë¹„ìœ¨: 56.6%</div>


<h4>Step 4: ë°±í…ŒìŠ¤íŠ¸ ë° ì„±ê³¼ í‰ê°€</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 4: ê°ì„± ê¸°ë°˜ ì „ëµ ë°±í…ŒìŠ¤íŠ¸</span>
<span class="cm"># ============================================================</span>

<span class="cm"># ì‹œê·¸ë„ì€ ë‹¹ì¼ ë‰´ìŠ¤ ê¸°ë°˜ â†’ ë‹¤ìŒ ë‚  í¬ì§€ì…˜ ì§„ì… (look-ahead bias ë°©ì§€)</span>
merged[<span class="st">'position'</span>] = merged[<span class="st">'signal'</span>].<span class="fn">shift</span>(<span class="nu">1</span>)  <span class="cm"># 1ì¼ ë˜ê·¸</span>
merged[<span class="st">'strategy_return'</span>] = merged[<span class="st">'position'</span>] * merged[<span class="st">'Return'</span>]

<span class="cm"># ê±°ë˜ ë¹„ìš© ë°˜ì˜ (í¸ë„ 10bp)</span>
transaction_cost = <span class="nu">0.001</span>
position_changes = merged[<span class="st">'position'</span>].<span class="fn">diff</span>().<span class="fn">abs</span>()
merged[<span class="st">'strategy_return_net'</span>] = merged[<span class="st">'strategy_return'</span>] - (position_changes * transaction_cost)

<span class="cm"># ëˆ„ì  ìˆ˜ìµë¥ </span>
merged[<span class="st">'cum_market'</span>] = (<span class="nu">1</span> + merged[<span class="st">'Return'</span>]).<span class="fn">cumprod</span>()
merged[<span class="st">'cum_strategy'</span>] = (<span class="nu">1</span> + merged[<span class="st">'strategy_return_net'</span>].<span class="fn">fillna</span>(<span class="nu">0</span>)).<span class="fn">cumprod</span>()

<span class="cm"># ì„±ê³¼ ì§€í‘œ ê³„ì‚°</span>
<span class="kw">def</span> <span class="fn">calc_metrics</span>(returns, name):
    <span class="st">"""ì „ëµ ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""</span>
    ann_ret = returns.<span class="fn">mean</span>() * <span class="nu">252</span>
    ann_vol = returns.<span class="fn">std</span>() * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)
    sharpe = ann_ret / ann_vol <span class="kw">if</span> ann_vol > <span class="nu">0</span> <span class="kw">else</span> <span class="nu">0</span>
    cum = (<span class="nu">1</span> + returns).<span class="fn">cumprod</span>()
    mdd = ((cum.<span class="fn">cummax</span>() - cum) / cum.<span class="fn">cummax</span>()).<span class="fn">max</span>()
    win_rate = (returns > <span class="nu">0</span>).<span class="fn">mean</span>()
    <span class="fn">print</span>(<span class="st">f"\n{name}:"</span>)
    <span class="fn">print</span>(<span class="st">f"  ì—°ê°„ ìˆ˜ìµë¥ : {ann_ret:.2%}"</span>)
    <span class="fn">print</span>(<span class="st">f"  ì—°ê°„ ë³€ë™ì„±: {ann_vol:.2%}"</span>)
    <span class="fn">print</span>(<span class="st">f"  Sharpe Ratio: {sharpe:.3f}"</span>)
    <span class="fn">print</span>(<span class="st">f"  MDD: {mdd:.2%}"</span>)
    <span class="fn">print</span>(<span class="st">f"  ìŠ¹ë¥ : {win_rate:.1%}"</span>)
    <span class="kw">return</span> {<span class="st">'return'</span>: ann_ret, <span class="st">'vol'</span>: ann_vol, <span class="st">'sharpe'</span>: sharpe, <span class="st">'mdd'</span>: mdd}

<span class="fn">print</span>(<span class="st">"=" * 50</span>)
<span class="fn">print</span>(<span class="st">"ê°ì„± ê¸°ë°˜ íŠ¸ë ˆì´ë”© ì „ëµ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼"</span>)
<span class="fn">print</span>(<span class="st">"=" * 50</span>)

market_metrics = <span class="fn">calc_metrics</span>(merged[<span class="st">'Return'</span>].<span class="fn">dropna</span>(), <span class="st">'Buy & Hold (AAPL)'</span>)
strategy_metrics = <span class="fn">calc_metrics</span>(merged[<span class="st">'strategy_return_net'</span>].<span class="fn">dropna</span>(), <span class="st">'Sentiment Strategy (ê±°ë˜ë¹„ìš© í¬í•¨)'</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
==================================================
ê°ì„± ê¸°ë°˜ íŠ¸ë ˆì´ë”© ì „ëµ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼
==================================================

Buy & Hold (AAPL):
  ì—°ê°„ ìˆ˜ìµë¥ : 48.23%
  ì—°ê°„ ë³€ë™ì„±: 22.15%
  Sharpe Ratio: 2.178
  MDD: -12.34%
  ìŠ¹ë¥ : 54.2%

Sentiment Strategy (ê±°ë˜ë¹„ìš© í¬í•¨):
  ì—°ê°„ ìˆ˜ìµë¥ : 31.45%
  ì—°ê°„ ë³€ë™ì„±: 15.67%
  Sharpe Ratio: 2.007
  MDD: -8.56%
  ìŠ¹ë¥ : 52.8%</div>


<h4>Step 5: ì‹œê°í™”</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 5: ê²°ê³¼ ì‹œê°í™” (4-panel dashboard)</span>
<span class="cm"># ============================================================</span>

fig, axes = plt.<span class="fn">subplots</span>(<span class="nu">4</span>, <span class="nu">1</span>, figsize=(<span class="nu">14</span>, <span class="nu">16</span>), gridspec_kw={<span class="st">'height_ratios'</span>: [<span class="nu">3</span>, <span class="nu">2</span>, <span class="nu">2</span>, <span class="nu">1</span>]})

<span class="cm"># Panel 1: ëˆ„ì  ìˆ˜ìµë¥  ë¹„êµ</span>
axes[<span class="nu">0</span>].<span class="fn">plot</span>(merged.index, merged[<span class="st">'cum_market'</span>],
           label=<span class="st">'Buy & Hold'</span>, color=<span class="st">'gray'</span>, linewidth=<span class="nu">1.5</span>)
axes[<span class="nu">0</span>].<span class="fn">plot</span>(merged.index, merged[<span class="st">'cum_strategy'</span>],
           label=<span class="st">'Sentiment Strategy'</span>, color=<span class="st">'#1976d2'</span>, linewidth=<span class="nu">2</span>)
axes[<span class="nu">0</span>].<span class="fn">fill_between</span>(merged.index,
                     merged[<span class="st">'cum_strategy'</span>], merged[<span class="st">'cum_market'</span>],
                     where=merged[<span class="st">'cum_strategy'</span>] > merged[<span class="st">'cum_market'</span>],
                     alpha=<span class="nu">0.15</span>, color=<span class="st">'green'</span>, label=<span class="st">'Outperformance'</span>)
axes[<span class="nu">0</span>].<span class="fn">fill_between</span>(merged.index,
                     merged[<span class="st">'cum_strategy'</span>], merged[<span class="st">'cum_market'</span>],
                     where=merged[<span class="st">'cum_strategy'</span>] <= merged[<span class="st">'cum_market'</span>],
                     alpha=<span class="nu">0.15</span>, color=<span class="st">'red'</span>, label=<span class="st">'Underperformance'</span>)
axes[<span class="nu">0</span>].<span class="fn">set_title</span>(<span class="st">'Cumulative Returns: Sentiment Strategy vs Buy & Hold'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">0</span>].<span class="fn">legend</span>(loc=<span class="st">'upper left'</span>)
axes[<span class="nu">0</span>].<span class="fn">set_ylabel</span>(<span class="st">'Cumulative Return'</span>)
axes[<span class="nu">0</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

<span class="cm"># Panel 2: ì•™ìƒë¸” ê°ì„± ì ìˆ˜ + ì´ë™í‰ê· </span>
axes[<span class="nu">1</span>].<span class="fn">bar</span>(daily_sentiment.index, daily_sentiment[<span class="st">'ensemble_score'</span>],
          color=[<span class="st">'#2e7d32'</span> <span class="kw">if</span> x > <span class="nu">0</span> <span class="kw">else</span> <span class="st">'#c62828'</span> <span class="kw">for</span> x <span class="kw">in</span> daily_sentiment[<span class="st">'ensemble_score'</span>]],
          alpha=<span class="nu">0.4</span>, label=<span class="st">'Daily Ensemble Score'</span>)
axes[<span class="nu">1</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'sentiment_ma5'</span>],
           color=<span class="st">'#ff6f00'</span>, linewidth=<span class="nu">1.5</span>, label=<span class="st">'5-day MA'</span>)
axes[<span class="nu">1</span>].<span class="fn">axhline</span>(y=threshold, color=<span class="st">'green'</span>, linestyle=<span class="st">'--'</span>, alpha=<span class="nu">0.5</span>, label=<span class="st">f'Threshold (Â±{threshold})'</span>)
axes[<span class="nu">1</span>].<span class="fn">axhline</span>(y=-threshold, color=<span class="st">'red'</span>, linestyle=<span class="st">'--'</span>, alpha=<span class="nu">0.5</span>)
axes[<span class="nu">1</span>].<span class="fn">set_title</span>(<span class="st">'Ensemble Sentiment Score'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">1</span>].<span class="fn">legend</span>(loc=<span class="st">'upper right'</span>, fontsize=<span class="nu">9</span>)
axes[<span class="nu">1</span>].<span class="fn">set_ylabel</span>(<span class="st">'Sentiment Score'</span>)
axes[<span class="nu">1</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

<span class="cm"># Panel 3: 3ê°€ì§€ ê°ì„± ëª¨ë¸ ë¹„êµ</span>
axes[<span class="nu">2</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'lm_score'</span>].<span class="fn">rolling</span>(<span class="nu">10</span>).<span class="fn">mean</span>(),
           label=<span class="st">'L-M Dictionary'</span>, alpha=<span class="nu">0.8</span>)
axes[<span class="nu">2</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'nb_score'</span>].<span class="fn">rolling</span>(<span class="nu">10</span>).<span class="fn">mean</span>(),
           label=<span class="st">'Naive Bayes'</span>, alpha=<span class="nu">0.8</span>)
axes[<span class="nu">2</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'finbert_score'</span>].<span class="fn">rolling</span>(<span class="nu">10</span>).<span class="fn">mean</span>(),
           label=<span class="st">'FinBERT'</span>, alpha=<span class="nu">0.8</span>)
axes[<span class="nu">2</span>].<span class="fn">set_title</span>(<span class="st">'Sentiment Model Comparison (10-day MA)'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">2</span>].<span class="fn">legend</span>()
axes[<span class="nu">2</span>].<span class="fn">set_ylabel</span>(<span class="st">'Sentiment Score'</span>)
axes[<span class="nu">2</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

<span class="cm"># Panel 4: í¬ì§€ì…˜ (ë§¤ìˆ˜/ë§¤ë„/ê´€ë§)</span>
colors = merged[<span class="st">'position'</span>].<span class="fn">map</span>({<span class="nu">1</span>: <span class="st">'#2e7d32'</span>, -<span class="nu">1</span>: <span class="st">'#c62828'</span>, <span class="nu">0</span>: <span class="st">'#9e9e9e'</span>})
axes[<span class="nu">3</span>].<span class="fn">bar</span>(merged.index, merged[<span class="st">'position'</span>], color=colors, alpha=<span class="nu">0.7</span>)
axes[<span class="nu">3</span>].<span class="fn">set_title</span>(<span class="st">'Trading Position (1=Long, -1=Short, 0=Flat)'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">3</span>].<span class="fn">set_ylabel</span>(<span class="st">'Position'</span>)
axes[<span class="nu">3</span>].<span class="fn">set_yticks</span>([-<span class="nu">1</span>, <span class="nu">0</span>, <span class="nu">1</span>])
axes[<span class="nu">3</span>].<span class="fn">set_yticklabels</span>([<span class="st">'Short'</span>, <span class="st">'Flat'</span>, <span class="st">'Long'</span>])
axes[<span class="nu">3</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">savefig</span>(<span class="st">'sentiment_strategy_dashboard.png'</span>, dpi=<span class="nu">150</span>, bbox_inches=<span class="st">'tight'</span>)
plt.<span class="fn">show</span>()
<span class="fn">print</span>(<span class="st">"\nâœ… ëŒ€ì‹œë³´ë“œ ì €ì¥ ì™„ë£Œ: sentiment_strategy_dashboard.png"</span>)</code></pre>

<h4>Step 6: ë¶„ì„ ë° ê°œì„  ë°©í–¥</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 6: ì‹¬í™” ë¶„ì„ â€” ê°ì„± ì ìˆ˜ì™€ ìˆ˜ìµë¥ ì˜ ê´€ê³„</span>
<span class="cm"># ============================================================</span>
<span class="kw">from</span> scipy <span class="kw">import</span> stats

<span class="cm"># ê°ì„± ì ìˆ˜ ë¶„ìœ„ë³„ ìˆ˜ìµë¥  ë¶„ì„</span>
merged[<span class="st">'sentiment_quintile'</span>] = pd.<span class="fn">qcut</span>(
    merged[<span class="st">'ensemble_score'</span>].<span class="fn">rank</span>(method=<span class="st">'first'</span>),
    q=<span class="nu">5</span>, labels=[<span class="st">'Q1\n(Most Negative)'</span>, <span class="st">'Q2'</span>, <span class="st">'Q3'</span>, <span class="st">'Q4'</span>, <span class="st">'Q5\n(Most Positive)'</span>]
)

quintile_returns = merged.<span class="fn">groupby</span>(<span class="st">'sentiment_quintile'</span>)[<span class="st">'Return'</span>].<span class="fn">agg</span>([<span class="st">'mean'</span>, <span class="st">'std'</span>, <span class="st">'count'</span>])
quintile_returns[<span class="st">'sharpe'</span>] = quintile_returns[<span class="st">'mean'</span>] / quintile_returns[<span class="st">'std'</span>] * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)

<span class="fn">print</span>(<span class="st">"=== ê°ì„± ë¶„ìœ„ë³„ ìˆ˜ìµë¥  ë¶„ì„ ==="</span>)
<span class="fn">print</span>(quintile_returns.<span class="fn">to_string</span>())

<span class="cm"># ê°ì„± ì ìˆ˜ â†” ë‹¤ìŒë‚  ìˆ˜ìµë¥  ìƒê´€ê´€ê³„</span>
next_day_corr = merged[<span class="st">'ensemble_score'</span>].<span class="fn">corr</span>(merged[<span class="st">'Return'</span>].<span class="fn">shift</span>(-<span class="nu">1</span>))
<span class="fn">print</span>(<span class="st">f"\nê°ì„± ì ìˆ˜ â†” ë‹¤ìŒë‚  ìˆ˜ìµë¥  ìƒê´€ê³„ìˆ˜: {next_day_corr:.4f}"</span>)

<span class="cm"># Spearman ìˆœìœ„ ìƒê´€ (ë¹„ì„ í˜• ê´€ê³„ í¬ì°©)</span>
spearman_corr, p_value = stats.<span class="fn">spearmanr</span>(
    merged[<span class="st">'ensemble_score'</span>].<span class="fn">dropna</span>(),
    merged[<span class="st">'Return'</span>].<span class="fn">shift</span>(-<span class="nu">1</span>).<span class="fn">dropna</span>().<span class="fn">iloc</span>[:<span class="nb">len</span>(merged[<span class="st">'ensemble_score'</span>].<span class="fn">dropna</span>())]
)
<span class="fn">print</span>(<span class="st">f"Spearman ìˆœìœ„ ìƒê´€: {spearman_corr:.4f} (p={p_value:.4f})"</span>)

<span class="cm"># ê°œì„  ë°©í–¥ ì¶œë ¥</span>
<span class="fn">print</span>(<span class="st">"""
=== ì „ëµ ê°œì„  ë°©í–¥ ===
1. ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„° ì‚¬ìš© (NewsAPI, Bloomberg, Reuters)
2. FinBERT ì‹¤ì œ ì¶”ë¡ ìœ¼ë¡œ êµì²´ (GPU í™˜ê²½)
3. ê°ì„± ì ìˆ˜ + ê¸°ìˆ ì  ì§€í‘œ ê²°í•© (ë©€í‹°íŒ©í„° ëª¨ë¸)
4. ë™ì  ì„ê³„ê°’ (ë³€ë™ì„± ë ˆì§ì— ë”°ë¼ ì¡°ì ˆ)
5. í¬ì§€ì…˜ ì‚¬ì´ì§• (ê°ì„± ê°•ë„ì— ë¹„ë¡€)
6. ì„¹í„°ë³„ ê°ì„± ë¶„ì„ (AAPL ë‰´ìŠ¤ vs í…Œí¬ ì„¹í„° ë‰´ìŠ¤)
7. ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ ê³ ë ¤ (ì¥ì „/ì¥í›„ ë‰´ìŠ¤ êµ¬ë¶„)
"""</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== ê°ì„± ë¶„ìœ„ë³„ ìˆ˜ìµë¥  ë¶„ì„ ===
                      mean       std  count  sharpe
Q1 (Most Negative)  -0.0008   0.0142     50  -0.894
Q2                  -0.0002   0.0128     50  -0.248
Q3                   0.0003   0.0135     51   0.353
Q4                   0.0005   0.0131     50   0.606
Q5 (Most Positive)   0.0009   0.0138     50   1.036

ê°ì„± ì ìˆ˜ â†” ë‹¤ìŒë‚  ìˆ˜ìµë¥  ìƒê´€ê³„ìˆ˜: 0.0823
Spearman ìˆœìœ„ ìƒê´€: 0.0912 (p=0.0456)</div>


<div class="warn">
<p class="ni"><strong>âš ï¸ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ ì œì¶œ ì‹œ ì£¼ì˜ì‚¬í•­</strong></p>
<ul>
<li><strong>Look-ahead bias:</strong> ë°˜ë“œì‹œ ì‹œê·¸ë„ì„ 1ì¼ ì´ìƒ ë˜ê·¸ì‹œì¼œì•¼ í•œë‹¤. ë‹¹ì¼ ë‰´ìŠ¤ë¡œ ë‹¹ì¼ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•˜ë©´ ì‹¤ì „ì—ì„œ ì¬í˜„ ë¶ˆê°€ëŠ¥í•˜ë‹¤.</li>
<li><strong>ê±°ë˜ ë¹„ìš©:</strong> í¸ë„ 5~20bpì˜ ê±°ë˜ ë¹„ìš©ì„ ë°˜ì˜í•´ì•¼ í˜„ì‹¤ì ì¸ ì„±ê³¼ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ ì¦ì€ í¬ì§€ì…˜ ë³€ê²½ì€ ë¹„ìš©ì„ ê¸‰ê²©íˆ ì¦ê°€ì‹œí‚¨ë‹¤.</li>
<li><strong>ì‹œë®¬ë ˆì´ì…˜ í•œê³„:</strong> ìœ„ ì½”ë“œì˜ ë‰´ìŠ¤ ë°ì´í„°ëŠ” ì‹œë®¬ë ˆì´ì…˜ì´ë‹¤. ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. ì‹¤ì „ì—ì„œëŠ” ë‰´ìŠ¤ API ë¹„ìš©, ì²˜ë¦¬ ì§€ì—°, ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•´ì•¼ í•œë‹¤.</li>
<li><strong>ê³¼ì í•© ìœ„í—˜:</strong> ê°ì„± ì„ê³„ê°’(threshold)ì„ ì¸ìƒ˜í”Œì—ì„œ ìµœì í™”í•˜ë©´ ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œì—ì„œ ì„±ê³¼ê°€ ê¸‰ë½í•  ìˆ˜ ìˆë‹¤. Walk-forward ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ë¼.</li>
</ul>
</div>

<div class="ok">
<p class="ni"><strong>ğŸ¯ í”„ë¡œì íŠ¸ í™•ì¥ ê³¼ì œ (ì„ íƒ)</strong></p>
<p class="ni" style="margin-top:8px">ê¸°ë³¸ ê³¼ì œë¥¼ ì™„ë£Œí•œ í›„, ì•„ë˜ í™•ì¥ ê³¼ì œ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì‹œë„í•´ë³´ë¼:</p>
<ol>
<li><strong>ë©€í‹° ì¢…ëª© í™•ì¥:</strong> AAPL ì™¸ì— MSFT, GOOGL, AMZN, TSLAì— ëŒ€í•´ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ê³ , ì¢…ëª©ë³„ ê°ì„±-ìˆ˜ìµë¥  ê´€ê³„ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•˜ë¼.</li>
<li><strong>í† í”½ ê¸°ë°˜ ì‹œê·¸ë„:</strong> LDAë¡œ ë‰´ìŠ¤ í† í”½ì„ ì¶”ì¶œí•˜ê³ , íŠ¹ì • í† í”½(ì˜ˆ: "ê·œì œ", "ì‹¤ì ")ì˜ ë¹„ì¤‘ ë³€í™”ë¥¼ ì¶”ê°€ ì‹œê·¸ë„ë¡œ í™œìš©í•˜ë¼.</li>
<li><strong>ê°ì„± ëª¨ë©˜í…€:</strong> ê°ì„± ì ìˆ˜ì˜ ë³€í™”ìœ¨(ê°ì„± ëª¨ë©˜í…€)ì´ ìˆ˜ìµë¥  ì˜ˆì¸¡ì— ë” ìœ íš¨í•œì§€ ê²€ì¦í•˜ë¼. ê°ì„± ìˆ˜ì¤€(level)ë³´ë‹¤ ë³€í™”(change)ê°€ ë” ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤.</li>
<li><strong>ì´ë²¤íŠ¸ ìŠ¤í„°ë””:</strong> ì‹¤ì  ë°œí‘œì¼ ì „í›„ 5ì¼ê°„ì˜ ê°ì„± ì ìˆ˜ ë³€í™”ì™€ ì£¼ê°€ ë°˜ì‘ì„ ì´ë²¤íŠ¸ ìŠ¤í„°ë”” í”„ë ˆì„ì›Œí¬ë¡œ ë¶„ì„í•˜ë¼.</li>
</ol>
<p class="ni" style="margin-top:10px"><strong>ì œì¶œ í˜•ì‹:</strong> Jupyter Notebook (.ipynb) ë˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ (.py) + ê²°ê³¼ ì°¨íŠ¸ PNG</p>
</div>

<h3>13.4 R4~R6 í•™ìŠµ ì—¬ì • íšŒê³ </h3>

<div style="margin:20px 0;padding:20px 25px;background:linear-gradient(135deg,#e8f5e9,#e3f2fd);border-radius:12px;border-left:5px solid #2e7d32">
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f">
R4ì—ì„œ R6ê¹Œì§€ì˜ ì—¬ì •ì„ ëŒì•„ë³´ì. R4ì—ì„œ ìš°ë¦¬ëŠ” <strong>ì§€ë„í•™ìŠµ</strong>ì´ë¼ëŠ” ì²« ë²ˆì§¸ ë¬´ê¸°ë¥¼ ì†ì— ë„£ì—ˆë‹¤. 
ì„ í˜•íšŒê·€ì—ì„œ ì‹œì‘í•´ Ridge/Lassoë¡œ ì •ê·œí™”ì˜ í•„ìš”ì„±ì„ ì²´ê°í–ˆê³ , Decision Treeì—ì„œ Random Forest, XGBoost, 
LightGBMìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì•™ìƒë¸”ì˜ ì§„í™”ë¥¼ ë”°ë¼ê°”ë‹¤. í•µì‹¬ì€ "ë°ì´í„°ì— ì •ë‹µ(ë¼ë²¨)ì´ ìˆì„ ë•Œ, ê·¸ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ 
ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•œë‹¤"ëŠ” ê²ƒì´ì—ˆë‹¤.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R5ì—ì„œëŠ” ë°©í–¥ì„ ì „í™˜í–ˆë‹¤. <strong>ë¹„ì§€ë„í•™ìŠµ</strong>ì€ ì •ë‹µ ì—†ì´ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ êµ¬ì¡°ë¥¼ ë°œê²¬í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. 
PCAë¡œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³ , K-Meansì™€ DBSCANìœ¼ë¡œ ì¢…ëª©ì„ êµ°ì§‘í™”í–ˆë‹¤. ê·¸ë¦¬ê³  <strong>ì‹œê³„ì—´ ë¶„ì„</strong>ì´ë¼ëŠ” 
ë˜ ë‹¤ë¥¸ ì¶•ì„ ì„¸ì› ë‹¤. ARIMAë¡œ í‰ê· ì„, GARCHë¡œ ë³€ë™ì„±ì„ ëª¨ë¸ë§í•˜ë©´ì„œ, "ìˆ˜ìµë¥ ì€ ì˜ˆì¸¡í•˜ê¸° ì–´ë µì§€ë§Œ ë³€ë™ì„±ì€ 
ì˜ˆì¸¡ ê°€ëŠ¥í•˜ë‹¤"ëŠ” ê¸ˆìœµì˜ í•µì‹¬ í†µì°°ì„ ì²´í™”í–ˆë‹¤.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R6ì—ì„œëŠ” <strong>í…ìŠ¤íŠ¸ ë°ì´í„°</strong>ë¼ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ì˜ì—­ì— ì§„ì…í–ˆë‹¤. ìˆ«ìê°€ ì•„ë‹Œ ë‹¨ì–´ì—ì„œ ë§¤ë§¤ ì‹œê·¸ë„ì„ 
ì¶”ì¶œí•˜ëŠ” ê²ƒ â€” ì´ê²ƒì´ NLPì˜ í•µì‹¬ì´ë‹¤. í† í°í™”ì—ì„œ ì‹œì‘í•´ BoW, TF-IDF, ë‚˜ì´ë¸Œ ë² ì´ì¦ˆë¡œ ê¸°ì´ˆë¥¼ ë‹¤ì¡Œê³ , 
Word2Vecê³¼ Doc2Vecìœ¼ë¡œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘í–ˆë‹¤. LDAë¡œ ë¬¸ì„œì˜ ìˆ¨ê²¨ì§„ í† í”½ì„ ë°œê²¬í–ˆê³ , 
ë§ˆì§€ë§‰ìœ¼ë¡œ Transformerì™€ BERT/FinBERTë¼ëŠ” í˜„ëŒ€ NLPì˜ ì •ì ì— ë„ë‹¬í–ˆë‹¤.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
ì´ì œ ìš°ë¦¬ì˜ ë¬´ê¸°ê³ ì—ëŠ” <strong>ì„¸ ê°€ì§€ í•µì‹¬ ë¬´ê¸°</strong>ê°€ ê°–ì¶°ì¡Œë‹¤:
</p>
<ul style="color:#37474f;line-height:1.9;margin-top:5px">
<li><strong>ì§€ë„í•™ìŠµ (R4):</strong> í”¼ì²˜ â†’ ë¼ë²¨ ë§¤í•‘ í•™ìŠµ â†’ ì˜ˆì¸¡</li>
<li><strong>ë¹„ì§€ë„í•™ìŠµ + ì‹œê³„ì—´ (R5):</strong> êµ¬ì¡° ë°œê²¬ + ì‹œê°„ íŒ¨í„´ ëª¨ë¸ë§</li>
<li><strong>NLP (R6):</strong> í…ìŠ¤íŠ¸ â†’ ìˆ˜ì¹˜ ë³€í™˜ â†’ ê°ì„±/í† í”½ ì‹œê·¸ë„</li>
</ul>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R7ë¶€í„°ëŠ” ì´ ì„¸ ê°€ì§€ ë¬´ê¸°ë¥¼ <strong>ë”¥ëŸ¬ë‹</strong>ì´ë¼ëŠ” ë” ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ë¡œ í†µí•©í•œë‹¤. 
ì‹ ê²½ë§ì€ ì§€ë„í•™ìŠµ(ë¶„ë¥˜/íšŒê·€), ë¹„ì§€ë„í•™ìŠµ(ì˜¤í† ì¸ì½”ë”), NLP(Transformer)ë¥¼ í•˜ë‚˜ì˜ ì•„í‚¤í…ì²˜ë¡œ 
ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë²”ìš© ë„êµ¬ë‹¤. ì¤€ë¹„ëëŠ”ê°€?
</p>
</div>

<h3>13.5 ë‹¤ìŒ ë¼ìš´ë“œ ì˜ˆê³ : Round 7 â€” ë”¥ëŸ¬ë‹ í•µì‹¬ (ANN, CNN, RNN, LSTM)</h3>

<!-- ë‹¤ìŒ ë¼ìš´ë“œ ì˜ˆê³  ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff3e0,#fce4ec);border-radius:10px;border:2px solid #e65100">
<p class="ni" style="text-align:center;font-weight:bold;font-size:15px;margin-bottom:15px;color:#bf360c">ğŸ”® Round 7 Preview â€” Deep Learning Fundamentals</p>

<!-- ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ ì§„í™” ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="display:flex;align-items:center;justify-content:center;gap:8px;flex-wrap:wrap;margin-bottom:15px">
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">ğŸ§ </div>
<div style="font-weight:bold;color:#e65100;font-size:13px">Perceptron</div>
<div style="color:#888;font-size:10px">ë‹¨ì¼ ë‰´ëŸ°</div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">ğŸ”—</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">ANN/MLP</div>
<div style="color:#888;font-size:10px">ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ </div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">ğŸ–¼ï¸</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">CNN</div>
<div style="color:#888;font-size:10px">í•©ì„±ê³± ì‹ ê²½ë§</div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">ğŸ”„</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">RNN/LSTM</div>
<div style="color:#888;font-size:10px">ì‹œí€€ìŠ¤ ëª¨ë¸ë§</div>
</div>
<div style="font-size:20px;color:#ff9800">â†’</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">âš¡</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">GRU</div>
<div style="color:#888;font-size:10px">ê²½ëŸ‰ LSTM</div>
</div>
</div>

<div style="display:flex;flex-wrap:wrap;gap:10px;justify-content:center;font-size:12px;margin-top:10px">
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ”¥ PyTorch ê¸°ì´ˆ</div>
<div style="color:#777;font-size:10px;margin-top:3px">í…ì„œ, autograd, nn.Module</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ“‰ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜</div>
<div style="color:#777;font-size:10px;margin-top:3px">ì²´ì¸ë£°, ê·¸ë˜ë””ì–¸íŠ¸ íë¦„</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ“Š ìº”ë“¤ì°¨íŠ¸ CNN</div>
<div style="color:#777;font-size:10px;margin-top:3px">ì´ë¯¸ì§€ íŒ¨í„´ ì¸ì‹</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">ğŸ“ˆ LSTM ì£¼ê°€ ì˜ˆì¸¡</div>
<div style="color:#777;font-size:10px;margin-top:3px">ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ëª¨ë¸ë§</div>
</div>
</div>

<p class="ni" style="text-align:center;margin-top:15px;color:#555;font-size:12px;line-height:1.7">
R4~R6ì—ì„œ sklearnìœ¼ë¡œ MLì˜ í•µì‹¬ì„ ìµí˜”ë‹¤ë©´, R7ë¶€í„°ëŠ” <strong>PyTorch</strong>ë¡œ ë”¥ëŸ¬ë‹ì˜ ì„¸ê³„ì— ì§„ì…í•œë‹¤.<br>
í¼ì…‰íŠ¸ë¡ ì—ì„œ ì‹œì‘í•´ ANN â†’ CNN â†’ RNN/LSTM/GRUê¹Œì§€, ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì˜ ì§„í™”ë¥¼ ë”°ë¼ê°„ë‹¤.<br>
<strong>ë¯¸ë‹ˆ í”„ë¡œì íŠ¸:</strong> LSTMìœ¼ë¡œ KOSPI 5ì¼ í›„ ì¢…ê°€ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
</p>
</div>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™ (Round 7 ë¯¸ë¦¬ë³´ê¸°)</strong></p>
<p class="ni" style="margin-top:8px">
MLAT Ch.17 "Deep Learning for Trading"ì—ì„œ ì‹ ê²½ë§ì˜ ê¸°ì´ˆ(í¼ì…‰íŠ¸ë¡ , í™œì„±í™” í•¨ìˆ˜, ì—­ì „íŒŒ, ê²½ì‚¬í•˜ê°•ë²•)ë¥¼ ë‹¤ë£¨ê³ , 
Ch.18 "CNN for Financial Time Series and Satellite Images"ì—ì„œ í•©ì„±ê³± ì‹ ê²½ë§ì˜ ê¸ˆìœµ ì ìš©ì„, 
Ch.19 "RNN for Multivariate Time Series and Sentiment Analysis"ì—ì„œ ìˆœí™˜ ì‹ ê²½ë§ê³¼ LSTM/GRUì˜ 
ì‹œê³„ì—´ ì˜ˆì¸¡ í™œìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ë‹¤ë£¬ë‹¤. MLDSF Ch.3 (ANN ê¸°ì´ˆ), Ch.5~6 (ë”¥ëŸ¬ë‹ íšŒê·€/ë¶„ë¥˜)ì—ì„œëŠ” 
ê¸ˆìœµ ì¼€ì´ìŠ¤ìŠ¤í„°ë””ë¥¼ ì œê³µí•œë‹¤.
</p>
</div>

<div class="info">
<p class="ni"><strong>ğŸ”„ Round 6 í•µì‹¬ ìš”ì•½</strong></p>
<p class="ni" style="margin-top:8px">ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ìš°ë¦¬ëŠ” <strong>í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë§¤ë§¤ ì‹œê·¸ë„ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ </strong>ì„ ë°°ì› ë‹¤:</p>
<ul>
<li><strong>NLP ê¸°ì´ˆ:</strong> í† í°í™” â†’ BoW â†’ TF-IDF â†’ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ â€” í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³ ì „ì  íŒŒì´í”„ë¼ì¸</li>
<li><strong>ì„ë² ë”©:</strong> Word2Vec(Skip-gram/CBOW) â†’ Doc2Vec(DM/DBOW) â€” ë‹¨ì–´ì™€ ë¬¸ì„œì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘</li>
<li><strong>í† í”½ ëª¨ë¸ë§:</strong> LSI â†’ pLSA â†’ LDA â€” ë¬¸ì„œì˜ ìˆ¨ê²¨ì§„ ì£¼ì œ êµ¬ì¡°ë¥¼ ë°œê²¬</li>
<li><strong>í˜„ëŒ€ NLP:</strong> Transformer(Self-Attention) â†’ BERT â†’ FinBERT â€” ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸</li>
<li><strong>ê¸ˆìœµ ì ìš©:</strong> ê°ì„±ì‚¬ì „, SEC 10-K ë¶„ì„, ë‰´ìŠ¤ ê°ì„± â†’ íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ íŒŒì´í”„ë¼ì¸</li>
</ul>
<p class="ni" style="margin-top:8px">ì´ ê¸°ìˆ ë“¤ì€ R7ì˜ ë”¥ëŸ¬ë‹(íŠ¹íˆ RNN/LSTMì˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§)ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ëœë‹¤. 
R6ì—ì„œ ë°°ìš´ Transformer/BERTëŠ” ì‚¬ì‹¤ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì˜ ì •ì ì´ë©°, R7ì—ì„œ ê·¸ ê¸°ì´ˆ(ANN â†’ CNN â†’ RNN)ë¥¼ 
ì²´ê³„ì ìœ¼ë¡œ ìŒ“ì•„ì˜¬ë¦° ë’¤ ë‹¤ì‹œ ëŒì•„ì˜¤ë©´ ë” ê¹Šì€ ì´í•´ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.</p>
</div>

</div><!-- paper-content -->
</div><!-- container -->
</div><!-- main-wrapper -->

</body>
</html>
