<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Round 6 - NLP + Sentiment Analysis for Algorithmic Trading</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300;400;500&family=Space+Mono:wght@400&family=Inter:wght@300;400&display=swap" rel="stylesheet">
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:'Inter',sans-serif;background:#fafaf8;color:#1a1a1a;line-height:1.7;overflow-x:hidden}
.sidebar{position:fixed;left:0;top:0;width:260px;height:100vh;background:rgba(255,255,255,.97);border-right:1px solid rgba(0,0,0,.06);padding:32px 24px;z-index:100;overflow-y:auto;display:flex;flex-direction:column}
.sidebar-profile{text-align:center;margin-bottom:28px;padding-bottom:24px;border-bottom:1px solid rgba(0,0,0,.08)}
.profile-icon{font-size:48px;margin-bottom:8px}
.profile-name{font-family:'Cormorant Garamond',serif;font-size:1.3rem;font-weight:500;margin-bottom:4px}
.profile-title{font-size:.68rem;color:#888;letter-spacing:.08em;text-transform:uppercase;margin-bottom:8px}
.profile-bio{font-size:.78rem;color:#666;line-height:1.5}
.sidebar-nav{flex:1;margin-top:16px}
.nav-section{margin-bottom:20px}
.nav-section-title{font-size:.6rem;font-weight:600;color:#aaa;letter-spacing:.15em;text-transform:uppercase;margin-bottom:10px}
.nav-list{list-style:none}
.nav-list li{margin-bottom:5px}
.nav-list a{font-size:.78rem;color:#555;text-decoration:none;transition:all .2s;display:block;padding:3px 0}
.nav-list a:hover{color:#0080c6;padding-left:4px}
.nav-list a.active{color:#0080c6;font-weight:500}
.nav-list a.done{color:#28a745}
.badge{display:inline-block;font-size:.5rem;background:#0080c6;color:#fff;padding:1px 5px;border-radius:8px;margin-left:3px;vertical-align:middle}
.badge-done{background:#28a745}
.sidebar-footer{padding-top:16px;border-top:1px solid rgba(0,0,0,.06);font-size:.65rem;color:#aaa;text-align:center}
.main-wrapper{margin-left:260px;min-height:100vh}
.container{max-width:1100px;margin:0 auto;padding:50px 40px 80px}
.paper-content{font-family:'Times New Roman','Nanum Myeongjo',serif;line-height:1.8;background:#fff;padding:40px;border-radius:8px;box-shadow:0 2px 20px rgba(0,0,0,.05)}
.paper-header{text-align:center;margin-bottom:40px;padding-bottom:30px;border-bottom:2px solid #333}
.paper-category{font-size:14px;color:#666;margin-bottom:10px}
.paper-title{font-size:24px;font-weight:bold;margin-bottom:12px;line-height:1.4}
.paper-subtitle{font-size:14px;color:#555;margin-bottom:8px}
.paper-team{font-size:13px;color:#444}
.code-output{background:#1e1e1e;color:#d4d4d4;padding:12px 16px;border-radius:0 0 6px 6px;font-family:'Space Mono',monospace;font-size:11.5px;line-height:1.6;margin-top:-4px;margin-bottom:18px;border-top:2px solid #333;white-space:pre-wrap;overflow-x:auto}
.code-output .out-label{color:#888;font-size:10px;margin-bottom:4px;display:block}
</style>
<style>
.abstract{background:#f8f9fa;padding:25px;margin:30px 0;border-left:4px solid #2c3e50}
.abstract-title{font-weight:bold;font-size:16px;margin-bottom:15px}
h2{font-size:18px;margin:35px 0 20px;padding-bottom:8px;border-bottom:1px solid #ddd;color:#2c3e50}
h3{font-size:15px;margin:25px 0 15px;color:#34495e}
h4{font-size:14px;margin:20px 0 12px;color:#34495e}
p{text-align:justify;margin-bottom:15px;text-indent:2em}
p.ni{text-indent:0}
table{width:100%;border-collapse:collapse;margin:20px 0;font-size:12px}
th,td{border:1px solid #ddd;padding:10px 8px;text-align:center}
th{background:#2c3e50;color:white;font-weight:bold}
tr:nth-child(even){background:#f8f9fa}
tr:hover{background:#e8f4f8}
.tc{font-size:13px;font-weight:bold;margin:15px 0 10px;text-align:center}
.eq{text-align:center;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:4px;overflow-x:auto}
ul,ol{margin-left:2em;margin-bottom:15px}
li{margin-bottom:6px}
.def{background:#fff9e6;border:1px solid #ffc107;border-radius:4px;padding:20px;margin:20px 0}
.info{background:#e8f4f8;border-left:4px solid #3498db;padding:20px;margin:20px 0}
.warn{background:#fff3cd;border-left:4px solid #f39c12;padding:20px;margin:20px 0}
.ok{background:#d4edda;border-left:4px solid #28a745;padding:20px;margin:20px 0}
pre{background:#1e1e1e;color:#d4d4d4;padding:20px;border-radius:6px;overflow-x:auto;margin:20px 0;font-family:'Space Mono','Consolas',monospace;font-size:13px;line-height:1.6}
code{font-family:'Space Mono','Consolas',monospace;font-size:13px}
p code,li code,td code{background:#f0f0f0;padding:2px 6px;border-radius:3px;color:#c7254e;font-size:12px}
.cc{font-size:12px;font-weight:bold;color:#2c3e50;margin-top:15px;margin-bottom:4px}
.cm{color:#6a9955}.kw{color:#569cd6}.st{color:#ce9178}.fn{color:#dcdcaa}.nb{color:#4ec9b0}.nu{color:#b5cea8}
.progress-bar{width:100%;height:6px;background:#e0e0e0;border-radius:3px;margin-top:16px}
.progress-fill{height:100%;background:linear-gradient(90deg,#0080c6,#00b894);border-radius:3px;width:60%}
.progress-label{font-size:11px;color:#888;margin-top:4px;text-align:center}
@media(max-width:1024px){
.sidebar{width:100%;height:auto;position:relative;border-right:none;border-bottom:1px solid rgba(0,0,0,.08);padding:16px}
.sidebar-profile{margin-bottom:10px;padding-bottom:10px;display:flex;align-items:center;gap:12px;text-align:left}
.profile-icon{font-size:32px;margin-bottom:0}.profile-bio{display:none}
.nav-section{display:inline-block;margin-right:16px;margin-bottom:8px}
.nav-list{display:flex;gap:10px;flex-wrap:wrap}.nav-list li{margin-bottom:0}
.sidebar-footer{display:none}
.main-wrapper{margin-left:0}
.container{padding:0}.paper-content{padding:20px 16px;border-radius:0;box-shadow:none}
.paper-title{font-size:18px}p{font-size:14px;text-indent:1.5em;text-align:left}
pre{font-size:11px;padding:14px}table{font-size:10px;display:block;overflow-x:auto}
}
</style>
</head>
<body>

<div class="sidebar">
<div class="sidebar-profile">
<div class="profile-icon">&#x1F680;</div>
<div class="profile-name">HFT ML Master Plan</div>
<div class="profile-title">Convex Opt + DL + HFT</div>
<div class="profile-bio">10 Rounds: Zero to HFT System Trading</div>
</div>
<div class="sidebar-nav">
<div class="nav-section">
<div class="nav-section-title">Curriculum</div>
<ul class="nav-list">
<li><a class="done" href="../round-01/">R1. Python + Finance <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-02/">R2. Linear Algebra + Stats <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-03/">R3. Data / Feature Eng. <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-04/">R4. Supervised Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-05/">R5. Unsupervised + TS <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-06/">R6. NLP + Sentiment <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round-07/">R7. Deep Learning <span class="badge badge-done">DONE</span></a></li>
<li><a href="#">R8. Convex Opt + Transformer</a></li>
<li><a href="#">R9. HFT + RL</a></li>
<li><a href="#">R10. Final Project</a></li>
</ul>
</div>
<div class="nav-section">
<div class="nav-section-title">This Lecture</div>
<ul class="nav-list">
<li><a href="#ch1">1. 텍스트 데이터의 힘</a></li>
<li><a href="#ch2">2. NLP 파이프라인</a></li>
<li><a href="#ch3">3. 토큰화와 전처리</a></li>
<li><a href="#ch4">4. Bag-of-Words</a></li>
<li><a href="#ch5">5. TF-IDF</a></li>
<li><a href="#ch6">6. 나이브 베이즈</a></li>
<li><a href="#ch7">7. 감성분석 실전</a></li>
<li><a href="#ch8">8. 토픽 모델링 (LDA)</a></li>
<li><a href="#ch9">9. Word2Vec</a></li>
<li><a href="#ch10">10. Doc2Vec + 감성</a></li>
<li><a href="#ch11">11. Transformer/BERT</a></li>
<li><a href="#ch12">12. 금융 NLP 실전</a></li>
<li><a href="#ch13">13. 피드백 + Quiz</a></li>
</ul>
</div>
</div>
<div class="sidebar-footer">Round 6 of 10 · 📝 NLP + Sentiment</div>
</div>

<div class="main-wrapper">
<div class="container">
<div class="paper-content">

<div class="paper-header">
<div class="paper-category">Round 6 / 10 · ML 핵심 무기 확장</div>
<h1 class="paper-title">NLP &amp; Sentiment Analysis for Algorithmic Trading</h1>
<div class="paper-subtitle">텍스트에서 알파를 추출한다 — 뉴스, 공시, 어닝콜이 숫자보다 먼저 말한다</div>
<div class="paper-team">Textbooks: MLAT Ch.14~16 / MLDSF Ch.11~12</div>
<div class="progress-bar"><div class="progress-fill"></div></div>
<div class="progress-label">Overall Progress: 60%</div>
</div>

<div class="abstract">
<div class="abstract-title">Abstract</div>
<p class="ni">
라운드 5까지 우리는 숫자 데이터만 다뤘다. 주가, 수익률, 변동성, 상관계수 — 모두 깔끔한 수치였다. 하지만 금융 시장을 움직이는 정보의 대부분은 텍스트로 존재한다. 연준 의장의 기자회견 한 마디가 시장을 뒤흔들고, 기업의 10-K 공시 한 문장이 주가를 10% 움직인다. Bloomberg 터미널에 쏟아지는 뉴스 헤드라인, 트위터의 실시간 여론, 애널리스트 리포트의 미묘한 어조 변화 — 이 모든 것이 알파의 원천이다.
</p>
<p class="ni" style="margin-top:10px">
이번 라운드에서는 <strong>자연어 처리(NLP)</strong>의 핵심 기법을 배운다. 텍스트를 토큰으로 쪼개고(토큰화), 숫자 벡터로 변환하고(BoW, TF-IDF), 의미를 압축하고(토픽 모델링, Word2Vec), 감성을 분류하고(나이브 베이즈, LightGBM), 최신 언어 모델(BERT, Transformer)까지 — 텍스트에서 매매 시그널을 추출하는 전체 파이프라인을 구축한다.
</p>
<p class="ni" style="margin-top:10px">
<strong>교재 연동:</strong> MLAT Ch.14 "Text Data for Trading – Sentiment Analysis" + Ch.15 "Topic Modeling – Summarizing Financial News" + Ch.16 "Word Embeddings for Earnings Calls and SEC Filings" / MLDSF Ch.11~12 (NLP 케이스스터디)
</p>
</div>


<!-- ==================== Ch.1 ==================== -->
<h2 id="ch1">Chapter 1. 텍스트 데이터의 힘 — 숫자보다 먼저 말하는 시장</h2>

<h3>1.1 왜 텍스트인가: 정보의 비대칭</h3>

<p>
금융 시장에서 가격은 정보를 반영한다 — 이것이 효율적 시장 가설(EMH)의 핵심이다. 하지만 모든 정보가 동시에 가격에 반영되는 것은 아니다. 기업의 10-K 연간 보고서가 SEC에 제출되는 순간, 그 수백 페이지의 텍스트를 읽고 해석하는 데는 시간이 걸린다. 연준 의장의 기자회견 발언이 나오는 순간, "hawkish"인지 "dovish"인지 판단하는 데도 시간이 걸린다. 이 시간 차이가 바로 알파의 원천이다.
</p>

<p>
MLAT Ch.14에서 Stefan Jansen은 이렇게 말한다: "Text data can be extremely valuable given how much information humans communicate and store using natural language." 실제로 금융 시장에서 생성되는 데이터의 80% 이상이 비정형(unstructured) 데이터이며, 그 대부분이 텍스트다. 뉴스 기사, 애널리스트 리포트, 기업 공시, 소셜 미디어, 중앙은행 의사록 — 이 모든 것이 가격에 반영되기 전에 텍스트로 먼저 존재한다.
</p>

<!-- 정보 흐름 다이어그램 -->
<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#f0f4f8,#e8eaf6);border-radius:12px;border:1px solid #c5cae9">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:20px;font-size:14px;color:#283593">📰 텍스트 → 시그널 → 가격: 정보 흐름의 시간 차</p>
<div style="display:flex;align-items:center;justify-content:center;gap:8px;flex-wrap:wrap;font-size:12px">
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #1565c0;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">📄</div>
<div style="font-weight:bold;color:#1565c0">텍스트 발생</div>
<div style="color:#777;font-size:10px;margin-top:4px">10-K 공시, 뉴스, 어닝콜</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = 0</div>
</div>
<div style="font-size:20px;color:#999">→</div>
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #7b1fa2;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">🤖</div>
<div style="font-weight:bold;color:#7b1fa2">NLP 분석</div>
<div style="color:#777;font-size:10px;margin-top:4px">감성 추출, 토픽 분류</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = 밀리초</div>
</div>
<div style="font-size:20px;color:#999">→</div>
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #2e7d32;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">📊</div>
<div style="font-weight:bold;color:#2e7d32">매매 시그널</div>
<div style="color:#777;font-size:10px;margin-top:4px">Long/Short 결정</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = 초</div>
</div>
<div style="font-size:20px;color:#999">→</div>
<div style="background:#fff;padding:14px 18px;border-radius:10px;border:2px solid #e65100;text-align:center;min-width:130px">
<div style="font-size:24px;margin-bottom:4px">💰</div>
<div style="font-weight:bold;color:#e65100">가격 반영</div>
<div style="color:#777;font-size:10px;margin-top:4px">시장 참여자 반응</div>
<div style="color:#999;font-size:9px;margin-top:2px">t = 분~시간</div>
</div>
</div>
<p class="ni" style="text-align:center;margin-top:14px;font-size:11px;color:#666">NLP로 텍스트를 밀리초 단위로 분석하면, 시장이 반응하기 전에 포지션을 잡을 수 있다</p>
</div>

<h3>1.2 금융 텍스트 데이터의 종류</h3>

<p>
금융에서 활용 가능한 텍스트 데이터는 크게 네 가지 범주로 나뉜다. 각각의 특성과 활용 방법이 다르므로, NLP 파이프라인을 설계할 때 데이터 소스의 특성을 이해하는 것이 중요하다.
</p>

<div class="tc">표 1-1. 금융 텍스트 데이터의 4대 범주</div>
<table>
<thead>
<tr><th>범주</th><th>예시</th><th>특성</th><th>빈도</th><th>NLP 활용</th></tr>
</thead>
<tbody>
<tr><td><strong>기업 공시</strong></td><td>10-K, 10-Q, 8-K, 어닝콜 트랜스크립트</td><td>공식적, 법적 구속력, 장문</td><td>분기/연간</td><td>감성분석, 토픽 변화 추적</td></tr>
<tr><td><strong>뉴스</strong></td><td>Bloomberg, Reuters, WSJ, FT</td><td>실시간, 간결, 헤드라인 중심</td><td>초~분 단위</td><td>이벤트 감지, 감성 스코어링</td></tr>
<tr><td><strong>애널리스트 리포트</strong></td><td>Goldman Sachs, Morgan Stanley 리서치</td><td>전문적, 의견 포함, 타겟 프라이스</td><td>수시</td><td>추천 변경 감지, 어조 분석</td></tr>
<tr><td><strong>소셜 미디어</strong></td><td>Twitter/X, Reddit, StockTwits</td><td>비공식, 노이즈 많음, 실시간</td><td>초 단위</td><td>군중 감성, 밈 주식 감지</td></tr>
</tbody>
</table>

<h3>1.3 R5까지의 연결: 숫자 → 텍스트</h3>

<p>
지금까지 우리가 배운 것을 정리해보자. R1에서 파이썬 기초를 익히고, R2에서 선형대수/통계를 배우고, R3에서 피처 엔지니어링을 하고, R4에서 지도학습 모델을 만들고, R5에서 비지도학습과 시계열을 다뤘다. 이 모든 과정에서 입력 데이터는 항상 숫자였다 — 주가, 수익률, 기술적 지표, 공분산 행렬.
</p>

<p>
이번 라운드에서는 입력 데이터가 근본적으로 달라진다. 텍스트는 숫자가 아니다. "Apple reported strong earnings"라는 문장을 XGBoost에 바로 넣을 수 없다. 텍스트를 숫자로 변환하는 과정 — 이것이 NLP의 핵심이다. 그리고 이 변환 방법에 따라 모델의 성능이 극적으로 달라진다.
</p>

<div class="def">
<p class="ni"><strong>이번 라운드의 핵심 질문</strong></p>
<p class="ni" style="margin-top:8px">"텍스트를 어떻게 숫자로 바꿀 것인가?" — 이 질문에 대한 답이 NLP의 역사 그 자체다.</p>
<ol style="margin-top:10px">
<li><strong>Bag-of-Words (Ch.4~5):</strong> 단어의 출현 빈도를 센다 → 희소(sparse) 벡터</li>
<li><strong>TF-IDF (Ch.5):</strong> 빈도에 중요도 가중치를 곱한다 → 개선된 희소 벡터</li>
<li><strong>토픽 모델링 (Ch.8):</strong> 문서의 잠재 주제를 추출한다 → 저차원 밀집(dense) 벡터</li>
<li><strong>Word2Vec (Ch.9):</strong> 단어의 의미를 벡터 공간에 임베딩한다 → 밀집 벡터</li>
<li><strong>Transformer/BERT (Ch.11):</strong> 문맥까지 고려한 임베딩 → 최신 기술</li>
</ol>
</div>

<h3>1.4 이번 라운드의 로드맵</h3>

<!-- 로드맵 다이어그램 -->
<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#f8f9fa,#e8eaf6);border-radius:12px;border:1px solid #c5cae9">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:20px;font-size:15px;color:#283593">🗺️ Round 6 학습 로드맵</p>
<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(220px,1fr));gap:14px">
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #1976d2">
<div style="font-size:11px;color:#1976d2;font-weight:bold;margin-bottom:4px">PART 1 · Ch.1~3</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">📝 텍스트 전처리</div>
<div style="font-size:11px;color:#666;line-height:1.5">텍스트를 토큰으로 쪼개고<br>정제하여 분석 가능한 형태로 만든다</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#e3f2fd;color:#1565c0;padding:2px 8px;border-radius:10px;font-size:10px">토큰화</span>
<span style="background:#e3f2fd;color:#1565c0;padding:2px 8px;border-radius:10px;font-size:10px">spaCy</span>
<span style="background:#e3f2fd;color:#1565c0;padding:2px 8px;border-radius:10px;font-size:10px">표제어화</span>
</div>
</div>
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #7b1fa2">
<div style="font-size:11px;color:#7b1fa2;font-weight:bold;margin-bottom:4px">PART 2 · Ch.4~7</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">📊 벡터화 + 감성분석</div>
<div style="font-size:11px;color:#666;line-height:1.5">BoW, TF-IDF로 벡터화하고<br>나이브 베이즈로 감성을 분류한다</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#f3e5f5;color:#6a1b9a;padding:2px 8px;border-radius:10px;font-size:10px">BoW</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:2px 8px;border-radius:10px;font-size:10px">TF-IDF</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:2px 8px;border-radius:10px;font-size:10px">Naive Bayes</span>
</div>
</div>
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #2e7d32">
<div style="font-size:11px;color:#2e7d32;font-weight:bold;margin-bottom:4px">PART 3 · Ch.8~10</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">🧠 임베딩 + 토픽</div>
<div style="font-size:11px;color:#666;line-height:1.5">LDA로 토픽을 추출하고<br>Word2Vec/Doc2Vec으로 의미를 임베딩한다</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#e8f5e9;color:#1b5e20;padding:2px 8px;border-radius:10px;font-size:10px">LDA</span>
<span style="background:#e8f5e9;color:#1b5e20;padding:2px 8px;border-radius:10px;font-size:10px">Word2Vec</span>
<span style="background:#e8f5e9;color:#1b5e20;padding:2px 8px;border-radius:10px;font-size:10px">Doc2Vec</span>
</div>
</div>
<div style="background:#fff;padding:16px;border-radius:10px;box-shadow:0 2px 8px rgba(0,0,0,.08);border-left:5px solid #e65100">
<div style="font-size:11px;color:#e65100;font-weight:bold;margin-bottom:4px">PART 4 · Ch.11~13</div>
<div style="font-size:14px;font-weight:bold;color:#1a1a1a;margin-bottom:6px">🔮 최신 기술 + 실전</div>
<div style="font-size:11px;color:#666;line-height:1.5">Transformer/BERT를 이해하고<br>금융 NLP 파이프라인을 구축한다</div>
<div style="margin-top:8px;display:flex;gap:4px;flex-wrap:wrap">
<span style="background:#fff3e0;color:#e65100;padding:2px 8px;border-radius:10px;font-size:10px">BERT</span>
<span style="background:#fff3e0;color:#e65100;padding:2px 8px;border-radius:10px;font-size:10px">FinBERT</span>
<span style="background:#fff3e0;color:#e65100;padding:2px 8px;border-radius:10px;font-size:10px">피드백</span>
</div>
</div>
</div>
<div style="text-align:center;margin-top:14px;font-size:11px;color:#666">
<span style="display:inline-block;width:10px;height:10px;background:#1976d2;border-radius:50%;margin-right:3px;vertical-align:middle"></span> 전처리
<span style="margin:0 8px">→</span>
<span style="display:inline-block;width:10px;height:10px;background:#7b1fa2;border-radius:50%;margin-right:3px;vertical-align:middle"></span> 벡터화+감성
<span style="margin:0 8px">→</span>
<span style="display:inline-block;width:10px;height:10px;background:#2e7d32;border-radius:50%;margin-right:3px;vertical-align:middle"></span> 임베딩
<span style="margin:0 8px">→</span>
<span style="display:inline-block;width:10px;height:10px;background:#e65100;border-radius:50%;margin-right:3px;vertical-align:middle"></span> 실전
</div>
</div>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.14의 제목은 "Text Data for Trading – Sentiment Analysis"이다. 제목이 모든 것을 말해준다 — 텍스트 데이터를 트레이딩에 활용하되, 감성분석이 핵심 도구다. Ch.15 "Topic Modeling – Summarizing Financial News"는 어닝콜과 금융 뉴스의 토픽을 추출하고, Ch.16 "Word Embeddings for Earnings Calls and SEC Filings"는 SEC 공시에서 워드 임베딩을 학습하여 주가 예측에 활용한다.</p>
</div>


<!-- ==================== Ch.2 ==================== -->
<h2 id="ch2">Chapter 2. NLP 파이프라인 — 텍스트에서 피처까지의 여정</h2>

<h3>2.1 NLP 워크플로우 개관</h3>

<p>
MLAT Ch.14에서 Jansen은 NLP 워크플로우를 명확하게 정리한다. 텍스트 데이터를 ML 모델에 넣기까지는 여러 단계의 전처리가 필요하다. 각 단계가 왜 필요한지, 어떤 선택지가 있는지 이해하는 것이 중요하다.
</p>

<!-- NLP 파이프라인 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:18px;font-size:14px;color:#2c3e50">🔧 NLP 파이프라인: 텍스트 → ML 피처</p>
<div style="display:flex;flex-direction:column;gap:10px;max-width:700px;margin:0 auto">

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#1976d2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">1</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#1976d2">텍스트 수집 (Data Acquisition)</div>
<div style="font-size:11px;color:#666;margin-top:4px">뉴스 API, SEC EDGAR, 웹 스크래핑, 소셜 미디어 API</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">2</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#7b1fa2">파싱 + 토큰화 (Parsing & Tokenization)</div>
<div style="font-size:11px;color:#666;margin-top:4px">문장 분리 → 단어 분리 → 소문자화 → 불용어 제거 → 표제어화(Lemmatization)</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">3</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#2e7d32">언어학적 주석 (Linguistic Annotation)</div>
<div style="font-size:11px;color:#666;margin-top:4px">품사 태깅(POS), 개체명 인식(NER), 의존 구문 분석(Dependency Parsing)</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#e65100;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">4</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#e65100">벡터화 (Vectorization)</div>
<div style="font-size:11px;color:#666;margin-top:4px">BoW / TF-IDF → 희소 행렬 | Word2Vec / Doc2Vec → 밀집 벡터 | BERT → 문맥 임베딩</div>
</div>
</div>

<div style="display:flex;align-items:center;gap:12px">
<div style="min-width:36px;height:36px;background:#c62828;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:14px">5</div>
<div style="flex:1;background:#fff;padding:12px 16px;border-radius:8px;border:1px solid #e0e0e0">
<div style="font-weight:bold;font-size:13px;color:#c62828">ML 모델링 (Classification / Regression)</div>
<div style="font-size:11px;color:#666;margin-top:4px">나이브 베이즈, LightGBM, 로지스틱 회귀, 딥러닝 → 감성 점수, 토픽 분류, 시그널 생성</div>
</div>
</div>

</div>
</div>

<h3>2.2 핵심 개념: 문서(Document)와 코퍼스(Corpus)</h3>

<p>
NLP에서 가장 기본적인 용어 두 가지를 정리하자. <strong>문서(Document)</strong>는 분석의 기본 단위다. 하나의 뉴스 기사, 하나의 10-K 공시, 하나의 트윗이 각각 하나의 문서다. <strong>코퍼스(Corpus)</strong>는 문서의 집합이다. "2024년 Bloomberg 금융 뉴스 전체"가 하나의 코퍼스가 된다.
</p>

<p>
R4의 지도학습과 비교하면 이해가 쉽다. R4에서 하나의 "샘플"이 하나의 종목-날짜 조합이었다면, NLP에서 하나의 "샘플"은 하나의 문서다. R4에서 "피처"가 RSI, MACD 같은 숫자였다면, NLP에서 "피처"는 단어의 출현 빈도나 임베딩 벡터다.
</p>

<div class="tc">표 2-1. R4 지도학습 vs R6 NLP의 대응 관계</div>
<table>
<thead>
<tr><th>개념</th><th>R4 (숫자 데이터)</th><th>R6 (텍스트 데이터)</th></tr>
</thead>
<tbody>
<tr><td><strong>샘플</strong></td><td>종목-날짜 조합</td><td>문서 (뉴스 기사, 공시)</td></tr>
<tr><td><strong>피처</strong></td><td>RSI, MACD, 볼린저밴드</td><td>단어 빈도, TF-IDF, 임베딩 벡터</td></tr>
<tr><td><strong>라벨</strong></td><td>상승(1) / 하락(0)</td><td>긍정 / 부정 / 중립</td></tr>
<tr><td><strong>피처 행렬</strong></td><td>N × 20 (밀집)</td><td>N × 50,000 (희소) 또는 N × 300 (밀집)</td></tr>
<tr><td><strong>모델</strong></td><td>XGBoost, RF</td><td>나이브 베이즈, LightGBM, BERT</td></tr>
</tbody>
</table>

<h3>2.3 NLP의 핵심 도전: 언어의 모호성</h3>

<p>
숫자 데이터와 달리, 텍스트 데이터에는 고유한 도전이 있다. 같은 단어가 다른 의미를 가질 수 있고(다의어), 다른 단어가 같은 의미를 가질 수 있다(동의어). "Apple"이 과일인지 회사인지는 문맥을 봐야 안다. "좋다"와 "훌륭하다"는 다른 단어지만 비슷한 감성을 표현한다.
</p>

<div class="warn">
<p class="ni"><strong>⚠️ 금융 텍스트의 특수한 도전 (MLAT Ch.14)</strong></p>
<ul>
<li><strong>도메인 특수 어휘:</strong> "bull market", "dovish", "headwinds" 같은 금융 전문 용어는 일반 NLP 모델이 잘 처리하지 못한다.</li>
<li><strong>미묘한 어조:</strong> "We expect moderate growth"와 "We expect robust growth"의 차이는 미묘하지만 시장에 큰 영향을 미친다.</li>
<li><strong>부정의 복잡성:</strong> "not unlikely to raise rates"는 이중 부정으로, 단순 감성 사전으로는 잘못 분류될 수 있다.</li>
<li><strong>시간 민감성:</strong> 같은 문장이라도 시장 상황에 따라 해석이 달라진다. "inflation is rising"은 2020년과 2023년에 전혀 다른 의미를 갖는다.</li>
</ul>
</div>


<h3>2.4 NLP 기법의 진화: 전통 → 딥러닝</h3>

<p>
NLP 기법은 크게 세 세대로 나눌 수 있다. 각 세대는 이전 세대의 한계를 극복하면서 발전했다. 
이번 라운드에서 우리는 이 세 세대를 모두 다룬다.
</p>

<!-- NLP 진화 타임라인 -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:18px;font-size:14px;color:#2c3e50">NLP 기법의 3세대 진화</p>

<div style="display:flex;gap:16px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:220px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #ff9800">
<p class="ni" style="font-weight:bold;color:#e65100;font-size:13px;margin-bottom:8px">🏛️ 1세대: 규칙 기반 (1950s~)</p>
<ul style="font-size:11px;color:#555;margin:0;padding-left:16px;line-height:1.7">
<li>감성 사전 (Loughran-McDonald)</li>
<li>정규표현식 패턴 매칭</li>
<li>규칙 기반 파싱</li>
</ul>
<p class="ni" style="font-size:10px;color:#999;margin-top:8px">장점: 해석 가능, 도메인 지식 반영<br>한계: 확장성 부족, 문맥 무시</p>
</div>

<div style="flex:1;min-width:220px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #2196f3">
<p class="ni" style="font-weight:bold;color:#1565c0;font-size:13px;margin-bottom:8px">📊 2세대: 통계/ML 기반 (1990s~)</p>
<ul style="font-size:11px;color:#555;margin:0;padding-left:16px;line-height:1.7">
<li>BoW, TF-IDF (Ch.4~5)</li>
<li>나이브 베이즈, SVM (Ch.6)</li>
<li>LDA 토픽 모델링 (Ch.8)</li>
<li>Word2Vec, Doc2Vec (Ch.9~10)</li>
</ul>
<p class="ni" style="font-size:10px;color:#999;margin-top:8px">장점: 데이터 기반 학습, 확장 가능<br>한계: 고정 벡터, 장거리 의존성 부족</p>
</div>

<div style="flex:1;min-width:220px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #4caf50">
<p class="ni" style="font-weight:bold;color:#2e7d32;font-size:13px;margin-bottom:8px">🤖 3세대: 딥러닝/사전학습 (2017~)</p>
<ul style="font-size:11px;color:#555;margin:0;padding-left:16px;line-height:1.7">
<li>Transformer / Self-Attention (Ch.11)</li>
<li>BERT, GPT (사전학습)</li>
<li>FinBERT (금융 특화)</li>
</ul>
<p class="ni" style="font-size:10px;color:#999;margin-top:8px">장점: 문맥 이해, 전이학습, SOTA 성능<br>한계: 계산 비용, 블랙박스</p>
</div>
</div>
</div>

<h3>2.5 금융 NLP의 데이터 소스 (MLAT Ch.14)</h3>

<p>
Jansen은 MLAT Ch.14에서 금융 NLP에 사용되는 주요 데이터 소스를 체계적으로 정리한다. 
각 소스의 특성과 접근 방법을 이해하는 것이 실전 파이프라인 구축의 첫 걸음이다.
</p>

<div class="tc">표 2-2. 금융 NLP 데이터 소스 (MLAT Ch.14 기반)</div>
<table>
<thead>
<tr><th>데이터 소스</th><th>접근 방법</th><th>갱신 주기</th><th>NLP 활용</th><th>난이도</th></tr>
</thead>
<tbody>
<tr><td><strong>SEC EDGAR</strong></td><td>EDGAR Full-Text Search API</td><td>분기/연간</td><td>10-K/10-Q 감성 변화 추적</td><td>⭐⭐</td></tr>
<tr><td><strong>어닝콜 트랜스크립트</strong></td><td>Seeking Alpha, S&P Capital IQ</td><td>분기</td><td>경영진 어조 분석, Q&A 감성</td><td>⭐⭐⭐</td></tr>
<tr><td><strong>금융 뉴스</strong></td><td>Reuters, Bloomberg, NewsAPI</td><td>실시간</td><td>이벤트 감지, 감성 시그널</td><td>⭐⭐</td></tr>
<tr><td><strong>연준 의사록</strong></td><td>Federal Reserve 웹사이트</td><td>6주마다</td><td>통화정책 방향 예측</td><td>⭐⭐</td></tr>
<tr><td><strong>소셜 미디어</strong></td><td>Twitter/X API, Reddit API</td><td>실시간</td><td>군중 심리, 밈 주식 감지</td><td>⭐</td></tr>
<tr><td><strong>애널리스트 리포트</strong></td><td>Bloomberg, Refinitiv</td><td>수시</td><td>목표가 변경, 투자의견 추출</td><td>⭐⭐⭐</td></tr>
</tbody>
</table>

<div class="ok">
<p class="ni"><strong>실전 팁: 무료로 시작하기</strong></p>
<p class="ni" style="margin-top:8px">
학습 단계에서는 비용이 들지 않는 데이터 소스부터 시작하는 것이 현실적이다. 
SEC EDGAR는 완전 무료이고 API가 잘 정비되어 있다. 연준 의사록도 공개 데이터다. 
NewsAPI는 무료 플랜(하루 100건)을 제공한다. Reddit은 PRAW 라이브러리로 쉽게 접근할 수 있다. 
Bloomberg Terminal이 없어도 충분히 의미 있는 NLP 프로젝트를 수행할 수 있다.
</p>
</div>

<!-- ==================== Ch.3 ==================== -->
<h2 id="ch3">Chapter 3. 토큰화와 전처리 — 텍스트를 원자 단위로 쪼개다</h2>

<h3>3.1 토큰(Token)이란 무엇인가</h3>

<p>
토큰(Token)은 텍스트의 최소 분석 단위다. 보통 하나의 단어가 하나의 토큰이지만, 반드시 그런 것은 아니다. "New York"은 두 단어지만 하나의 의미 단위이므로 하나의 토큰으로 처리하는 것이 좋다. "don't"는 "do"와 "not"으로 분리할 수도 있고, 하나의 토큰으로 유지할 수도 있다. 이런 선택이 모델 성능에 영향을 미친다.
</p>

<p>
MLAT Ch.14에서 Jansen은 토큰화의 핵심을 이렇게 정리한다: "A token is an instance of a sequence of characters in a given document and is considered a useful semantic unit for processing." 즉, 토큰은 의미 있는 최소 단위여야 한다.
</p>

<h3>3.2 spaCy를 이용한 NLP 파이프라인</h3>

<p>
MLAT에서 주로 사용하는 NLP 라이브러리는 <code>spaCy</code>다. spaCy는 산업용 NLP 라이브러리로, 토큰화, 품사 태깅, 개체명 인식, 의존 구문 분석을 한 번에 처리한다. NLTK보다 빠르고, 파이프라인 방식으로 설계되어 있어 대량의 텍스트를 효율적으로 처리할 수 있다.
</p>

<div class="cc">코드 3-1. spaCy 기본 파이프라인</div>
<pre><code><span class="kw">import</span> spacy

<span class="cm"># 영어 모델 로드 (처음 한 번: python -m spacy download en_core_web_sm)</span>
nlp = spacy.<span class="fn">load</span>(<span class="st">'en_core_web_sm'</span>)

<span class="cm"># 금융 뉴스 예시 문장</span>
text = <span class="st">"Apple reported strong Q4 earnings, beating Wall Street estimates by 15%."</span>

<span class="cm"># spaCy 파이프라인 실행 — 한 줄로 토큰화 + POS + NER + 의존구문 분석</span>
doc = <span class="fn">nlp</span>(text)

<span class="cm"># 각 토큰의 정보 출력</span>
<span class="fn">print</span>(<span class="st">f"{'토큰':<12} {'표제어':<12} {'품사':<8} {'태그':<6} {'의존관계':<10} {'불용어?'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">70</span>)
<span class="kw">for</span> token <span class="kw">in</span> doc:
    <span class="fn">print</span>(<span class="st">f"{token.text:<12} {token.lemma_:<12} {token.pos_:<8} {token.tag_:<6} {token.dep_:<10} {token.is_stop}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>실행 결과:</strong></p>
<pre style="background:#f8f9fa;color:#333;font-size:11px;padding:12px"><code>토큰         표제어        품사     태그    의존관계     불용어?
----------------------------------------------------------------------
Apple        apple        PROPN    NNP    nsubj      False
reported     report       VERB     VBD    ROOT       False
strong       strong       ADJ      JJ     amod       False
Q4           q4           NOUN     NN     compound   False
earnings     earning      NOUN     NNS    dobj       False
,            ,            PUNCT    ,      punct      False
beating      beat         VERB     VBG    advcl      False
Wall         Wall         PROPN    NNP    compound   False
Street       Street       PROPN    NNP    compound   False
estimates    estimate     NOUN     NNS    dobj       False
by           by           ADP      IN     prep       True
15           15           NUM      CD     pobj       False
%            %            NOUN     NN     nmod       False
.            .            PUNCT    .      punct      False</code></pre>
</div>

<p>
이 한 줄의 코드(<code>nlp(text)</code>)가 내부적으로 수행하는 작업을 분해해보자:
</p>

<div class="tc">표 3-1. spaCy 파이프라인의 각 단계</div>
<table>
<thead>
<tr><th>단계</th><th>이름</th><th>설명</th><th>예시</th></tr>
</thead>
<tbody>
<tr><td>1</td><td><strong>Tokenizer</strong></td><td>텍스트를 토큰으로 분리</td><td>"don't" → "do", "n't"</td></tr>
<tr><td>2</td><td><strong>Tagger</strong></td><td>품사(POS) 태깅</td><td>"reported" → VERB</td></tr>
<tr><td>3</td><td><strong>Parser</strong></td><td>의존 구문 분석</td><td>"Apple" → nsubj (주어)</td></tr>
<tr><td>4</td><td><strong>NER</strong></td><td>개체명 인식</td><td>"Apple" → ORG, "15%" → PERCENT</td></tr>
<tr><td>5</td><td><strong>Lemmatizer</strong></td><td>표제어 추출</td><td>"reported" → "report"</td></tr>
</tbody>
</table>

<h3>3.3 표제어화(Lemmatization) vs 어간 추출(Stemming)</h3>

<p>
같은 단어의 변형을 하나로 통합하는 방법은 두 가지가 있다. <strong>어간 추출(Stemming)</strong>은 규칙 기반으로 접미사를 잘라낸다. "running" → "run", "better" → "bet" (오류!). 빠르지만 부정확하다. <strong>표제어화(Lemmatization)</strong>은 사전을 참조하여 원형을 찾는다. "running" → "run", "better" → "good". 느리지만 정확하다.
</p>

<div class="cc">코드 3-2. Stemming vs Lemmatization 비교</div>
<pre><code><span class="kw">from</span> nltk.stem <span class="kw">import</span> SnowballStemmer
<span class="kw">import</span> spacy

stemmer = <span class="fn">SnowballStemmer</span>(<span class="st">'english'</span>)
nlp = spacy.<span class="fn">load</span>(<span class="st">'en_core_web_sm'</span>)

words = [<span class="st">'running'</span>, <span class="st">'better'</span>, <span class="st">'earnings'</span>, <span class="st">'reported'</span>, <span class="st">'companies'</span>]

<span class="fn">print</span>(<span class="st">f"{'원본':<12} {'Stemming':<12} {'Lemmatization'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">40</span>)
<span class="kw">for</span> word <span class="kw">in</span> words:
    stem = stemmer.<span class="fn">stem</span>(word)
    doc = <span class="fn">nlp</span>(word)
    lemma = doc[<span class="nu">0</span>].lemma_
    <span class="fn">print</span>(<span class="st">f"{word:<12} {stem:<12} {lemma}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>실행 결과:</strong></p>
<pre style="background:#f8f9fa;color:#333;font-size:11px;padding:12px"><code>원본         Stemming     Lemmatization
----------------------------------------
running      run          run
better       better       well
earnings     earn         earning
reported     report       report
companies    compani      company</code></pre>
<p class="ni" style="margin-top:8px;font-size:12px"><strong>핵심:</strong> Stemming은 "companies"를 "compani"로 잘못 자르지만, Lemmatization은 "company"로 정확히 복원한다. 금융 텍스트에서는 정확성이 중요하므로 Lemmatization을 권장한다.</p>
</div>

<h3>3.4 불용어(Stop Words) 제거</h3>

<p>
"the", "is", "at", "which" 같은 단어는 거의 모든 문서에 등장하므로 문서를 구별하는 데 도움이 되지 않는다. 이런 단어를 <strong>불용어(Stop Words)</strong>라고 하며, 보통 전처리 단계에서 제거한다. spaCy는 각 토큰에 <code>is_stop</code> 속성을 제공하여 불용어 여부를 쉽게 판별할 수 있다.
</p>

<div class="warn">
<p class="ni"><strong>⚠️ 금융에서 불용어 제거 시 주의점</strong></p>
<p class="ni" style="margin-top:8px">일반적인 불용어 목록을 그대로 쓰면 위험할 수 있다. 예를 들어 "not"은 일반 NLP에서 불용어로 분류되지만, 금융 감성분석에서는 핵심 단어다. "not profitable"에서 "not"을 제거하면 "profitable"만 남아 감성이 완전히 뒤집힌다. 따라서 금융 NLP에서는 불용어 목록을 커스터마이즈해야 한다.</p>
</div>

<h3>3.5 개체명 인식(NER): 텍스트에서 핵심 엔티티 추출</h3>

<p>
MLAT Ch.14에서 강조하는 또 하나의 핵심 기법이 <strong>개체명 인식(Named Entity Recognition, NER)</strong>이다. NER은 텍스트에서 사람, 조직, 장소, 날짜, 금액 등의 엔티티를 자동으로 식별한다. 금융에서는 특히 기업명, 인물명, 금액, 날짜를 정확히 추출하는 것이 중요하다.
</p>

<div class="cc">코드 3-3. spaCy NER로 금융 엔티티 추출</div>
<pre><code>nlp = spacy.<span class="fn">load</span>(<span class="st">'en_core_web_sm'</span>)

text = <span class="st">"""
Federal Reserve Chair Jerome Powell signaled on Wednesday that the central bank 
may cut interest rates by 25 basis points in September. Goldman Sachs analysts 
expect the S&P 500 to reach 5,500 by year-end.
"""</span>

doc = <span class="fn">nlp</span>(text)

<span class="fn">print</span>(<span class="st">f"{'엔티티':<25} {'라벨':<12} {'설명'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">60</span>)
<span class="kw">for</span> ent <span class="kw">in</span> doc.ents:
    <span class="fn">print</span>(<span class="st">f"{ent.text:<25} {ent.label_:<12} {spacy.explain(ent.label_)}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>실행 결과:</strong></p>
<pre style="background:#f8f9fa;color:#333;font-size:11px;padding:12px"><code>엔티티                    라벨         설명
------------------------------------------------------------
Federal Reserve          ORG          Companies, agencies, institutions
Jerome Powell            PERSON       People, including fictional
Wednesday                DATE         Absolute or relative dates
25 basis points          QUANTITY     Measurements
September                DATE         Absolute or relative dates
Goldman Sachs            ORG          Companies, agencies, institutions
S&P 500                  ORG          Companies, agencies, institutions
5,500                    CARDINAL     Numerals
year-end                 DATE         Absolute or relative dates</code></pre>
</div>

<h3>3.6 N-gram: 연속 토큰의 조합</h3>

<p>
단일 토큰만으로는 포착할 수 없는 의미가 있다. "interest rate"는 "interest"와 "rate"를 따로 보면 의미가 달라진다. <strong>N-gram</strong>은 N개의 연속 토큰을 하나의 단위로 묶는 기법이다. Bigram(2-gram)은 "interest rate", "stock market", "earnings call" 같은 2단어 조합이고, Trigram(3-gram)은 "federal reserve bank", "year over year" 같은 3단어 조합이다.
</p>

<p>
MLAT Ch.14에서는 N-gram이 Bag-of-Words 모델에서 특히 유용하다고 강조한다. 단어 순서 정보가 완전히 사라지는 BoW에서, bigram을 포함하면 일부 순서 정보를 보존할 수 있기 때문이다.
</p>

<div class="cc">코드 3-4. sklearn CountVectorizer로 N-gram 생성</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer

corpus = [
    <span class="st">"Federal Reserve raised interest rates"</span>,
    <span class="st">"Interest rates impact stock market"</span>,
    <span class="st">"Stock market rallied after Fed announcement"</span>
]

<span class="cm"># unigram + bigram</span>
vectorizer = <span class="fn">CountVectorizer</span>(ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>), stop_words=<span class="st">'english'</span>)
X = vectorizer.<span class="fn">fit_transform</span>(corpus)

<span class="fn">print</span>(<span class="st">"어휘 (unigram + bigram):"</span>)
<span class="kw">for</span> term, idx <span class="kw">in</span> <span class="fn">sorted</span>(vectorizer.vocabulary_.items(), key=<span class="kw">lambda</span> x: x[<span class="nu">1</span>]):
    <span class="fn">print</span>(<span class="st">f"  [{idx:2d}] {term}"</span>)

<span class="fn">print</span>(<span class="st">f"\n문서-용어 행렬 (shape: {X.shape}):"</span>)
<span class="fn">print</span>(X.<span class="fn">toarray</span>())
</code></pre>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.14에서 Jansen은 BBC 뉴스 2,225개 기사를 사용하여 CountVectorizer의 다양한 설정(min_df, max_df, ngram_range)이 어휘 크기와 모델 성능에 미치는 영향을 실험한다. 핵심 발견: bigram을 포함하면 어휘 크기가 크게 늘어나지만, 분류 정확도도 향상된다.</p>
</div>


<!-- ==================== Ch.4 ==================== -->
<h2 id="ch4">Chapter 4. Bag-of-Words — 단어를 세는 가장 단순한 방법</h2>

<h3>4.1 BoW의 핵심 아이디어</h3>

<p>
Bag-of-Words(BoW)는 텍스트를 숫자로 변환하는 가장 기본적인 방법이다. 아이디어는 놀라울 정도로 단순하다: 문서에 등장하는 각 단어의 빈도를 세서 벡터로 만든다. 단어의 순서는 완전히 무시한다 — 그래서 "bag(가방)"이라는 이름이 붙었다. 단어들을 가방에 넣고 흔들면 순서가 사라지지만, 어떤 단어가 몇 개 있는지는 알 수 있다.
</p>

<div class="def">
<p class="ni"><strong>Bag-of-Words 모델 (한 문장 정의)</strong></p>
<p class="ni" style="margin-top:8px">문서를 어휘(vocabulary)의 각 단어가 몇 번 등장하는지를 나타내는 벡터로 표현한다. 결과는 <strong>문서-용어 행렬(Document-Term Matrix, DTM)</strong>이다.</p>
</div>

<h3>4.2 문서-용어 행렬(DTM) 구축</h3>

<p>
구체적인 예를 들어보자. 세 개의 금융 뉴스 헤드라인이 있다고 하자:
</p>

<ul>
<li>Doc 1: "Fed raises rates"</li>
<li>Doc 2: "Fed cuts rates sharply"</li>
<li>Doc 3: "Market rallies after rate cuts"</li>
</ul>

<p>
전체 어휘는 {after, cuts, fed, market, raises, rallies, rate, rates, sharply}이다. 각 문서를 이 어휘에 대한 빈도 벡터로 표현하면:
</p>

<!-- DTM 시각화 -->
<div style="margin:20px 0;overflow-x:auto">
<table style="font-size:12px">
<thead>
<tr><th></th><th>after</th><th>cuts</th><th>fed</th><th>market</th><th>raises</th><th>rallies</th><th>rate</th><th>rates</th><th>sharply</th></tr>
</thead>
<tbody>
<tr><td class="left"><strong>Doc 1</strong></td><td>0</td><td>0</td><td style="background:#e3f2fd;font-weight:bold">1</td><td>0</td><td style="background:#e3f2fd;font-weight:bold">1</td><td>0</td><td>0</td><td style="background:#e3f2fd;font-weight:bold">1</td><td>0</td></tr>
<tr><td class="left"><strong>Doc 2</strong></td><td>0</td><td style="background:#f3e5f5;font-weight:bold">1</td><td style="background:#f3e5f5;font-weight:bold">1</td><td>0</td><td>0</td><td>0</td><td>0</td><td style="background:#f3e5f5;font-weight:bold">1</td><td style="background:#f3e5f5;font-weight:bold">1</td></tr>
<tr><td class="left"><strong>Doc 3</strong></td><td style="background:#e8f5e9;font-weight:bold">1</td><td style="background:#e8f5e9;font-weight:bold">1</td><td>0</td><td style="background:#e8f5e9;font-weight:bold">1</td><td>0</td><td style="background:#e8f5e9;font-weight:bold">1</td><td style="background:#e8f5e9;font-weight:bold">1</td><td>0</td><td>0</td></tr>
</tbody>
</table>
</div>

<p>
이것이 문서-용어 행렬(DTM)이다. 행은 문서, 열은 어휘의 각 단어, 값은 출현 빈도다. 이제 각 문서는 9차원 벡터로 표현된다. 이 벡터를 R4에서 배운 ML 모델에 바로 넣을 수 있다!
</p>

<h3>4.3 sklearn CountVectorizer 실전</h3>

<div class="cc">코드 4-1. CountVectorizer로 DTM 구축</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer
<span class="kw">import</span> pandas <span class="kw">as</span> pd

<span class="cm"># 금융 뉴스 코퍼스</span>
corpus = [
    <span class="st">"Apple reported record quarterly earnings beating analyst expectations"</span>,
    <span class="st">"Tesla shares dropped after disappointing earnings report"</span>,
    <span class="st">"Federal Reserve raised interest rates citing persistent inflation"</span>,
    <span class="st">"Goldman Sachs upgraded Apple stock to buy rating"</span>,
    <span class="st">"Oil prices surged amid supply disruptions in Middle East"</span>
]

<span class="cm"># CountVectorizer 설정</span>
vectorizer = <span class="fn">CountVectorizer</span>(
    stop_words=<span class="st">'english'</span>,     <span class="cm"># 영어 불용어 제거</span>
    min_df=<span class="nu">1</span>,                  <span class="cm"># 최소 1개 문서에 등장</span>
    max_df=<span class="nu">0.9</span>,                <span class="cm"># 90% 이상 문서에 등장하면 제거</span>
    ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>),       <span class="cm"># unigram + bigram</span>
    max_features=<span class="nu">1000</span>          <span class="cm"># 최대 1000개 피처</span>
)

<span class="cm"># DTM 생성</span>
dtm = vectorizer.<span class="fn">fit_transform</span>(corpus)

<span class="cm"># DataFrame으로 시각화</span>
feature_names = vectorizer.<span class="fn">get_feature_names_out</span>()
df_dtm = pd.<span class="fn">DataFrame</span>(
    dtm.<span class="fn">toarray</span>(),
    columns=feature_names,
    index=[<span class="st">f'Doc {i+1}'</span> <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus))]
)

<span class="fn">print</span>(<span class="st">f"DTM shape: {dtm.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"어휘 크기: {len(feature_names)}"</span>)
<span class="fn">print</span>(<span class="st">f"희소율: {1 - dtm.nnz / (dtm.shape[0] * dtm.shape[1]):.1%}"</span>)
<span class="fn">print</span>(<span class="st">f"\n상위 10개 피처:\n{df_dtm.sum().sort_values(ascending=False).head(10)}"</span>)
</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
DTM shape: (5, 28)
어휘 크기: 28
희소율: 80.0%

상위 10개 피처:
earnings        2
apple           2
stock           2
report          1
record          1</div>


<h3>4.4 BoW의 한계: 왜 이것만으로는 부족한가</h3>

<p>
BoW는 단순하고 직관적이지만, 심각한 한계가 있다. 첫째, <strong>단어 순서를 완전히 무시</strong>한다. "Fed raises rates"와 "rates raises Fed"가 동일한 벡터를 갖는다. 둘째, <strong>고빈도 단어가 지배</strong>한다. "the", "is", "of" 같은 단어가 모든 문서에 많이 등장하여 벡터를 지배하지만, 이런 단어는 문서를 구별하는 데 도움이 되지 않는다. 셋째, <strong>희소성(Sparsity)</strong> 문제가 있다. 어휘가 50,000개이고 문서에 평균 200개 단어가 있다면, 벡터의 99.6%가 0이다.
</p>

<!-- BoW 한계 시각화 -->
<div style="margin:25px 0;display:flex;gap:16px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:200px;max-width:300px;background:#fff;padding:16px;border-radius:10px;border:2px solid #e74c3c;text-align:center">
<div style="font-size:28px;margin-bottom:6px">🔀</div>
<div style="font-weight:bold;color:#e74c3c;font-size:13px;margin-bottom:6px">순서 무시</div>
<div style="font-size:11px;color:#666">"dog bites man" =<br>"man bites dog"<br>같은 벡터!</div>
</div>
<div style="flex:1;min-width:200px;max-width:300px;background:#fff;padding:16px;border-radius:10px;border:2px solid #f39c12;text-align:center">
<div style="font-size:28px;margin-bottom:6px">📊</div>
<div style="font-weight:bold;color:#f39c12;font-size:13px;margin-bottom:6px">고빈도 지배</div>
<div style="font-size:11px;color:#666">"the"가 100번 등장하면<br>핵심 단어 "earnings"(1번)을<br>압도한다</div>
</div>
<div style="flex:1;min-width:200px;max-width:300px;background:#fff;padding:16px;border-radius:10px;border:2px solid #9b59b6;text-align:center">
<div style="font-size:28px;margin-bottom:6px">🕳️</div>
<div style="font-weight:bold;color:#9b59b6;font-size:13px;margin-bottom:6px">극단적 희소성</div>
<div style="font-size:11px;color:#666">50,000차원 벡터에서<br>99.6%가 0<br>메모리 낭비 + 과적합</div>
</div>
</div>

<p>
이 한계들을 해결하기 위해 TF-IDF가 등장한다. 다음 챕터에서 자세히 다룬다.
</p>


<h3>4.5 BoW의 수학적 표현</h3>

<p>
BoW를 수학적으로 정리하면 다음과 같다. 어휘(vocabulary) \(V = \{w_1, w_2, \ldots, w_{|V|}\}\)가 주어졌을 때, 
문서 \(d\)의 BoW 표현은:
</p>

$$\mathbf{x}_d = [\text{count}(w_1, d), \; \text{count}(w_2, d), \; \ldots, \; \text{count}(w_{|V|}, d)]$$

<p>
여기서 \(\text{count}(w_i, d)\)는 단어 \(w_i\)가 문서 \(d\)에 등장하는 횟수다. 
전체 코퍼스 \(D = \{d_1, d_2, \ldots, d_N\}\)에 대해 이를 행렬로 쌓으면 
<strong>문서-용어 행렬(DTM)</strong> \(\mathbf{X} \in \mathbb{R}^{N \times |V|}\)가 된다.
</p>

<p>
이진(binary) 변형도 있다. 단어의 출현 여부만 기록하는 방식이다:
</p>

$$x_{d,i} = \begin{cases} 1 & \text{if } w_i \in d \\ 0 & \text{otherwise} \end{cases}$$

<p>
MLAT Ch.14에서 Jansen은 이진 BoW가 긴 문서에서 더 안정적일 수 있다고 지적한다. 
10-K 공시처럼 수만 단어의 문서에서는 단어 빈도의 절대값보다 출현 여부가 더 유의미한 피처가 될 수 있다.
</p>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.14 "The Document-Term Matrix"에서 Jansen은 DTM의 구축 과정을 
상세히 다루며, sklearn의 CountVectorizer 파라미터(min_df, max_df, max_features, ngram_range)의 
실전적 튜닝 가이드를 제공한다. 특히 금융 텍스트에서는 max_df=0.95, min_df=5 정도가 좋은 출발점이라고 권장한다.</p>
</div>

<h3>4.6 실전: BoW로 금융 뉴스 감성 분류 (기초)</h3>

<div class="cc">코드 4-2. BoW + 로지스틱 회귀로 감성 분류</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer
<span class="kw">from</span> sklearn.linear_model <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 금융 뉴스 감성 데이터 (시뮬레이션)</span>
texts = [
    <span class="st">"Apple reports record quarterly revenue beating expectations"</span>,
    <span class="st">"Strong earnings growth drives stock to new highs"</span>,
    <span class="st">"Company announces massive share buyback program"</span>,
    <span class="st">"Analysts upgrade rating citing robust demand"</span>,
    <span class="st">"Revenue surges on strong consumer spending"</span>,
    <span class="st">"Stock plunges after disappointing earnings miss"</span>,
    <span class="st">"Company warns of supply chain disruptions"</span>,
    <span class="st">"Analysts downgrade amid slowing growth concerns"</span>,
    <span class="st">"Shares tumble on weak guidance and rising costs"</span>,
    <span class="st">"Regulatory lawsuit threatens core business model"</span>,
]
labels = [<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>]  <span class="cm"># 1=긍정, 0=부정</span>

<span class="cm"># BoW 벡터화</span>
vec = <span class="fn">CountVectorizer</span>(stop_words=<span class="st">'english'</span>)
X = vec.<span class="fn">fit_transform</span>(texts)

<span class="fn">print</span>(<span class="st">f"DTM shape: {X.shape}"</span>)
<span class="fn">print</span>(<span class="st">f"어휘: {vec.get_feature_names_out()}"</span>)

<span class="cm"># 로지스틱 회귀 학습</span>
clf = <span class="fn">LogisticRegression</span>()
clf.<span class="fn">fit</span>(X, labels)

<span class="cm"># 새 뉴스 예측</span>
new_texts = [
    <span class="st">"Company beats earnings expectations with strong revenue"</span>,
    <span class="st">"Stock drops after disappointing quarterly results"</span>,
]
X_new = vec.<span class="fn">transform</span>(new_texts)
predictions = clf.<span class="fn">predict</span>(X_new)
probas = clf.<span class="fn">predict_proba</span>(X_new)

<span class="kw">for</span> text, pred, prob <span class="kw">in</span> <span class="nb">zip</span>(new_texts, predictions, probas):
    sentiment = <span class="st">"긍정 📈"</span> <span class="kw">if</span> pred == <span class="nu">1</span> <span class="kw">else</span> <span class="st">"부정 📉"</span>
    <span class="fn">print</span>(<span class="st">f"[{sentiment}] (확률: {prob[pred]:.2%}) {text}"</span>)

<span class="cm"># 피처 중요도 (R4 연결: 어떤 단어가 감성을 결정하는가?)</span>
coefs = pd.<span class="fn">Series</span>(clf.coef_[<span class="nu">0</span>], index=vec.<span class="fn">get_feature_names_out</span>())
<span class="fn">print</span>(<span class="st">"\n=== 긍정 단어 Top 5 ==="</span>)
<span class="fn">print</span>(coefs.<span class="fn">nlargest</span>(<span class="nu">5</span>))
<span class="fn">print</span>(<span class="st">"\n=== 부정 단어 Top 5 ==="</span>)
<span class="fn">print</span>(coefs.<span class="fn">nsmallest</span>(<span class="nu">5</span>))</code></pre>

<div class="ok">
<p class="ni"><strong>BoW → TF-IDF로의 자연스러운 전환</strong></p>
<p class="ni" style="margin-top:8px">
위 코드에서 CountVectorizer를 TfidfVectorizer로 바꾸기만 하면 TF-IDF 기반 분류가 된다. 
코드 한 줄의 차이지만, 성능은 크게 달라질 수 있다. 다음 챕터에서 그 이유를 수학적으로 파헤친다.
</p>
</div>

<!-- ==================== Ch.5 ==================== -->
<h2 id="ch5">Chapter 5. TF-IDF — 단어의 중요도에 가중치를 부여하다</h2>

<h3>5.1 TF-IDF의 직관</h3>

<p>
TF-IDF(Term Frequency–Inverse Document Frequency)는 BoW의 핵심 한계인 "고빈도 단어 지배" 문제를 해결한다. 아이디어는 간단하다: 특정 문서에서 자주 등장하지만(TF 높음), 전체 코퍼스에서는 드물게 등장하는 단어(IDF 높음)에 높은 가중치를 부여한다.
</p>

<p>
비유를 들어보자. 모든 금융 뉴스에 "market"이라는 단어가 등장한다면, 이 단어는 특정 뉴스를 구별하는 데 도움이 되지 않는다. 하지만 "quantitative easing"이라는 단어가 특정 뉴스에만 등장한다면, 이 단어는 그 뉴스의 핵심 주제를 나타낼 가능성이 높다. TF-IDF는 이런 직관을 수학적으로 구현한다.
</p>

<h3>5.2 수학적 정의</h3>

<p>
TF-IDF는 두 가지 요소의 곱이다:
</p>

<p class="ni"><strong>Term Frequency (TF):</strong> 단어 \(t\)가 문서 \(d\)에 등장하는 빈도</p>
<div class="eq">
$$\text{TF}(t, d) = \frac{\text{count}(t, d)}{\sum_{t' \in d} \text{count}(t', d)}$$
</div>

<p class="ni"><strong>Inverse Document Frequency (IDF):</strong> 단어 \(t\)가 전체 코퍼스에서 얼마나 드문지</p>
<div class="eq">
$$\text{IDF}(t) = \log\left(\frac{N}{\text{df}(t)}\right) + 1$$
</div>

<p>
여기서 \(N\)은 전체 문서 수, \(\text{df}(t)\)는 단어 \(t\)가 등장하는 문서 수다. 모든 문서에 등장하는 단어는 \(\text{IDF} = \log(1) + 1 = 1\)로 가중치가 낮고, 하나의 문서에만 등장하는 단어는 \(\text{IDF} = \log(N) + 1\)로 가중치가 높다.
</p>

<p class="ni"><strong>TF-IDF 최종 가중치:</strong></p>
<div class="eq">
$$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)$$
</div>

<div class="warn">
<p class="ni"><strong>⚠️ sklearn의 스무딩 (MLAT Ch.14)</strong></p>
<p class="ni" style="margin-top:8px">sklearn의 <code>TfidfVectorizer</code>는 기본적으로 스무딩을 적용한다. <code>smooth_idf=True</code>이면 분모에 1을 더해 0으로 나누는 것을 방지한다: \(\text{IDF}(t) = \log\left(\frac{1 + N}{1 + \text{df}(t)}\right) + 1\). 또한 <code>sublinear_tf=True</code>로 설정하면 TF에 로그를 취한다: \(\text{TF} = 1 + \log(\text{count})\). 이는 한 단어가 100번 등장하는 것이 1번 등장하는 것보다 100배 중요하지는 않다는 직관을 반영한다.</p>
</div>

<h3>5.3 TF-IDF 실전 구현</h3>

<div class="cc">코드 5-1. TfidfVectorizer로 금융 뉴스 벡터화</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 금융 뉴스 코퍼스</span>
corpus = [
    <span class="st">"Apple reported record quarterly earnings beating analyst expectations"</span>,
    <span class="st">"Tesla shares dropped after disappointing quarterly earnings report"</span>,
    <span class="st">"Federal Reserve raised interest rates citing persistent inflation"</span>,
    <span class="st">"Goldman Sachs upgraded Apple stock to buy rating after strong earnings"</span>,
    <span class="st">"Oil prices surged amid supply disruptions in Middle East"</span>,
    <span class="st">"Apple stock rose sharply following better than expected earnings"</span>,
    <span class="st">"Federal Reserve signaled potential rate cuts in upcoming meetings"</span>,
    <span class="st">"Inflation data came in higher than expected raising rate hike fears"</span>
]

<span class="cm"># TF-IDF 벡터화</span>
tfidf = <span class="fn">TfidfVectorizer</span>(
    stop_words=<span class="st">'english'</span>,
    ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>),
    max_features=<span class="nu">50</span>,
    sublinear_tf=<span class="kw">True</span>    <span class="cm"># TF에 로그 적용</span>
)
X = tfidf.<span class="fn">fit_transform</span>(corpus)

<span class="cm"># 각 문서에서 가장 중요한 단어 (TF-IDF 가중치 기준)</span>
feature_names = tfidf.<span class="fn">get_feature_names_out</span>()
<span class="kw">for</span> i, doc <span class="kw">in</span> <span class="fn">enumerate</span>(corpus):
    row = X[i].<span class="fn">toarray</span>().<span class="fn">flatten</span>()
    top_idx = row.<span class="fn">argsort</span>()[-<span class="nu">3</span>:][::-<span class="nu">1</span>]
    top_terms = [(feature_names[j], <span class="fn">round</span>(row[j], <span class="nu">3</span>)) <span class="kw">for</span> j <span class="kw">in</span> top_idx]
    <span class="fn">print</span>(<span class="st">f"Doc {i+1}: {top_terms}"</span>)
</code></pre>

<h3>5.4 문서 유사도: 코사인 유사도</h3>

<p>
TF-IDF 벡터를 얻으면, 문서 간 유사도를 계산할 수 있다. 가장 널리 쓰이는 방법이 <strong>코사인 유사도(Cosine Similarity)</strong>다. 두 벡터 사이의 각도의 코사인 값을 계산한다.
</p>

<div class="eq">
$$\text{cosine}(\mathbf{d}_1, \mathbf{d}_2) = \frac{\mathbf{d}_1 \cdot \mathbf{d}_2}{\|\mathbf{d}_1\| \|\mathbf{d}_2\|}$$
</div>

<p>
값이 1에 가까우면 두 문서가 유사하고, 0에 가까우면 관련이 없다. 코사인 유사도는 벡터의 크기(문서 길이)에 영향을 받지 않으므로, 길이가 다른 문서를 비교할 때 유클리드 거리보다 적합하다.
</p>

<div class="cc">코드 5-2. 코사인 유사도로 유사 뉴스 찾기</div>
<pre><code><span class="kw">from</span> sklearn.metrics.pairwise <span class="kw">import</span> cosine_similarity

<span class="cm"># 문서 간 코사인 유사도 행렬</span>
sim_matrix = <span class="fn">cosine_similarity</span>(X)

<span class="cm"># 히트맵 시각화</span>
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

fig, ax = plt.<span class="fn">subplots</span>(figsize=(<span class="nu">8</span>, <span class="nu">6</span>))
im = ax.<span class="fn">imshow</span>(sim_matrix, cmap=<span class="st">'YlOrRd'</span>, vmin=<span class="nu">0</span>, vmax=<span class="nu">1</span>)
ax.<span class="fn">set_xticks</span>(<span class="fn">range</span>(<span class="fn">len</span>(corpus)))
ax.<span class="fn">set_yticks</span>(<span class="fn">range</span>(<span class="fn">len</span>(corpus)))
ax.<span class="fn">set_xticklabels</span>([<span class="st">f'D{i+1}'</span> <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus))], rotation=<span class="nu">45</span>)
ax.<span class="fn">set_yticklabels</span>([<span class="st">f'D{i+1}'</span> <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus))])

<span class="cm"># 각 셀에 값 표시</span>
<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus)):
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(corpus)):
        ax.<span class="fn">text</span>(j, i, <span class="st">f'{sim_matrix[i,j]:.2f}'</span>, ha=<span class="st">'center'</span>, va=<span class="st">'center'</span>, fontsize=<span class="nu">9</span>)

plt.<span class="fn">colorbar</span>(im)
plt.<span class="fn">title</span>(<span class="st">'Document Similarity (Cosine)'</span>)
plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</code></pre>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.14에서 Jansen은 <code>scipy.spatial.distance.pdist()</code>를 사용하여 BBC 뉴스 기사 간 유사도를 계산하고, 가장 유사한 문서 쌍을 찾는다. 금융에서 이 기법은 "유사한 뉴스 클러스터링", "중복 뉴스 제거", "관련 종목 발견" 등에 활용된다.</p>
</div>


<!-- ==================== Ch.6 ==================== -->
<h2 id="ch6">Chapter 6. 나이브 베이즈 — 텍스트 분류의 고전적 무기</h2>

<h3>6.1 베이즈 정리 복습 (R2 연결)</h3>

<p>
R2에서 우리는 베이즈 정리를 배웠다. 사전 확률(prior)과 우도(likelihood)를 결합하여 사후 확률(posterior)을 계산하는 공식이었다. 나이브 베이즈 분류기는 이 베이즈 정리를 텍스트 분류에 직접 적용한다.
</p>

<div class="eq">
$$P(\text{class} | \text{document}) = \frac{P(\text{document} | \text{class}) \times P(\text{class})}{P(\text{document})}$$
</div>

<p>
금융 감성분석에 적용하면: "이 뉴스 기사가 주어졌을 때, 이것이 긍정적(positive)일 확률은 얼마인가?"를 계산하는 것이다.
</p>

<h3>6.2 "나이브"한 가정: 조건부 독립</h3>

<p>
나이브 베이즈가 "나이브(순진한)"라고 불리는 이유는 핵심 가정 때문이다: <strong>모든 단어가 서로 독립적</strong>이라고 가정한다. 즉, "earnings"가 등장할 확률은 "strong"이 등장했는지 여부와 무관하다고 가정한다. 현실에서 이 가정은 거의 항상 틀리다 — "strong earnings"는 함께 등장할 확률이 높다. 하지만 놀랍게도, 이 순진한 가정에도 불구하고 나이브 베이즈는 텍스트 분류에서 놀라울 정도로 잘 작동한다.
</p>

<p>
MLAT Ch.14에서 Jansen은 스팸 필터링 예시로 이를 설명한다. "send money now"라는 메시지가 스팸일 확률을 계산할 때:
</p>

<div class="eq">
$$P(\text{spam} | \text{send money now}) = \frac{P(\text{send}|\text{spam}) \times P(\text{money}|\text{spam}) \times P(\text{now}|\text{spam}) \times P(\text{spam})}{P(\text{send money now})}$$
</div>

<p>
각 단어의 조건부 확률을 독립적으로 곱한다 — 이것이 "나이브" 가정이다. 분모 \(P(\text{send money now})\)는 모든 클래스에 대해 동일하므로, 분류 시에는 분자만 비교하면 된다.
</p>

<h3>6.3 다항 나이브 베이즈 (Multinomial NB)</h3>

<p>
텍스트 분류에서 가장 많이 쓰이는 변형은 <strong>다항 나이브 베이즈(Multinomial Naive Bayes)</strong>다. 각 단어의 출현 빈도를 다항 분포로 모델링한다. BoW나 TF-IDF로 만든 DTM을 직접 입력으로 받을 수 있다.
</p>

<div class="cc">코드 6-1. 나이브 베이즈로 금융 뉴스 감성 분류</div>
<pre><code><span class="kw">from</span> sklearn.naive_bayes <span class="kw">import</span> MultinomialNB
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report, confusion_matrix
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 금융 뉴스 감성 데이터 (실제로는 수천~수만 개 필요)</span>
texts = [
    <span class="st">"Company reported record profits exceeding expectations"</span>,
    <span class="st">"Strong revenue growth driven by new product launches"</span>,
    <span class="st">"Earnings beat estimates with impressive margin expansion"</span>,
    <span class="st">"Stock upgraded to buy after stellar quarterly results"</span>,
    <span class="st">"Dividend increased reflecting confidence in future growth"</span>,
    <span class="st">"Company missed earnings estimates significantly"</span>,
    <span class="st">"Revenue declined sharply due to weak demand"</span>,
    <span class="st">"Stock downgraded to sell amid deteriorating fundamentals"</span>,
    <span class="st">"Profit warning issued citing supply chain disruptions"</span>,
    <span class="st">"Massive layoffs announced as company restructures operations"</span>,
    <span class="st">"Market remained flat with mixed economic signals"</span>,
    <span class="st">"Trading volume was average with no significant catalysts"</span>,
]
labels = [<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>, <span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>, <span class="nu">2</span>,<span class="nu">2</span>]  <span class="cm"># 1=긍정, 0=부정, 2=중립</span>
label_names = [<span class="st">'Negative'</span>, <span class="st">'Positive'</span>, <span class="st">'Neutral'</span>]

<span class="cm"># TF-IDF 벡터화</span>
tfidf = <span class="fn">TfidfVectorizer</span>(stop_words=<span class="st">'english'</span>, ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>))
X = tfidf.<span class="fn">fit_transform</span>(texts)

<span class="cm"># 나이브 베이즈 학습</span>
nb = <span class="fn">MultinomialNB</span>(alpha=<span class="nu">1.0</span>)  <span class="cm"># alpha: 라플라스 스무딩</span>
nb.<span class="fn">fit</span>(X, labels)

<span class="cm"># 새로운 뉴스에 대한 예측</span>
new_texts = [
    <span class="st">"Company delivered outstanding results with record revenue"</span>,
    <span class="st">"Shares plunged after accounting fraud allegations"</span>,
    <span class="st">"Trading was subdued ahead of Federal Reserve decision"</span>
]
X_new = tfidf.<span class="fn">transform</span>(new_texts)
predictions = nb.<span class="fn">predict</span>(X_new)
probabilities = nb.<span class="fn">predict_proba</span>(X_new)

<span class="kw">for</span> text, pred, prob <span class="kw">in</span> <span class="fn">zip</span>(new_texts, predictions, probabilities):
    <span class="fn">print</span>(<span class="st">f"\n📰 {text}"</span>)
    <span class="fn">print</span>(<span class="st">f"   예측: {label_names[pred]}"</span>)
    <span class="fn">print</span>(<span class="st">f"   확률: Neg={prob[0]:.3f}, Pos={prob[1]:.3f}, Neu={prob[2]:.3f}"</span>)
</code></pre>

<h3>6.4 라플라스 스무딩 (Additive Smoothing)</h3>

<p>
나이브 베이즈에서 한 가지 문제가 있다. 훈련 데이터에 한 번도 등장하지 않은 단어가 테스트 데이터에 나타나면, 그 단어의 조건부 확률이 0이 되어 전체 확률이 0이 된다. 이를 방지하기 위해 <strong>라플라스 스무딩(Laplace Smoothing)</strong>을 적용한다. 모든 단어의 빈도에 \(\alpha\)(보통 1)를 더해준다.
</p>

<div class="eq">
$$P(w_i | c) = \frac{\text{count}(w_i, c) + \alpha}{\sum_{w \in V} [\text{count}(w, c) + \alpha]} = \frac{\text{count}(w_i, c) + \alpha}{\text{count}(c) + \alpha |V|}$$
</div>

<p>
여기서 \(|V|\)는 어휘 크기다. \(\alpha = 1\)이면 라플라스 스무딩, \(\alpha < 1\)이면 리드스톤 스무딩이라 한다.
</p>

<div class="ok">
<p class="ni"><strong>나이브 베이즈의 장단점 정리</strong></p>
<ul>
<li><strong>장점:</strong> 학습이 매우 빠르다 (단순 빈도 계산), 적은 데이터에서도 잘 작동한다, 확률 출력을 제공한다, 고차원 희소 데이터에 강하다</li>
<li><strong>단점:</strong> 조건부 독립 가정이 비현실적이다, 단어 순서를 무시한다, 확률 추정이 부정확할 수 있다 (calibration 문제)</li>
<li><strong>금융 적용:</strong> 뉴스 감성 분류의 빠른 베이스라인으로 적합하다. MLAT Ch.14에서 BBC 뉴스 분류에 64.7% 정확도를 달성한다.</li>
</ul>
</div>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.14에서 Jansen은 나이브 베이즈를 두 가지 데이터셋에 적용한다. (1) BBC 뉴스 2,225개 기사를 5개 카테고리로 분류 — 97.7% 정확도. (2) Yelp 리뷰 600만 개를 1~5점으로 분류 — 64.7% 정확도. 이후 LightGBM과 비교하여 73.6% 정확도를 달성하는데, 이는 앙상블 모델의 우위를 보여준다.</p>
</div>


<h3>6.5 나이브 베이즈의 변형들</h3>

<p>
sklearn은 세 가지 나이브 베이즈 변형을 제공한다. 각각 다른 데이터 분포를 가정한다.
</p>

<div class="tc">표 6-1. 나이브 베이즈 변형 비교</div>
<table>
<thead>
<tr><th>변형</th><th>분포 가정</th><th>입력 데이터</th><th>금융 NLP 적용</th></tr>
</thead>
<tbody>
<tr><td><strong>MultinomialNB</strong></td><td>다항 분포</td><td>단어 빈도 (정수, ≥0)</td><td>BoW, TF-IDF 기반 텍스트 분류 → <strong>가장 일반적</strong></td></tr>
<tr><td><strong>BernoulliNB</strong></td><td>베르누이 분포</td><td>이진 (0/1)</td><td>단어 출현 여부만 사용. 짧은 텍스트(트윗, 헤드라인)에 적합</td></tr>
<tr><td><strong>GaussianNB</strong></td><td>가우시안 분포</td><td>연속값 (실수)</td><td>Word2Vec/Doc2Vec 밀집 벡터 입력 시 사용</td></tr>
</tbody>
</table>

<div class="cc">코드 6-2. 3가지 나이브 베이즈 변형 비교 실험</div>
<pre><code><span class="kw">from</span> sklearn.naive_bayes <span class="kw">import</span> MultinomialNB, BernoulliNB, GaussianNB
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer, TfidfVectorizer
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> cross_val_score
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 금융 뉴스 코퍼스 (긍정/부정 라벨)</span>
corpus = [
    <span class="st">"record revenue strong earnings beat expectations growth"</span>,
    <span class="st">"upgrade buy rating robust demand momentum"</span>,
    <span class="st">"buyback dividend increase shareholder value"</span>,
    <span class="st">"innovation breakthrough market leader expansion"</span>,
    <span class="st">"decline loss miss disappointing weak guidance"</span>,
    <span class="st">"downgrade sell warning risk regulatory pressure"</span>,
    <span class="st">"lawsuit antitrust investigation penalty fine"</span>,
    <span class="st">"slowdown recession layoffs restructuring costs"</span>,
]
labels = np.<span class="fn">array</span>([<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">1</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>])

<span class="cm"># 벡터화</span>
count_vec = <span class="fn">CountVectorizer</span>()
tfidf_vec = <span class="fn">TfidfVectorizer</span>()
X_count = count_vec.<span class="fn">fit_transform</span>(corpus)
X_tfidf = tfidf_vec.<span class="fn">fit_transform</span>(corpus)

<span class="cm"># 3가지 NB 비교</span>
experiments = [
    (<span class="st">"MultinomialNB + BoW"</span>, <span class="fn">MultinomialNB</span>(), X_count),
    (<span class="st">"MultinomialNB + TF-IDF"</span>, <span class="fn">MultinomialNB</span>(), X_tfidf),
    (<span class="st">"BernoulliNB + BoW"</span>, <span class="fn">BernoulliNB</span>(), X_count),
    (<span class="st">"GaussianNB + TF-IDF"</span>, <span class="fn">GaussianNB</span>(), X_tfidf.<span class="fn">toarray</span>()),
]

<span class="kw">for</span> name, model, X <span class="kw">in</span> experiments:
    scores = <span class="fn">cross_val_score</span>(model, X, labels, cv=<span class="nu">3</span>, scoring=<span class="st">'accuracy'</span>)
    <span class="fn">print</span>(<span class="st">f"{name:30s}: {scores.mean():.3f} ± {scores.std():.3f}"</span>)</code></pre>

<h3>6.6 나이브 베이즈 vs 다른 분류기: 언제 NB를 쓸 것인가</h3>

<p>
나이브 베이즈는 "나이브"한 가정에도 불구하고 텍스트 분류에서 놀라울 정도로 잘 작동한다. 
하지만 모든 상황에서 최선은 아니다. R4에서 배운 다른 분류기들과 비교해보자.
</p>

<!-- NB vs 다른 분류기 비교 -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:15px;font-size:14px;color:#2c3e50">텍스트 분류기 선택 가이드</p>
<div style="display:flex;gap:12px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:200px;background:#fff;padding:14px;border-radius:8px;border-left:4px solid #4caf50">
<p class="ni" style="font-weight:bold;color:#2e7d32;font-size:12px">✅ NB가 좋은 경우</p>
<ul style="font-size:11px;color:#555;margin:6px 0 0;padding-left:16px;line-height:1.6">
<li>학습 데이터가 적을 때 (수백 개)</li>
<li>피처 수가 매우 많을 때 (고차원)</li>
<li>빠른 프로토타이핑이 필요할 때</li>
<li>실시간 분류가 필요할 때</li>
</ul>
</div>
<div style="flex:1;min-width:200px;background:#fff;padding:14px;border-radius:8px;border-left:4px solid #ff9800">
<p class="ni" style="font-weight:bold;color:#e65100;font-size:12px">⚡ LightGBM이 좋은 경우</p>
<ul style="font-size:11px;color:#555;margin:6px 0 0;padding-left:16px;line-height:1.6">
<li>학습 데이터가 충분할 때 (수만 개+)</li>
<li>피처 간 상호작용이 중요할 때</li>
<li>최고 성능이 필요할 때</li>
<li>피처 중요도 분석이 필요할 때</li>
</ul>
</div>
<div style="flex:1;min-width:200px;background:#fff;padding:14px;border-radius:8px;border-left:4px solid #9c27b0">
<p class="ni" style="font-weight:bold;color:#7b1fa2;font-size:12px">🤖 FinBERT가 좋은 경우</p>
<ul style="font-size:11px;color:#555;margin:6px 0 0;padding-left:16px;line-height:1.6">
<li>문맥 이해가 중요할 때</li>
<li>부정, 비교, 조건문이 많을 때</li>
<li>GPU 자원이 있을 때</li>
<li>최고 정확도가 필요할 때</li>
</ul>
</div>
</div>
</div>

<!-- ==================== Ch.7 ==================== -->
<h2 id="ch7">Chapter 7. 감성분석 실전 — 금융 뉴스에서 시그널 추출</h2>

<h3>7.1 감성분석의 두 가지 접근법</h3>

<p>
감성분석(Sentiment Analysis)은 텍스트에서 감성(긍정/부정/중립)을 자동으로 판별하는 기법이다. 크게 두 가지 접근법이 있다:
</p>

<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:linear-gradient(135deg,#e3f2fd,#bbdefb);padding:20px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#1565c0;margin-bottom:10px">📖 사전 기반 (Lexicon-Based)</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">📋 미리 정의된 감성 사전 사용</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">⚡ 학습 데이터 불필요, 즉시 적용 가능</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">📊 도구: VADER, TextBlob, Loughran-McDonald</p>
<p class="ni" style="font-size:11px;color:#555;margin-top:10px;border-top:1px solid #90caf9;padding-top:8px">한계: 문맥 무시, 도메인 특수성 부족</p>
</div>
<div style="flex:1;min-width:280px;background:linear-gradient(135deg,#f3e5f5,#e1bee7);padding:20px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#6a1b9a;margin-bottom:10px">🤖 ML 기반 (Machine Learning)</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">📋 라벨링된 데이터로 모델 학습</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">⚡ 높은 정확도, 도메인 적응 가능</p>
<p class="ni" style="font-size:12px;margin-bottom:8px">📊 도구: NB, LightGBM, BERT, FinBERT</p>
<p class="ni" style="font-size:11px;color:#555;margin-top:10px;border-top:1px solid #ce93d8;padding-top:8px">한계: 대량 라벨 데이터 필요, 학습 시간</p>
</div>
</div>

<h3>7.2 금융 감성 사전: Loughran-McDonald</h3>

<p>
일반적인 감성 사전(VADER, TextBlob)은 금융 텍스트에 적합하지 않다. "liability"는 일반 영어에서 부정적이지만, 금융에서는 중립적인 회계 용어다. "outstanding"은 일반적으로 긍정적이지만, "outstanding shares"에서는 중립적이다. 이런 문제를 해결하기 위해 Loughran과 McDonald(2011)가 금융 전용 감성 사전을 개발했다.
</p>

<div class="tc">표 7-1. 일반 감성 사전 vs 금융 감성 사전</div>
<table>
<thead>
<tr><th>단어</th><th>일반 감성 (VADER)</th><th>금융 감성 (L-M)</th><th>이유</th></tr>
</thead>
<tbody>
<tr><td>liability</td><td style="color:#e74c3c">부정 ❌</td><td>중립 ✅</td><td>회계 용어</td></tr>
<tr><td>outstanding</td><td style="color:#27ae60">긍정 ❌</td><td>중립 ✅</td><td>"outstanding shares"</td></tr>
<tr><td>capital</td><td>중립</td><td>중립 ✅</td><td>금융 기본 용어</td></tr>
<tr><td>risk</td><td style="color:#e74c3c">부정 ❌</td><td>불확실 ✅</td><td>리스크 ≠ 부정</td></tr>
<tr><td>tax</td><td style="color:#e74c3c">부정 ❌</td><td>중립 ✅</td><td>회계 용어</td></tr>
<tr><td>decline</td><td style="color:#e74c3c">부정</td><td style="color:#e74c3c">부정 ✅</td><td>실적 하락</td></tr>
</tbody>
</table>

<div class="cc">코드 7-1. Loughran-McDonald 감성 사전 활용</div>
<pre><code><span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> re

<span class="cm"># Loughran-McDonald 감성 사전 로드</span>
<span class="cm"># 다운로드: https://sraf.nd.edu/loughranmcdonald-master-dictionary/</span>
lm_dict = pd.<span class="fn">read_csv</span>(<span class="st">'LoughranMcDonald_MasterDictionary_2020.csv'</span>)

<span class="cm"># 감성 카테고리별 단어 목록 추출</span>
positive_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Positive'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())
negative_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Negative'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())
uncertain_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Uncertainty'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())
litigious_words = <span class="fn">set</span>(lm_dict[lm_dict[<span class="st">'Litigious'</span>] > <span class="nu">0</span>][<span class="st">'Word'</span>].str.<span class="fn">lower</span>())

<span class="fn">print</span>(<span class="st">f"긍정 단어: {len(positive_words)}개 (예: {list(positive_words)[:5]})"</span>)
<span class="fn">print</span>(<span class="st">f"부정 단어: {len(negative_words)}개 (예: {list(negative_words)[:5]})"</span>)
<span class="fn">print</span>(<span class="st">f"불확실 단어: {len(uncertain_words)}개"</span>)
<span class="fn">print</span>(<span class="st">f"소송 관련: {len(litigious_words)}개"</span>)

<span class="cm"># 감성 점수 계산 함수</span>
<span class="kw">def</span> <span class="fn">lm_sentiment_score</span>(text):
    <span class="st">"""Loughran-McDonald 감성 점수 계산"""</span>
    words = re.<span class="fn">findall</span>(<span class="st">r'\b[a-z]+\b'</span>, text.<span class="fn">lower</span>())
    n_words = <span class="fn">len</span>(words)
    <span class="kw">if</span> n_words == <span class="nu">0</span>:
        <span class="kw">return</span> <span class="nu">0</span>
    
    pos_count = <span class="fn">sum</span>(<span class="nu">1</span> <span class="kw">for</span> w <span class="kw">in</span> words <span class="kw">if</span> w <span class="kw">in</span> positive_words)
    neg_count = <span class="fn">sum</span>(<span class="nu">1</span> <span class="kw">for</span> w <span class="kw">in</span> words <span class="kw">if</span> w <span class="kw">in</span> negative_words)
    
    <span class="cm"># 감성 점수: (긍정 - 부정) / 전체 단어 수</span>
    <span class="kw">return</span> (pos_count - neg_count) / n_words

<span class="cm"># 테스트</span>
test_texts = [
    <span class="st">"Company achieved strong growth with improved profitability"</span>,
    <span class="st">"Significant losses due to impairment charges and restructuring"</span>,
    <span class="st">"Revenue remained stable with moderate capital expenditures"</span>
]

<span class="kw">for</span> text <span class="kw">in</span> test_texts:
    score = <span class="fn">lm_sentiment_score</span>(text)
    sentiment = <span class="st">"긍정"</span> <span class="kw">if</span> score > <span class="nu">0</span> <span class="kw">else</span> (<span class="st">"부정"</span> <span class="kw">if</span> score < <span class="nu">0</span> <span class="kw">else</span> <span class="st">"중립"</span>)
    <span class="fn">print</span>(<span class="st">f"[{sentiment}] score={score:+.3f} | {text}"</span>)
</code></pre>

<h3>7.3 LightGBM으로 감성 분류 성능 높이기</h3>

<p>
나이브 베이즈는 빠르고 간단하지만, 더 높은 정확도가 필요하다면 R4에서 배운 앙상블 모델을 활용할 수 있다. MLAT Ch.14에서 Jansen은 Yelp 리뷰 데이터에 LightGBM을 적용하여 나이브 베이즈(64.7%)보다 크게 향상된 73.6% 정확도를 달성한다.
</p>

<div class="cc">코드 7-2. TF-IDF + LightGBM 감성 분류 파이프라인</div>
<pre><code><span class="kw">import</span> lightgbm <span class="kw">as</span> lgb
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> accuracy_score, classification_report

<span class="cm"># 1. 데이터 로드 (예: Yelp 리뷰 또는 금융 뉴스)</span>
<span class="cm"># df = pd.read_parquet('financial_news_sentiment.parquet')</span>
<span class="cm"># X_text, y = df['text'], df['sentiment']</span>

<span class="cm"># 2. TF-IDF 벡터화</span>
tfidf = <span class="fn">TfidfVectorizer</span>(
    max_features=<span class="nu">50000</span>,
    ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>),
    sublinear_tf=<span class="kw">True</span>,
    stop_words=<span class="st">'english'</span>
)
X = tfidf.<span class="fn">fit_transform</span>(X_text)

<span class="cm"># 3. Train/Test 분할</span>
X_train, X_test, y_train, y_test = <span class="fn">train_test_split</span>(
    X, y, test_size=<span class="nu">0.2</span>, random_state=<span class="nu">42</span>, stratify=y
)

<span class="cm"># 4. LightGBM 학습</span>
train_data = lgb.<span class="fn">Dataset</span>(X_train, label=y_train)
test_data = lgb.<span class="fn">Dataset</span>(X_test, label=y_test, reference=train_data)

params = {
    <span class="st">'objective'</span>: <span class="st">'multiclass'</span>,
    <span class="st">'num_classes'</span>: <span class="nu">3</span>,
    <span class="st">'metric'</span>: <span class="st">'multi_logloss'</span>,
    <span class="st">'learning_rate'</span>: <span class="nu">0.1</span>,
    <span class="st">'num_leaves'</span>: <span class="nu">63</span>,
    <span class="st">'verbose'</span>: -<span class="nu">1</span>
}

model = lgb.<span class="fn">train</span>(
    params,
    train_data,
    num_boost_round=<span class="nu">1000</span>,
    valid_sets=[test_data],
    callbacks=[lgb.<span class="fn">early_stopping</span>(<span class="nu">25</span>), lgb.<span class="fn">log_evaluation</span>(<span class="nu">50</span>)]
)

<span class="cm"># 5. 예측 및 평가</span>
y_pred = model.<span class="fn">predict</span>(X_test).<span class="fn">argmax</span>(axis=<span class="nu">1</span>)
<span class="fn">print</span>(<span class="fn">classification_report</span>(y_test, y_pred, target_names=[<span class="st">'Negative'</span>,<span class="st">'Positive'</span>,<span class="st">'Neutral'</span>]))
</code></pre>

<div class="ok">
<p class="ni"><strong>핵심 포인트:</strong> TF-IDF + LightGBM 조합은 텍스트 감성분석의 강력한 베이스라인이다. MLAT에서 Yelp 600만 리뷰에 대해 73.6% 정확도를 달성했다. 이 파이프라인은 (1) 빠른 학습, (2) 피처 중요도 해석 가능, (3) 대규모 데이터 처리 가능이라는 장점이 있다. R4에서 배운 XGBoost/LightGBM의 텍스트 데이터 적용 버전이라고 생각하면 된다.</p>
</div>


<!-- ==================== Ch.8 ==================== -->
<h2 id="ch8">Chapter 8. 토픽 모델링 — 문서의 숨겨진 주제를 발견하다</h2>

<h3>8.1 토픽 모델링이란</h3>

<p>
R5에서 우리는 PCA로 숫자 데이터의 숨겨진 구조를 발견했다. 토픽 모델링은 텍스트 데이터에 대한 PCA라고 생각하면 된다. 수천 개의 뉴스 기사가 있을 때, 이 기사들이 다루는 "잠재 주제(latent topics)"를 자동으로 추출한다. 예를 들어, 금융 뉴스 코퍼스에서 "통화정책", "기업 실적", "에너지 시장", "기술주" 같은 토픽이 자동으로 발견될 수 있다.
</p>

<p>
MLAT Ch.15의 제목은 "Topic Modeling – Summarizing Financial News"다. 핵심 메시지는 명확하다: 토픽 모델링으로 대량의 금융 뉴스를 자동으로 요약하고 분류할 수 있다.
</p>

<h3>8.2 LSI에서 LDA까지: 토픽 모델의 진화</h3>

<!-- 토픽 모델 진화 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:18px;font-size:14px;color:#2c3e50">📈 토픽 모델의 진화</p>
<div style="display:flex;align-items:stretch;gap:12px;flex-wrap:wrap;justify-content:center">

<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #90caf9">
<div style="font-size:11px;color:#1976d2;font-weight:bold;margin-bottom:6px">1990s</div>
<div style="font-weight:bold;font-size:13px;margin-bottom:6px">LSI / LSA</div>
<div style="font-size:11px;color:#666;line-height:1.5">
<strong>방법:</strong> DTM에 SVD 적용<br>
<strong>장점:</strong> 동의어 처리 가능<br>
<strong>단점:</strong> 토픽 해석 어려움 (음수 값)<br>
<strong>수학:</strong> \(DTM \approx U \Sigma V^T\)
</div>
</div>

<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #ce93d8">
<div style="font-size:11px;color:#7b1fa2;font-weight:bold;margin-bottom:6px">1999</div>
<div style="font-weight:bold;font-size:13px;margin-bottom:6px">pLSA / NMF</div>
<div style="font-size:11px;color:#666;line-height:1.5">
<strong>방법:</strong> 확률적 모델 / 비음수 분해<br>
<strong>장점:</strong> 토픽이 확률 분포<br>
<strong>단점:</strong> 새 문서에 일반화 어려움<br>
<strong>수학:</strong> \(DTM \approx W H\) (W,H ≥ 0)
</div>
</div>

<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;border-top:4px solid #81c784">
<div style="font-size:11px;color:#2e7d32;font-weight:bold;margin-bottom:6px">2003 ⭐</div>
<div style="font-weight:bold;font-size:13px;margin-bottom:6px">LDA</div>
<div style="font-size:11px;color:#666;line-height:1.5">
<strong>방법:</strong> 베이지안 생성 모델<br>
<strong>장점:</strong> 확률적 해석, 일반화 가능<br>
<strong>단점:</strong> 토픽 수 사전 지정 필요<br>
<strong>수학:</strong> 디리클레 분포 기반
</div>
</div>

</div>
</div>

<h3>8.3 LDA (Latent Dirichlet Allocation) 핵심 원리</h3>

<p>
LDA는 현재 가장 널리 쓰이는 토픽 모델이다. 핵심 아이디어는 "역공학(reverse engineering)"이다. LDA는 문서가 생성되는 과정을 다음과 같이 가정한다:
</p>

<ol>
<li>각 문서는 여러 토픽의 <strong>혼합(mixture)</strong>이다. 예: "이 기사는 70% 통화정책 + 30% 주식시장"</li>
<li>각 토픽은 단어들의 <strong>확률 분포</strong>다. 예: "통화정책 토픽에서 'rate'가 나올 확률 5%, 'inflation'이 나올 확률 3%..."</li>
<li>문서를 쓸 때, 먼저 토픽을 선택하고, 그 토픽의 단어 분포에서 단어를 뽑는다.</li>
</ol>

<p>
물론 실제로 사람이 이렇게 글을 쓰지는 않는다. 하지만 이 "가상의 생성 과정"을 역으로 추적하면, 주어진 문서들에서 잠재 토픽을 추출할 수 있다.
</p>

<!-- LDA 생성 과정 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e8f5e9,#c8e6c9);border-radius:12px;border:1px solid #a5d6a7">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#2e7d32">🎲 LDA의 생성 과정 (Generative Process)</p>
<div style="display:flex;flex-direction:column;gap:10px;max-width:600px;margin:0 auto;font-size:12px">

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">1</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>토픽 분포 선택:</strong> 디리클레 분포에서 문서의 토픽 비율 θ를 뽑는다<br>
<span style="color:#888;font-size:11px">예: θ = [0.7, 0.2, 0.1] → "70% 통화정책, 20% 실적, 10% 에너지"</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">2</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>단어마다 토픽 할당:</strong> θ에 따라 각 단어의 토픽 z를 선택<br>
<span style="color:#888;font-size:11px">예: 첫 번째 단어 → 토픽 1(통화정책), 두 번째 단어 → 토픽 1, 세 번째 → 토픽 2(실적)...</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#2e7d32;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">3</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>단어 생성:</strong> 선택된 토픽의 단어 분포 β에서 실제 단어를 뽑는다<br>
<span style="color:#888;font-size:11px">예: 토픽 1에서 → "rate" (확률 5%), "inflation" (확률 3%), "Fed" (확률 4%)...</span>
</div>
</div>

</div>
<p class="ni" style="text-align:center;margin-top:12px;font-size:11px;color:#555">LDA는 이 과정을 <strong>역으로 추적</strong>하여, 관찰된 문서들에서 θ(문서-토픽 분포)와 β(토픽-단어 분포)를 추정한다</p>
</div>

<h3>8.4 디리클레 분포 (Dirichlet Distribution)</h3>

<p>
LDA의 "D"는 디리클레(Dirichlet)를 의미한다. 디리클레 분포는 확률 벡터를 생성하는 분포다. 즉, 합이 1이 되는 양수 벡터를 만들어낸다. 파라미터 \(\alpha\)가 이 분포의 형태를 결정한다.
</p>

<div class="eq">
$$\text{Dir}(\boldsymbol{\theta} | \boldsymbol{\alpha}) = \frac{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma(\alpha_k)} \prod_{k=1}^{K} \theta_k^{\alpha_k - 1}$$
</div>

<ul>
<li>\(\alpha > 1\): 토픽이 고르게 분포 (모든 토픽이 비슷한 비율)</li>
<li>\(\alpha < 1\): 토픽이 희소하게 분포 (한두 개 토픽이 지배적) — 실제 문서에 더 적합</li>
<li>\(\alpha = 1\): 균등 분포 (모든 조합이 동일 확률)</li>
</ul>

<div class="cc">코드 8-1. 디리클레 분포 시각화</div>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

fig, axes = plt.<span class="fn">subplots</span>(<span class="nu">1</span>, <span class="nu">3</span>, figsize=(<span class="nu">15</span>, <span class="nu">4</span>))

alphas = [<span class="nu">0.1</span>, <span class="nu">1.0</span>, <span class="nu">10.0</span>]
titles = [<span class="st">'α=0.1 (희소: 한 토픽 지배)'</span>, <span class="st">'α=1.0 (균등)'</span>, <span class="st">'α=10.0 (고르게 분포)'</span>]

<span class="kw">for</span> ax, alpha, title <span class="kw">in</span> <span class="fn">zip</span>(axes, alphas, titles):
    <span class="cm"># 3개 토픽에 대한 디리클레 샘플 1000개</span>
    samples = np.random.<span class="fn">dirichlet</span>([alpha] * <span class="nu">3</span>, size=<span class="nu">1000</span>)
    
    <span class="cm"># 삼각 좌표로 시각화 (simplex)</span>
    ax.<span class="fn">scatter</span>(samples[:, <span class="nu">0</span>], samples[:, <span class="nu">1</span>], alpha=<span class="nu">0.3</span>, s=<span class="nu">5</span>)
    ax.<span class="fn">set_xlim</span>(<span class="nu">0</span>, <span class="nu">1</span>)
    ax.<span class="fn">set_ylim</span>(<span class="nu">0</span>, <span class="nu">1</span>)
    ax.<span class="fn">set_xlabel</span>(<span class="st">'Topic 1'</span>)
    ax.<span class="fn">set_ylabel</span>(<span class="st">'Topic 2'</span>)
    ax.<span class="fn">set_title</span>(title, fontsize=<span class="nu">11</span>)
    ax.<span class="fn">set_aspect</span>(<span class="st">'equal'</span>)

plt.<span class="fn">suptitle</span>(<span class="st">'디리클레 분포: α가 토픽 분포에 미치는 영향'</span>, fontsize=<span class="nu">13</span>)
plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</code></pre>

<h3>8.5 sklearn과 Gensim으로 LDA 구현</h3>

<div class="cc">코드 8-2. sklearn LDA로 금융 뉴스 토픽 추출</div>
<pre><code><span class="kw">from</span> sklearn.decomposition <span class="kw">import</span> LatentDirichletAllocation
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> CountVectorizer

<span class="cm"># 1. DTM 생성 (LDA는 빈도 기반이므로 CountVectorizer 사용)</span>
vectorizer = <span class="fn">CountVectorizer</span>(
    max_df=<span class="nu">0.95</span>,           <span class="cm"># 95% 이상 문서에 등장하면 제거</span>
    min_df=<span class="nu">2</span>,              <span class="cm"># 최소 2개 문서에 등장</span>
    stop_words=<span class="st">'english'</span>,
    max_features=<span class="nu">5000</span>
)
dtm = vectorizer.<span class="fn">fit_transform</span>(corpus)  <span class="cm"># corpus: 뉴스 기사 리스트</span>

<span class="cm"># 2. LDA 모델 학습</span>
n_topics = <span class="nu">5</span>
lda = <span class="fn">LatentDirichletAllocation</span>(
    n_components=n_topics,
    max_iter=<span class="nu">20</span>,
    learning_method=<span class="st">'online'</span>,  <span class="cm"># 대규모 데이터에 적합</span>
    random_state=<span class="nu">42</span>,
    doc_topic_prior=<span class="nu">0.1</span>,      <span class="cm"># α: 문서-토픽 디리클레 파라미터</span>
    topic_word_prior=<span class="nu">0.01</span>     <span class="cm"># β: 토픽-단어 디리클레 파라미터</span>
)
lda.<span class="fn">fit</span>(dtm)

<span class="cm"># 3. 각 토픽의 상위 단어 출력</span>
feature_names = vectorizer.<span class="fn">get_feature_names_out</span>()

<span class="fn">print</span>(<span class="st">"="</span> * <span class="nu">60</span>)
<span class="kw">for</span> topic_idx, topic <span class="kw">in</span> <span class="fn">enumerate</span>(lda.components_):
    top_words = [feature_names[i] <span class="kw">for</span> i <span class="kw">in</span> topic.<span class="fn">argsort</span>()[-<span class="nu">10</span>:]]
    <span class="fn">print</span>(<span class="st">f"Topic {topic_idx + 1}: {', '.join(top_words)}"</span>)
<span class="fn">print</span>(<span class="st">"="</span> * <span class="nu">60</span>)

<span class="cm"># 4. 문서의 토픽 분포 확인</span>
doc_topics = lda.<span class="fn">transform</span>(dtm)
<span class="fn">print</span>(<span class="st">f"\n첫 번째 문서의 토픽 분포: {doc_topics[0].round(3)}"</span>)
<span class="fn">print</span>(<span class="st">f"지배적 토픽: Topic {doc_topics[0].argmax() + 1}"</span>)
</code></pre>

<h3>8.6 토픽 품질 평가: Coherence Score</h3>

<p>
"토픽 수를 몇 개로 설정해야 하는가?" — R5에서 K-Means의 최적 K를 찾을 때 엘보우 방법과 실루엣 점수를 사용했듯이, LDA에서는 <strong>Coherence Score</strong>를 사용한다. Coherence Score는 각 토픽의 상위 단어들이 얼마나 의미적으로 일관성이 있는지를 측정한다.
</p>

<p>
MLAT Ch.15에서 Jansen은 어닝콜 데이터에 대해 토픽 수를 5~50까지 변화시키며 coherence score를 측정한다. 결과적으로 토픽 수가 증가할수록 coherence가 감소하는 경향을 보이며, 이는 토픽이 너무 세분화되면 품질이 떨어진다는 것을 의미한다.
</p>

<div class="cc">코드 8-3. Gensim으로 Coherence Score 기반 최적 토픽 수 탐색</div>
<pre><code><span class="kw">from</span> gensim.models <span class="kw">import</span> LdaMulticore
<span class="kw">from</span> gensim.corpora <span class="kw">import</span> Dictionary
<span class="kw">from</span> gensim.models.coherencemodel <span class="kw">import</span> CoherenceModel

<span class="cm"># 1. Gensim 형식으로 데이터 준비</span>
<span class="cm"># texts: 토큰화된 문서 리스트 (예: [['fed', 'rate', 'hike'], ['apple', 'earnings', ...]])</span>
dictionary = <span class="fn">Dictionary</span>(texts)
dictionary.<span class="fn">filter_extremes</span>(no_below=<span class="nu">5</span>, no_above=<span class="nu">0.5</span>)
corpus_bow = [dictionary.<span class="fn">doc2bow</span>(doc) <span class="kw">for</span> doc <span class="kw">in</span> texts]

<span class="cm"># 2. 토픽 수별 Coherence Score 계산</span>
coherence_scores = []
topic_range = <span class="fn">range</span>(<span class="nu">3</span>, <span class="nu">21</span>)

<span class="kw">for</span> n_topics <span class="kw">in</span> topic_range:
    model = <span class="fn">LdaMulticore</span>(
        corpus=corpus_bow,
        id2word=dictionary,
        num_topics=n_topics,
        passes=<span class="nu">10</span>,
        workers=<span class="nu">4</span>,
        random_state=<span class="nu">42</span>
    )
    cm = <span class="fn">CoherenceModel</span>(model=model, texts=texts, dictionary=dictionary, coherence=<span class="st">'c_v'</span>)
    coherence_scores.<span class="fn">append</span>(cm.<span class="fn">get_coherence</span>())
    <span class="fn">print</span>(<span class="st">f"Topics={n_topics:2d} | Coherence={coherence_scores[-1]:.4f}"</span>)

<span class="cm"># 3. 시각화</span>
plt.<span class="fn">figure</span>(figsize=(<span class="nu">10</span>, <span class="nu">5</span>))
plt.<span class="fn">plot</span>(<span class="fn">list</span>(topic_range), coherence_scores, <span class="st">'bo-'</span>)
plt.<span class="fn">xlabel</span>(<span class="st">'Number of Topics'</span>)
plt.<span class="fn">ylabel</span>(<span class="st">'Coherence Score (c_v)'</span>)
plt.<span class="fn">title</span>(<span class="st">'LDA Topic Coherence by Number of Topics'</span>)
best_n = <span class="fn">list</span>(topic_range)[np.<span class="fn">argmax</span>(coherence_scores)]
plt.<span class="fn">axvline</span>(x=best_n, color=<span class="st">'r'</span>, linestyle=<span class="st">'--'</span>, label=<span class="st">f'Best: {best_n} topics'</span>)
plt.<span class="fn">legend</span>()
plt.<span class="fn">show</span>()
</code></pre>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.15에서 Jansen은 pyLDAvis를 사용하여 LDA 토픽을 인터랙티브하게 시각화한다. pyLDAvis는 각 토픽의 크기, 토픽 간 거리, 각 토픽의 핵심 단어를 2D 공간에 표시한다. 또한 어닝콜 데이터에 LDA를 적용하여 "경영진이 어떤 주제를 강조하는지"를 자동으로 추출하고, 이를 주가 변동과 연결하는 실험을 수행한다.</p>
</div>


<!-- ==================== Ch.9 ==================== -->
<h2 id="ch9">Chapter 9. Word2Vec — 단어의 의미를 벡터 공간에 임베딩하다</h2>

<h3>9.1 BoW/TF-IDF의 근본적 한계</h3>

<p>
지금까지 배운 BoW와 TF-IDF는 단어를 "원-핫(one-hot)" 방식으로 표현한다. 어휘가 50,000개면 각 단어는 50,000차원 벡터에서 하나의 위치만 1이고 나머지는 0이다. 이 표현의 근본적 문제는 <strong>단어 간 의미적 관계를 전혀 포착하지 못한다</strong>는 것이다. "king"과 "queen"의 코사인 유사도는 0이다 — 완전히 다른 단어로 취급된다.
</p>

<p>
Word2Vec은 이 문제를 혁명적으로 해결한다. 각 단어를 100~300차원의 <strong>밀집 벡터(dense vector)</strong>로 표현하되, 비슷한 문맥에서 사용되는 단어가 비슷한 벡터를 갖도록 학습한다. "king"과 "queen"은 가까운 벡터를 갖게 되고, 심지어 벡터 연산으로 의미적 관계를 표현할 수 있다.
</p>

<div class="def">
<p class="ni"><strong>Word2Vec의 핵심 아이디어 (한 문장)</strong></p>
<p class="ni" style="margin-top:8px">"You shall know a word by the company it keeps." — J.R. Firth (1957)</p>
<p class="ni" style="margin-top:4px;font-size:12px;color:#666">단어의 의미는 그 단어가 함께 사용되는 문맥(context)에 의해 결정된다. 비슷한 문맥에서 등장하는 단어는 비슷한 의미를 갖는다.</p>
</div>

<!-- BoW vs Word2Vec 비교 다이어그램 -->
<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #e74c3c">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#e74c3c;margin-bottom:12px">❌ BoW / TF-IDF (희소 벡터)</p>
<div style="font-family:'Space Mono',monospace;font-size:10px;background:#f8f9fa;padding:12px;border-radius:6px;line-height:1.8">
king &nbsp;= [0, 0, 1, 0, 0, ..., 0] &nbsp;(50,000차원)<br>
queen = [0, 0, 0, 0, 1, ..., 0] &nbsp;(50,000차원)<br>
cosine(king, queen) = <span style="color:#e74c3c;font-weight:bold">0.0</span> 😢
</div>
<p class="ni" style="font-size:11px;color:#888;margin-top:8px;text-align:center">의미적 유사성을 전혀 포착하지 못함</p>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #27ae60">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#27ae60;margin-bottom:12px">✅ Word2Vec (밀집 벡터)</p>
<div style="font-family:'Space Mono',monospace;font-size:10px;background:#f8f9fa;padding:12px;border-radius:6px;line-height:1.8">
king &nbsp;= [0.52, -0.31, 0.78, ...] &nbsp;(300차원)<br>
queen = [0.48, -0.29, 0.81, ...] &nbsp;(300차원)<br>
cosine(king, queen) = <span style="color:#27ae60;font-weight:bold">0.85</span> 🎉
</div>
<p class="ni" style="font-size:11px;color:#888;margin-top:8px;text-align:center">의미적으로 유사한 단어가 가까운 벡터를 가짐</p>
</div>
</div>

<h3>9.2 Word2Vec의 두 가지 아키텍처</h3>

<p>
Word2Vec(Mikolov et al., 2013)은 얕은 신경망(shallow neural network)을 학습하여 단어 임베딩을 생성한다. 두 가지 아키텍처가 있다:
</p>

<!-- Skip-gram vs CBOW 다이어그램 -->
<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:300px;background:#fff;padding:20px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#1976d2;margin-bottom:12px">Skip-gram</p>
<div style="text-align:center;padding:15px;background:#f0f4f8;border-radius:8px;margin-bottom:12px">
<div style="font-size:12px;color:#666;margin-bottom:8px">입력: 중심 단어</div>
<div style="display:inline-block;background:#1976d2;color:#fff;padding:8px 20px;border-radius:20px;font-weight:bold;font-size:13px">earnings</div>
<div style="font-size:20px;margin:8px 0">↓</div>
<div style="font-size:12px;color:#666;margin-bottom:8px">예측: 주변 단어들</div>
<div style="display:flex;gap:6px;justify-content:center;flex-wrap:wrap">
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">strong</span>
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">quarterly</span>
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">beat</span>
<span style="background:#e3f2fd;color:#1565c0;padding:4px 12px;border-radius:15px;font-size:11px">estimates</span>
</div>
</div>
<p class="ni" style="font-size:11px;color:#666;line-height:1.6">
<strong>방식:</strong> 중심 단어 → 주변 단어 예측<br>
<strong>장점:</strong> 드문 단어에 강함, 더 높은 품질<br>
<strong>MLAT 권장:</strong> ⭐ 금융 데이터에 더 적합
</p>
</div>
<div style="flex:1;min-width:300px;background:#fff;padding:20px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;color:#7b1fa2;margin-bottom:12px">CBOW (Continuous Bag of Words)</p>
<div style="text-align:center;padding:15px;background:#faf0ff;border-radius:8px;margin-bottom:12px">
<div style="font-size:12px;color:#666;margin-bottom:8px">입력: 주변 단어들</div>
<div style="display:flex;gap:6px;justify-content:center;flex-wrap:wrap;margin-bottom:8px">
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">strong</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">quarterly</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">beat</span>
<span style="background:#f3e5f5;color:#6a1b9a;padding:4px 12px;border-radius:15px;font-size:11px">estimates</span>
</div>
<div style="font-size:20px;margin:8px 0">↓</div>
<div style="font-size:12px;color:#666;margin-bottom:8px">예측: 중심 단어</div>
<div style="display:inline-block;background:#7b1fa2;color:#fff;padding:8px 20px;border-radius:20px;font-weight:bold;font-size:13px">earnings</div>
</div>
<p class="ni" style="font-size:11px;color:#666;line-height:1.6">
<strong>방식:</strong> 주변 단어들 → 중심 단어 예측<br>
<strong>장점:</strong> 학습이 빠름, 고빈도 단어에 강함<br>
<strong>적합:</strong> 대규모 코퍼스에서 빠른 학습
</p>
</div>
</div>

<h3>9.3 Skip-gram의 수학</h3>

<p>
Skip-gram 모델의 목적 함수를 수학적으로 정리하자. 코퍼스의 모든 단어 \(w_t\)에 대해, 윈도우 크기 \(c\) 내의 문맥 단어 \(w_{t+j}\)를 예측하는 확률을 최대화한다:
</p>

<div class="eq">
$$\max \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$$
</div>

<p>
여기서 \(P(w_{t+j} | w_t)\)는 소프트맥스로 정의된다:
</p>

<div class="eq">
$$P(w_O | w_I) = \frac{\exp(\mathbf{h}^T \mathbf{v}'_{w_O})}{\sum_{w_i \in V} \exp(\mathbf{h}^T \mathbf{v}'_{w_i})}$$
</div>

<p>
하지만 분모에서 전체 어휘에 대한 합을 계산해야 하므로 매우 비효율적이다. 이를 해결하기 위해 <strong>Negative Sampling</strong>을 사용한다.
</p>

<h3>9.4 Negative Sampling: 효율적 학습</h3>

<p>
MLAT Ch.16에서 Jansen은 세 가지 효율화 기법을 소개한다: Hierarchical Softmax, NCE(Noise Contrastive Estimation), Negative Sampling. 실전에서 가장 많이 쓰이는 것은 <strong>Negative Sampling(NEG)</strong>이다.
</p>

<p>
아이디어는 간단하다. 전체 어휘에 대한 소프트맥스 대신, 실제 문맥 단어(positive sample)와 랜덤으로 뽑은 "노이즈" 단어(negative samples) 몇 개만 구별하는 이진 분류 문제로 바꾼다. 보통 5~15개의 negative sample을 사용한다.
</p>

<div class="eq">
$$\log \sigma(\mathbf{v}'_{w_O}{}^T \mathbf{h}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-\mathbf{v}'_{w_i}{}^T \mathbf{h})]$$
</div>

<p>
여기서 \(\sigma\)는 시그모이드 함수, \(k\)는 negative sample 수, \(P_n(w)\)는 노이즈 분포(보통 단어 빈도의 3/4 제곱)다.
</p>

<h3>9.5 의미적 산술: king - man + woman = queen</h3>

<p>
Word2Vec의 가장 놀라운 특성은 <strong>벡터 산술(vector arithmetic)</strong>로 의미적 관계를 표현할 수 있다는 것이다. MLAT Ch.16에서 Jansen은 이를 "semantic arithmetic"이라고 부른다.
</p>

<!-- 벡터 산술 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff8e1,#fff3e0);border-radius:12px;border:2px solid #f9a825">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#e65100">🧮 Word2Vec 벡터 산술</p>
<div style="text-align:center;font-size:13px;line-height:2.5">
<div style="margin-bottom:12px">
<span style="background:#1976d2;color:#fff;padding:6px 14px;border-radius:20px">king</span>
<span style="margin:0 6px;font-size:18px">−</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">man</span>
<span style="margin:0 6px;font-size:18px">+</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">woman</span>
<span style="margin:0 6px;font-size:18px">≈</span>
<span style="background:#2e7d32;color:#fff;padding:6px 14px;border-radius:20px;font-weight:bold">queen</span>
</div>
<div style="margin-bottom:12px">
<span style="background:#1976d2;color:#fff;padding:6px 14px;border-radius:20px">Paris</span>
<span style="margin:0 6px;font-size:18px">−</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">France</span>
<span style="margin:0 6px;font-size:18px">+</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">Japan</span>
<span style="margin:0 6px;font-size:18px">≈</span>
<span style="background:#2e7d32;color:#fff;padding:6px 14px;border-radius:20px;font-weight:bold">Tokyo</span>
</div>
<div>
<span style="background:#1976d2;color:#fff;padding:6px 14px;border-radius:20px">stock</span>
<span style="margin:0 6px;font-size:18px">−</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">equity</span>
<span style="margin:0 6px;font-size:18px">+</span>
<span style="background:#e74c3c;color:#fff;padding:6px 14px;border-radius:20px">debt</span>
<span style="margin:0 6px;font-size:18px">≈</span>
<span style="background:#2e7d32;color:#fff;padding:6px 14px;border-radius:20px;font-weight:bold">bond</span>
</div>
</div>
<p class="ni" style="text-align:center;margin-top:12px;font-size:11px;color:#666">"king - man"은 "왕족" 개념을 추출하고, "woman"을 더하면 "여왕"이 된다.<br>같은 원리로 금융 용어 간의 관계도 포착할 수 있다.</p>
</div>

<h3>9.6 Gensim으로 금융 뉴스 Word2Vec 학습</h3>

<div class="cc">코드 9-1. Gensim Word2Vec으로 금융 임베딩 학습</div>
<pre><code><span class="kw">from</span> gensim.models <span class="kw">import</span> Word2Vec
<span class="kw">from</span> gensim.models.phrases <span class="kw">import</span> Phrases, Phraser
<span class="kw">from</span> gensim.models.word2vec <span class="kw">import</span> LineSentence

<span class="cm"># 1. 문장 로드 (전처리된 텍스트 파일)</span>
sentence_path = <span class="st">'data/financial_news_sentences.txt'</span>
sentences = <span class="fn">LineSentence</span>(sentence_path)

<span class="cm"># 2. 구문 탐지 (bigram, trigram)</span>
<span class="cm">#    "interest" + "rate" → "interest_rate"</span>
phrases = <span class="fn">Phrases</span>(
    sentences=sentences,
    min_count=<span class="nu">10</span>,
    threshold=<span class="nu">0.5</span>,
    delimiter=<span class="st">'_'</span>,
    scoring=<span class="st">'npmi'</span>  <span class="cm"># Normalized Pointwise Mutual Information</span>
)
bigram = <span class="fn">Phraser</span>(phrases)
sentences_with_bigrams = bigram[sentences]

<span class="cm"># 3. Word2Vec 모델 학습</span>
model = <span class="fn">Word2Vec</span>(
    sentences=sentences_with_bigrams,
    sg=<span class="nu">1</span>,            <span class="cm"># 1=Skip-gram (MLAT 권장), 0=CBOW</span>
    vector_size=<span class="nu">300</span>, <span class="cm"># 임베딩 차원</span>
    window=<span class="nu">5</span>,        <span class="cm"># 문맥 윈도우 크기</span>
    min_count=<span class="nu">20</span>,    <span class="cm"># 최소 빈도</span>
    negative=<span class="nu">15</span>,     <span class="cm"># Negative sampling 수</span>
    workers=<span class="nu">8</span>,       <span class="cm"># 병렬 처리 스레드</span>
    epochs=<span class="nu">10</span>,       <span class="cm"># 학습 에포크</span>
    alpha=<span class="nu">0.025</span>,     <span class="cm"># 초기 학습률</span>
    min_alpha=<span class="nu">0.0001</span> <span class="cm"># 최종 학습률</span>
)

<span class="cm"># 4. 모델 저장</span>
model.<span class="fn">save</span>(<span class="st">'models/financial_word2vec.model'</span>)

<span class="cm"># 5. 유사 단어 탐색</span>
<span class="fn">print</span>(<span class="st">"'inflation'과 유사한 단어:"</span>)
<span class="kw">for</span> word, score <span class="kw">in</span> model.wv.<span class="fn">most_similar</span>(<span class="st">'inflation'</span>, topn=<span class="nu">10</span>):
    <span class="fn">print</span>(<span class="st">f"  {word:<25} {score:.4f}"</span>)

<span class="cm"># 6. 벡터 산술</span>
<span class="fn">print</span>(<span class="st">"\n'bull' - 'stock' + 'bond' ≈ ?"</span>)
result = model.wv.<span class="fn">most_similar</span>(positive=[<span class="st">'bull'</span>, <span class="st">'bond'</span>], negative=[<span class="st">'stock'</span>], topn=<span class="nu">5</span>)
<span class="kw">for</span> word, score <span class="kw">in</span> result:
    <span class="fn">print</span>(<span class="st">f"  {word:<25} {score:.4f}"</span>)
</code></pre>

<h3>9.7 GloVe: 사전 학습된 임베딩 활용</h3>

<p>
Word2Vec 외에 <strong>GloVe(Global Vectors for Word Representation)</strong>도 널리 사용된다. Stanford NLP 연구실에서 개발한 GloVe는 전역 단어-단어 동시 출현 통계를 활용하여 임베딩을 학습한다. MLAT Ch.16에서 Jansen은 Wikipedia 20억 토큰으로 학습된 GloVe 벡터가 word2vec 유추 테스트에서 75.44% 정확도를 달성한다고 보고한다.
</p>

<div class="tc">표 9-1. 사전 학습된 GloVe 벡터 (MLAT Ch.16)</div>
<table>
<thead>
<tr><th>소스</th><th>토큰 수</th><th>어휘 크기</th><th>유추 정확도</th></tr>
</thead>
<tbody>
<tr><td>Wikipedia + Gigaword</td><td>60억</td><td>40만</td><td>75.4%</td></tr>
<tr><td>Common Crawl (42B)</td><td>420억</td><td>190만</td><td>78.0%</td></tr>
<tr><td>Twitter</td><td>270억</td><td>120만</td><td>56.4%</td></tr>
</tbody>
</table>

<div class="warn">
<p class="ni"><strong>⚠️ 사전 학습 vs 도메인 특화 임베딩 (MLAT Ch.16)</strong></p>
<p class="ni" style="margin-top:8px">Wikipedia로 학습된 GloVe는 일반적인 언어 관계를 잘 포착하지만, 금융 전문 용어의 미묘한 차이는 놓칠 수 있다. "quantitative easing"이나 "yield curve inversion" 같은 금융 특수 표현은 금융 뉴스/공시로 직접 학습한 임베딩이 더 정확하다. MLAT에서 SEC 10-K 공시로 학습한 커스텀 임베딩이 38.5% 유추 정확도를 달성했는데, 이는 일반 코퍼스보다 낮지만 금융 특화 유추에서는 더 높은 성능을 보인다.</p>
</div>


<!-- ==================== Ch.10 ==================== -->
<h2 id="ch10">Chapter 10. Doc2Vec + 감성분석 — 문서 전체를 벡터로</h2>

<h3>10.1 Word2Vec에서 Doc2Vec으로</h3>

<p>
Word2Vec은 개별 단어를 벡터로 변환한다. 하지만 감성분석이나 문서 분류에서는 문서 전체를 하나의 벡터로 표현해야 한다. 가장 단순한 방법은 문서 내 모든 단어의 Word2Vec 벡터를 평균내는 것이다. 하지만 이 방법은 단어 순서 정보를 완전히 잃는다.
</p>

<p>
<strong>Doc2Vec</strong>(Paragraph Vector, Le & Mikolov, 2014)은 이 문제를 해결한다. Word2Vec을 확장하여 문서 자체에도 고유한 벡터를 부여한다. 학습 과정에서 문서 벡터가 해당 문서의 "기억(memory)" 역할을 하여, 단어 순서와 문맥 정보를 보존한다.
</p>

<!-- Doc2Vec 아키텍처 다이어그램 -->
<div style="margin:25px 0;display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:13px;color:#1976d2;margin-bottom:12px">DM (Distributed Memory)</p>
<div style="text-align:center;padding:12px;background:#f0f4f8;border-radius:8px;font-size:11px">
<div style="margin-bottom:8px">입력: <span style="background:#1976d2;color:#fff;padding:2px 8px;border-radius:10px">Doc ID</span> + 문맥 단어들</div>
<div style="margin-bottom:4px">↓ 연결(concatenate) 또는 평균</div>
<div>예측: 다음 단어</div>
</div>
<p class="ni" style="font-size:11px;color:#666;margin-top:8px">Word2Vec Skip-gram에 대응. 문서 벡터가 "기억" 역할을 하여 문맥 정보를 보존한다.</p>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:20px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="text-align:center;font-weight:bold;font-size:13px;color:#7b1fa2;margin-bottom:12px">DBOW (Distributed Bag of Words)</p>
<div style="text-align:center;padding:12px;background:#faf0ff;border-radius:8px;font-size:11px">
<div style="margin-bottom:8px">입력: <span style="background:#7b1fa2;color:#fff;padding:2px 8px;border-radius:10px">Doc ID</span></div>
<div style="margin-bottom:4px">↓</div>
<div>예측: 문서 내 랜덤 단어</div>
</div>
<p class="ni" style="font-size:11px;color:#666;margin-top:8px">Word2Vec CBOW에 대응. 더 빠르고 간단하지만, 단어 순서 정보를 덜 보존한다.</p>
</div>
</div>

<h3>10.2 Gensim Doc2Vec 실전</h3>

<div class="cc">코드 10-1. Doc2Vec으로 금융 문서 임베딩 + 감성 분류</div>
<pre><code><span class="kw">from</span> gensim.models.doc2vec <span class="kw">import</span> Doc2Vec, TaggedDocument
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.linear_model <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.ensemble <span class="kw">import</span> RandomForestClassifier
<span class="kw">import</span> lightgbm <span class="kw">as</span> lgb
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd

<span class="cm"># 1. 데이터 준비 (TaggedDocument 형식)</span>
<span class="cm"># df: 'text' (전처리된 텍스트), 'sentiment' (0/1/2) 컬럼</span>
documents = []
<span class="kw">for</span> i, row <span class="kw">in</span> df.<span class="fn">iterrows</span>():
    tokens = row[<span class="st">'text'</span>].<span class="fn">split</span>()
    documents.<span class="fn">append</span>(<span class="fn">TaggedDocument</span>(words=tokens, tags=[i]))

<span class="cm"># 2. Doc2Vec 모델 학습</span>
model = <span class="fn">Doc2Vec</span>(
    documents=documents,
    dm=<span class="nu">1</span>,            <span class="cm"># 1=DM (Distributed Memory), 0=DBOW</span>
    vector_size=<span class="nu">300</span>, <span class="cm"># 문서 벡터 차원</span>
    window=<span class="nu">5</span>,        <span class="cm"># 문맥 윈도우</span>
    min_count=<span class="nu">50</span>,    <span class="cm"># 최소 빈도</span>
    negative=<span class="nu">5</span>,      <span class="cm"># Negative sampling</span>
    epochs=<span class="nu">20</span>,       <span class="cm"># 학습 에포크</span>
    workers=<span class="nu">4</span>
)

<span class="cm"># 3. 문서 벡터 추출 → ML 피처로 사용</span>
X = np.<span class="fn">zeros</span>((len(df), <span class="nu">300</span>))
<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(df)):
    X[i] = model.dv[i]

y = df[<span class="st">'sentiment'</span>].values

<span class="cm"># 4. Train/Test 분할</span>
X_train, X_test, y_train, y_test = <span class="fn">train_test_split</span>(
    X, y, test_size=<span class="nu">0.2</span>, random_state=<span class="nu">42</span>, stratify=y
)

<span class="cm"># 5. 여러 분류기 비교 (MLAT Ch.16 방식)</span>
models = {
    <span class="st">'Logistic Regression'</span>: <span class="fn">LogisticRegression</span>(multi_class=<span class="st">'multinomial'</span>, max_iter=<span class="nu">1000</span>),
    <span class="st">'Random Forest'</span>: <span class="fn">RandomForestClassifier</span>(n_estimators=<span class="nu">500</span>, n_jobs=-<span class="nu">1</span>),
}

<span class="kw">for</span> name, clf <span class="kw">in</span> models.items():
    clf.<span class="fn">fit</span>(X_train, y_train)
    acc = clf.<span class="fn">score</span>(X_test, y_test)
    <span class="fn">print</span>(<span class="st">f"{name}: {acc:.4f}"</span>)

<span class="cm"># LightGBM</span>
train_data = lgb.<span class="fn">Dataset</span>(X_train, label=y_train)
test_data = lgb.<span class="fn">Dataset</span>(X_test, label=y_test, reference=train_data)
params = {<span class="st">'objective'</span>: <span class="st">'multiclass'</span>, <span class="st">'num_classes'</span>: <span class="nu">3</span>, <span class="st">'verbose'</span>: -<span class="nu">1</span>}
lgb_model = lgb.<span class="fn">train</span>(params, train_data, num_boost_round=<span class="nu">5000</span>,
                      valid_sets=[test_data],
                      callbacks=[lgb.<span class="fn">early_stopping</span>(<span class="nu">25</span>)])
lgb_pred = lgb_model.<span class="fn">predict</span>(X_test).<span class="fn">argmax</span>(axis=<span class="nu">1</span>)
<span class="fn">print</span>(<span class="st">f"LightGBM: {(lgb_pred == y_test).mean():.4f}"</span>)
</code></pre>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.16에서 Jansen은 Yelp 리뷰 50만 개에 Doc2Vec을 적용하여 감성 분류를 수행한다. LightGBM이 62.24%로 가장 높은 정확도를 달성했다. 이는 TF-IDF + LightGBM(73.6%)보다 낮은데, Jansen은 이를 "Doc2Vec은 더 적은 피처(300차원)로 작동하므로, 데이터가 충분하면 TF-IDF가 더 나을 수 있다"고 설명한다. 하지만 Doc2Vec의 장점은 (1) 밀집 벡터라 메모리 효율적, (2) 새 문서에 대한 추론이 가능, (3) 다른 ML 모델의 입력 피처로 활용 가능하다는 점이다.</p>
</div>


<h3>10.3 DM vs DBOW: 수학적 비교</h3>

<p>
Doc2Vec의 두 아키텍처를 수학적으로 비교해보자.
</p>

<p>
<strong>DM (Distributed Memory)</strong>의 목적함수는 Word2Vec CBOW를 확장한 것이다. 
문서 벡터 \(\mathbf{d}\)와 문맥 단어 벡터 \(\mathbf{w}_{t-k}, \ldots, \mathbf{w}_{t+k}\)를 
결합하여 타겟 단어 \(w_t\)를 예측한다:
</p>

$$\max \sum_{t=k}^{T-k} \log P(w_t \mid \mathbf{d}, w_{t-k}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+k})$$

<p>
<strong>DBOW (Distributed Bag of Words)</strong>는 더 단순하다. 문서 벡터만으로 문서 내 랜덤 단어를 예측한다:
</p>

$$\max \sum_{w \in d} \log P(w \mid \mathbf{d})$$

<div class="tc">표 10-1. DM vs DBOW 비교</div>
<table>
<thead>
<tr><th>특성</th><th>DM (Distributed Memory)</th><th>DBOW (Distributed BoW)</th></tr>
</thead>
<tbody>
<tr><td><strong>입력</strong></td><td>문서 ID + 문맥 단어</td><td>문서 ID만</td></tr>
<tr><td><strong>예측 대상</strong></td><td>다음 단어</td><td>문서 내 랜덤 단어</td></tr>
<tr><td><strong>순서 정보</strong></td><td>보존 (윈도우 내)</td><td>무시</td></tr>
<tr><td><strong>학습 속도</strong></td><td>느림</td><td>빠름</td></tr>
<tr><td><strong>성능 (일반적)</strong></td><td>긴 문서에서 우수</td><td>짧은 문서에서 우수</td></tr>
<tr><td><strong>Word2Vec 대응</strong></td><td>CBOW 확장</td><td>Skip-gram 확장</td></tr>
<tr><td><strong>Gensim 파라미터</strong></td><td><code>dm=1</code></td><td><code>dm=0</code></td></tr>
</tbody>
</table>

<div class="ok">
<p class="ni"><strong>실전 팁: DM + DBOW 결합</strong></p>
<p class="ni" style="margin-top:8px">
Le & Mikolov의 원 논문과 MLAT Ch.16 모두 DM과 DBOW 벡터를 <strong>연결(concatenate)</strong>하여 
사용하는 것을 권장한다. 예를 들어 DM으로 300차원, DBOW로 300차원을 학습한 뒤 600차원 벡터로 
결합하면, 순서 정보(DM)와 전역 의미(DBOW)를 모두 활용할 수 있다.
</p>
</div>

<h3>10.4 Doc2Vec vs 다른 문서 표현 방법 비교</h3>

<p>
문서를 벡터로 표현하는 방법은 여러 가지가 있다. 각 방법의 장단점을 비교해보자.
</p>

<!-- 문서 표현 방법 비교 다이어그램 -->
<div style="margin:25px 0;display:flex;gap:12px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #ff9800;text-align:center">
<div style="font-size:20px;margin-bottom:6px">📊</div>
<div style="font-weight:bold;color:#e65100;font-size:12px;margin-bottom:6px">TF-IDF 평균</div>
<div style="font-size:10px;color:#666;line-height:1.5">희소 벡터<br>|V|차원<br>빠르고 단순<br>문맥 무시</div>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #2196f3;text-align:center">
<div style="font-size:20px;margin-bottom:6px">📐</div>
<div style="font-weight:bold;color:#1565c0;font-size:12px;margin-bottom:6px">Word2Vec 평균</div>
<div style="font-size:10px;color:#666;line-height:1.5">밀집 벡터<br>300차원<br>의미 보존<br>순서 무시</div>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #4caf50;text-align:center">
<div style="font-size:20px;margin-bottom:6px">📄</div>
<div style="font-weight:bold;color:#2e7d32;font-size:12px;margin-bottom:6px">Doc2Vec</div>
<div style="font-size:10px;color:#666;line-height:1.5">밀집 벡터<br>300차원<br>문서 고유 벡터<br>순서 일부 보존</div>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:14px;border-radius:10px;border:2px solid #9c27b0;text-align:center">
<div style="font-size:20px;margin-bottom:6px">🤖</div>
<div style="font-weight:bold;color:#7b1fa2;font-size:12px;margin-bottom:6px">BERT [CLS]</div>
<div style="font-size:10px;color:#666;line-height:1.5">밀집 벡터<br>768차원<br>문맥 완전 이해<br>계산 비용 높음</div>
</div>
</div>

<div class="cc">코드 10-2. 4가지 문서 표현 방법 성능 비교</div>
<pre><code><span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.linear_model <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> cross_val_score
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 가정: texts (리스트), labels (배열)이 준비되어 있다</span>

<span class="cm"># 방법 1: TF-IDF</span>
tfidf = <span class="fn">TfidfVectorizer</span>(max_features=<span class="nu">5000</span>, stop_words=<span class="st">'english'</span>)
X_tfidf = tfidf.<span class="fn">fit_transform</span>(texts)

<span class="cm"># 방법 2: Word2Vec 평균 (사전학습 모델 사용)</span>
<span class="kw">from</span> gensim.models <span class="kw">import</span> KeyedVectors
<span class="cm"># wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)</span>
<span class="kw">def</span> <span class="fn">doc_to_w2v_mean</span>(text, wv, dim=<span class="nu">300</span>):
    <span class="st">"""문서 내 단어 벡터의 평균"""</span>
    words = text.<span class="fn">lower</span>().<span class="fn">split</span>()
    vecs = [wv[w] <span class="kw">for</span> w <span class="kw">in</span> words <span class="kw">if</span> w <span class="kw">in</span> wv]
    <span class="kw">if</span> <span class="nb">len</span>(vecs) == <span class="nu">0</span>:
        <span class="kw">return</span> np.<span class="fn">zeros</span>(dim)
    <span class="kw">return</span> np.<span class="fn">mean</span>(vecs, axis=<span class="nu">0</span>)

<span class="cm"># 방법 3: Doc2Vec (위에서 학습한 모델)</span>
<span class="cm"># X_d2v = np.array([d2v_model.dv[i] for i in range(len(texts))])</span>

<span class="cm"># 방법 4: BERT [CLS] 토큰 (GPU 필요)</span>
<span class="cm"># from transformers import AutoTokenizer, AutoModel</span>
<span class="cm"># tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')</span>
<span class="cm"># model = AutoModel.from_pretrained('bert-base-uncased')</span>

<span class="cm"># 성능 비교 (TF-IDF 예시)</span>
clf = <span class="fn">LogisticRegression</span>(max_iter=<span class="nu">1000</span>)
scores = <span class="fn">cross_val_score</span>(clf, X_tfidf, labels, cv=<span class="nu">5</span>, scoring=<span class="st">'accuracy'</span>)
<span class="fn">print</span>(<span class="st">f"TF-IDF + LogReg: {scores.mean():.4f} ± {scores.std():.4f}"</span>)

<span class="cm"># MLAT Ch.16 벤치마크 결과 (Yelp 리뷰 50만 개):</span>
<span class="cm"># TF-IDF + LightGBM:  73.6%</span>
<span class="cm"># Doc2Vec + LightGBM: 62.2%</span>
<span class="cm"># Word2Vec avg + LR:  ~65%</span>
<span class="cm"># BERT fine-tuned:    ~90%+ (but much slower)</span></code></pre>

<div class="warn">
<p class="ni"><strong>⚠️ Doc2Vec의 현실적 위치 (MLAT Ch.16 결론)</strong></p>
<p class="ni" style="margin-top:8px">
Jansen의 실험 결과에서 Doc2Vec은 TF-IDF보다 낮은 성능을 보였다. 이는 Doc2Vec이 나쁜 모델이라는 뜻이 아니라, 
<strong>데이터가 충분하고 어휘가 풍부한 경우 TF-IDF의 고차원 희소 표현이 더 많은 정보를 담을 수 있다</strong>는 것이다. 
Doc2Vec의 진정한 가치는 (1) 메모리 효율성 (300차원 vs 50,000차원), (2) 새 문서에 대한 추론 가능, 
(3) 다른 피처와 결합하기 쉬운 밀집 벡터라는 점이다. 실전에서는 TF-IDF와 Doc2Vec을 모두 피처로 사용하고, 
모델이 알아서 유용한 정보를 선택하게 하는 것이 좋다.
</p>
</div>

<!-- ==================== Ch.11 ==================== -->
<h2 id="ch11">Chapter 11. Transformer와 BERT — NLP의 게임 체인저</h2>

<h3>11.1 Attention is All You Need</h3>

<p>
2017년, Google의 "Attention is All You Need" 논문(Vaswani et al.)이 NLP의 판도를 완전히 바꿨다. 기존의 RNN(순환 신경망)은 텍스트를 순차적으로 처리해야 했기 때문에 (1) 먼 거리의 단어 관계를 포착하기 어렵고, (2) 병렬 처리가 불가능했다. Transformer는 <strong>Self-Attention</strong> 메커니즘으로 이 두 문제를 동시에 해결했다.
</p>

<div class="def">
<p class="ni"><strong>Self-Attention의 핵심 아이디어</strong></p>
<p class="ni" style="margin-top:8px">문장의 각 단어가 다른 모든 단어와의 관계를 동시에 계산한다. "The Fed raised rates because inflation was persistent"에서 "rates"를 이해하려면 "Fed"와 "inflation"이 중요하다는 것을 attention score로 학습한다.</p>
</div>

<!-- Attention 시각화 -->
<div style="margin:25px 0;padding:20px;background:#f8f9fa;border-radius:10px;border:1px solid #dee2e6">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#2c3e50">🔍 Self-Attention 시각화: "rates"에 대한 attention</p>
<div style="display:flex;justify-content:center;gap:6px;flex-wrap:wrap;margin-bottom:12px">
<span style="background:rgba(25,118,210,0.7);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">The</span>
<span style="background:rgba(25,118,210,0.9);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px;font-weight:bold">Fed</span>
<span style="background:rgba(25,118,210,0.5);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">raised</span>
<span style="background:rgba(233,30,99,1.0);color:#fff;padding:6px 14px;border-radius:6px;font-size:13px;font-weight:bold;border:2px solid #c2185b">rates ← target</span>
<span style="background:rgba(25,118,210,0.3);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">because</span>
<span style="background:rgba(25,118,210,0.85);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px;font-weight:bold">inflation</span>
<span style="background:rgba(25,118,210,0.2);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">was</span>
<span style="background:rgba(25,118,210,0.6);color:#fff;padding:6px 12px;border-radius:6px;font-size:12px">persistent</span>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#666">색이 진할수록 "rates"에 대한 attention score가 높다. "Fed"와 "inflation"이 가장 높은 attention을 받는다.</p>
</div>

<h3>11.2 Attention의 수학</h3>

<p>
Self-Attention은 Query(Q), Key(K), Value(V) 세 가지 행렬을 사용한다. 입력 임베딩 \(X\)에 학습 가능한 가중치 행렬을 곱하여 Q, K, V를 생성한다:
</p>

<div class="eq">
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
</div>

<p>
여기서 \(d_k\)는 Key 벡터의 차원이다. \(\sqrt{d_k}\)로 나누는 것은 내적 값이 너무 커지는 것을 방지하기 위한 스케일링이다. Transformer는 이 attention을 여러 개의 "head"로 병렬 실행하는 <strong>Multi-Head Attention</strong>을 사용한다.
</p>

<h3>11.3 BERT: 양방향 사전 학습</h3>

<p>
MLAT Ch.16에서 Jansen은 BERT(Bidirectional Encoder Representations from Transformers, Devlin et al., 2019)를 "NLP의 게임 체인저"로 소개한다. BERT의 핵심 혁신은 두 가지다:
</p>

<ol>
<li><strong>양방향 문맥:</strong> Word2Vec이나 GloVe는 단어에 하나의 고정된 벡터를 부여한다. "bank"가 "은행"인지 "강둑"인지 구분하지 못한다. BERT는 문맥에 따라 다른 벡터를 생성한다.</li>
<li><strong>사전 학습 + 미세 조정:</strong> Wikipedia + BookCorpus(33억 단어)로 사전 학습한 후, 특정 태스크(감성분석, NER 등)에 맞게 미세 조정(fine-tuning)한다.</li>
</ol>

<!-- BERT 아키텍처 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e3f2fd,#bbdefb);border-radius:12px;border:1px solid #90caf9">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#1565c0">🏗️ BERT 아키텍처 개요</p>
<div style="display:flex;flex-direction:column;gap:8px;max-width:500px;margin:0 auto;font-size:12px">

<div style="background:#fff;padding:10px 14px;border-radius:8px;text-align:center;border:1px solid #90caf9">
<strong>사전 학습 (Pre-training)</strong><br>
<span style="font-size:11px;color:#666">Task 1: Masked Language Model (단어 15% 마스킹 → 예측)<br>
Task 2: Next Sentence Prediction (다음 문장 맞추기)</span>
</div>

<div style="text-align:center;font-size:16px;color:#1976d2">↓</div>

<div style="background:#fff;padding:10px 14px;border-radius:8px;text-align:center;border:1px solid #90caf9">
<strong>BERT 모델</strong><br>
<span style="font-size:11px;color:#666">12 layers × 12 attention heads = 144 attention 메커니즘<br>
110M 파라미터 (BERT-base) / 340M (BERT-large)</span>
</div>

<div style="text-align:center;font-size:16px;color:#1976d2">↓</div>

<div style="background:#fff;padding:10px 14px;border-radius:8px;text-align:center;border:1px solid #90caf9">
<strong>미세 조정 (Fine-tuning)</strong><br>
<span style="font-size:11px;color:#666">감성분석, NER, 질의응답, 텍스트 분류 등<br>
소량의 라벨 데이터로 특정 태스크에 적응</span>
</div>

</div>
</div>

<h3>11.4 FinBERT: 금융 특화 BERT</h3>

<p>
일반 BERT는 Wikipedia로 학습되었기 때문에 금융 텍스트의 미묘한 뉘앙스를 완벽히 포착하지 못한다. 이를 해결하기 위해 <strong>FinBERT</strong>(Araci, 2019)가 개발되었다. FinBERT는 BERT를 금융 뉴스와 기업 공시 데이터로 추가 학습(domain-adaptive pre-training)한 모델이다.
</p>

<div class="cc">코드 11-1. FinBERT로 금융 뉴스 감성분석</div>
<pre><code><span class="kw">from</span> transformers <span class="kw">import</span> AutoTokenizer, AutoModelForSequenceClassification
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F

<span class="cm"># 1. FinBERT 모델 로드</span>
model_name = <span class="st">"ProsusAI/finbert"</span>
tokenizer = <span class="fn">AutoTokenizer</span>.<span class="fn">from_pretrained</span>(model_name)
model = <span class="fn">AutoModelForSequenceClassification</span>.<span class="fn">from_pretrained</span>(model_name)

<span class="cm"># 2. 금융 뉴스 감성 분석</span>
headlines = [
    <span class="st">"Apple reported record quarterly revenue of $123.9 billion"</span>,
    <span class="st">"Tesla shares plunged 12% after missing delivery targets"</span>,
    <span class="st">"Federal Reserve kept interest rates unchanged as expected"</span>,
    <span class="st">"Goldman Sachs sees significant upside potential in AI stocks"</span>,
    <span class="st">"Oil prices fell sharply amid global recession fears"</span>,
    <span class="st">"Company announced massive layoffs affecting 10000 employees"</span>,
]

label_map = {<span class="nu">0</span>: <span class="st">'Positive'</span>, <span class="nu">1</span>: <span class="st">'Negative'</span>, <span class="nu">2</span>: <span class="st">'Neutral'</span>}

<span class="fn">print</span>(<span class="st">f"{'헤드라인':<55} {'감성':<10} {'확률'}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">85</span>)

<span class="kw">for</span> headline <span class="kw">in</span> headlines:
    inputs = <span class="fn">tokenizer</span>(headline, return_tensors=<span class="st">"pt"</span>, padding=<span class="kw">True</span>, truncation=<span class="kw">True</span>, max_length=<span class="nu">512</span>)
    
    <span class="kw">with</span> torch.<span class="fn">no_grad</span>():
        outputs = <span class="fn">model</span>(**inputs)
    
    probs = F.<span class="fn">softmax</span>(outputs.logits, dim=-<span class="nu">1</span>)[<span class="nu">0</span>]
    pred = probs.<span class="fn">argmax</span>().<span class="fn">item</span>()
    
    <span class="fn">print</span>(<span class="st">f"{headline:<55} {label_map[pred]:<10} "
          f"P={probs[0]:.2f} N={probs[1]:.2f} U={probs[2]:.2f}"</span>)
</code></pre>

<div class="ok">
<p class="ni"><strong>FinBERT vs 전통 방법 성능 비교</strong></p>
<div style="margin-top:10px">
<table style="font-size:12px">
<thead>
<tr><th>방법</th><th>모델</th><th>피처</th><th>정확도 (금융 감성)</th><th>학습 시간</th></tr>
</thead>
<tbody>
<tr><td>사전 기반</td><td>Loughran-McDonald</td><td>감성 사전</td><td>~60%</td><td>즉시</td></tr>
<tr><td>전통 ML</td><td>NB + TF-IDF</td><td>50,000차원 희소</td><td>~65%</td><td>초</td></tr>
<tr><td>전통 ML</td><td>LightGBM + TF-IDF</td><td>50,000차원 희소</td><td>~74%</td><td>분</td></tr>
<tr><td>임베딩</td><td>LightGBM + Doc2Vec</td><td>300차원 밀집</td><td>~62%</td><td>시간</td></tr>
<tr style="background:#d4edda"><td><strong>Transformer</strong></td><td><strong>FinBERT</strong></td><td><strong>768차원 문맥 임베딩</strong></td><td><strong>~87%</strong></td><td><strong>GPU 시간</strong></td></tr>
</tbody>
</table>
</div>
</div>

<div class="info">
<p class="ni"><strong>교재 연동:</strong> MLAT Ch.16의 마지막 섹션 "New frontiers – pretrained transformer models"에서 Jansen은 Attention 메커니즘, BERT, GPT 등의 최신 모델을 소개한다. 그는 "2018 is now considered a turning point for NLP research"라고 강조하며, BERT가 GLUE 벤치마크에서 인간 수준을 넘어선 것을 언급한다. R7(딥러닝)과 R8(Transformer)에서 이 내용을 더 깊이 다룰 예정이다.</p>
</div>



<h3>11.5 Self-Attention 계산 예제: 단계별 워크스루</h3>

<p>
Self-Attention을 구체적인 숫자로 따라가보자. 3개 단어 "Fed raises rates"를 처리한다고 하자.
</p>

<div class="cc">코드 11-2. Self-Attention 수동 계산 (NumPy)</div>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 입력: 3개 단어의 임베딩 (d_model=4로 단순화)</span>
<span class="cm"># 실제 BERT는 d_model=768</span>
X = np.<span class="fn">array</span>([
    [<span class="nu">1.0</span>, <span class="nu">0.5</span>, <span class="nu">0.2</span>, <span class="nu">0.1</span>],  <span class="cm"># "Fed"</span>
    [<span class="nu">0.3</span>, <span class="nu">0.8</span>, <span class="nu">0.9</span>, <span class="nu">0.4</span>],  <span class="cm"># "raises"</span>
    [<span class="nu">0.2</span>, <span class="nu">0.1</span>, <span class="nu">0.7</span>, <span class="nu">1.0</span>],  <span class="cm"># "rates"</span>
])

<span class="cm"># 가중치 행렬 (학습되는 파라미터)</span>
np.random.<span class="fn">seed</span>(<span class="nu">42</span>)
d_k = <span class="nu">3</span>  <span class="cm"># Key/Query 차원</span>
W_Q = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, d_k) * <span class="nu">0.5</span>
W_K = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, d_k) * <span class="nu">0.5</span>
W_V = np.random.<span class="fn">randn</span>(<span class="nu">4</span>, d_k) * <span class="nu">0.5</span>

<span class="cm"># Step 1: Q, K, V 계산</span>
Q = X @ W_Q  <span class="cm"># (3, 3)</span>
K = X @ W_K  <span class="cm"># (3, 3)</span>
V = X @ W_V  <span class="cm"># (3, 3)</span>
<span class="fn">print</span>(<span class="st">"Q (Query):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(Q, <span class="nu">3</span>))

<span class="cm"># Step 2: Attention Score = Q @ K^T / sqrt(d_k)</span>
scores = Q @ K.T / np.<span class="fn">sqrt</span>(d_k)
<span class="fn">print</span>(<span class="st">f"\nAttention Scores (before softmax):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(scores, <span class="nu">3</span>))

<span class="cm"># Step 3: Softmax → Attention Weights</span>
<span class="kw">def</span> <span class="fn">softmax</span>(x):
    exp_x = np.<span class="fn">exp</span>(x - np.<span class="fn">max</span>(x, axis=-<span class="nu">1</span>, keepdims=<span class="kw">True</span>))
    <span class="kw">return</span> exp_x / exp_x.<span class="fn">sum</span>(axis=-<span class="nu">1</span>, keepdims=<span class="kw">True</span>)

weights = <span class="fn">softmax</span>(scores)
<span class="fn">print</span>(<span class="st">f"\nAttention Weights (after softmax):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(weights, <span class="nu">3</span>))
<span class="fn">print</span>(<span class="st">f"\n각 행의 합 = {weights.sum(axis=1)}"</span>)  <span class="cm"># 모두 1.0</span>

<span class="cm"># Step 4: Output = Weights @ V</span>
output = weights @ V
<span class="fn">print</span>(<span class="st">f"\nOutput (문맥 반영된 표현):"</span>)
<span class="fn">print</span>(np.<span class="fn">round</span>(output, <span class="nu">3</span>))

<span class="cm"># 해석: "Fed"의 출력 벡터는 "raises"와 "rates"의 정보를 포함한다</span>
<span class="cm"># weights[0]을 보면 "Fed"가 어떤 단어에 얼마나 주의를 기울이는지 알 수 있다</span>
<span class="fn">print</span>(<span class="st">f"\n'Fed'의 Attention 분포: Fed={weights[0,0]:.3f}, raises={weights[0,1]:.3f}, rates={weights[0,2]:.3f}"</span>)</code></pre>

<!-- Attention 시각화 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:#f0f4f8;border-radius:10px;border:1px solid #cfd8dc">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:15px;font-size:14px;color:#2c3e50">Self-Attention 시각화: "Fed raises rates"</p>
<div style="display:flex;justify-content:center;gap:40px;flex-wrap:wrap">
<div style="text-align:center">
<div style="font-weight:bold;font-size:13px;color:#1976d2;margin-bottom:10px">Fed가 보는 세계</div>
<div style="display:flex;gap:8px;justify-content:center">
<div style="padding:8px 14px;border-radius:8px;background:#1976d2;color:#fff;font-size:12px;opacity:0.9">Fed <span style="font-size:10px">(0.45)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#1976d2;color:#fff;font-size:12px;opacity:0.6">raises <span style="font-size:10px">(0.30)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#1976d2;color:#fff;font-size:12px;opacity:0.4">rates <span style="font-size:10px">(0.25)</span></div>
</div>
</div>
<div style="text-align:center">
<div style="font-weight:bold;font-size:13px;color:#2e7d32;margin-bottom:10px">rates가 보는 세계</div>
<div style="display:flex;gap:8px;justify-content:center">
<div style="padding:8px 14px;border-radius:8px;background:#2e7d32;color:#fff;font-size:12px;opacity:0.3">Fed <span style="font-size:10px">(0.15)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#2e7d32;color:#fff;font-size:12px;opacity:0.7">raises <span style="font-size:10px">(0.40)</span></div>
<div style="padding:8px 14px;border-radius:8px;background:#2e7d32;color:#fff;font-size:12px;opacity:0.9">rates <span style="font-size:10px">(0.45)</span></div>
</div>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#777;margin-top:12px">
각 단어는 다른 단어들에 대해 서로 다른 attention weight를 갖는다.<br>
"rates"는 "raises"에 높은 가중치를 부여한다 — "금리를 올린다"는 문맥을 포착하는 것이다.
</p>
</div>

<h3>11.6 Multi-Head Attention: 여러 관점에서 동시에 보기</h3>

<p>
Single-head attention은 하나의 관점에서만 문맥을 파악한다. 하지만 언어에는 여러 종류의 관계가 동시에 존재한다. 
"The Fed raised rates because inflation was persistent"에서:
</p>

<ul>
<li><strong>구문적 관계:</strong> "Fed" → "raised" (주어-동사)</li>
<li><strong>의미적 관계:</strong> "rates" → "inflation" (인과)</li>
<li><strong>감성적 관계:</strong> "raised" + "persistent" → 매파적(hawkish)</li>
</ul>

<p>
<strong>Multi-Head Attention</strong>은 이 문제를 해결한다. \(h\)개의 독립적인 attention head를 병렬로 실행하고, 
결과를 연결(concatenate)한 뒤 선형 변환한다:
</p>

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \cdot W^O$$

<p>
여기서 각 head는 독립적인 가중치 행렬을 사용한다:
</p>

$$\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$

<p>
BERT-base는 \(h = 12\)개의 head를 사용한다. 각 head의 차원은 \(d_k = d_{\text{model}} / h = 768 / 12 = 64\)이다. 
12개의 head가 각각 다른 종류의 관계를 학습한다 — 어떤 head는 구문을, 어떤 head는 의미를, 어떤 head는 위치 관계를 포착한다.
</p>

<div class="def">
<p class="ni"><strong>BERT 아키텍처 핵심 숫자 (BERT-base)</strong></p>
<ul style="margin-top:8px">
<li>레이어 수: 12</li>
<li>Hidden size (d_model): 768</li>
<li>Attention heads: 12</li>
<li>Head 차원 (d_k): 64</li>
<li>총 파라미터: 110M</li>
<li>최대 시퀀스 길이: 512 토큰</li>
<li>사전학습 데이터: BookCorpus + English Wikipedia (3.3B 단어)</li>
</ul>
</div>

<!-- ==================== Ch.12 ==================== -->
<h2 id="ch12">Chapter 12. 금융 NLP 실전 — SEC 공시에서 알파 추출</h2>

<h3>12.1 SEC 10-K 공시 분석 파이프라인</h3>

<p>
MLAT Ch.16에서 Jansen은 SEC 10-K 연간 보고서 22,000건을 분석하여 주가 예측에 활용하는 실전 파이프라인을 구축한다. 이것이 NLP를 알고리즘 트레이딩에 적용하는 가장 구체적인 사례다.
</p>

<!-- SEC 파이프라인 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#f3e5f5,#e1bee7);border-radius:12px;border:1px solid #ce93d8">
<p class="ni" style="text-align:center;font-weight:bold;margin-bottom:16px;font-size:14px;color:#6a1b9a">📋 SEC 10-K → 주가 예측 파이프라인 (MLAT Ch.16)</p>
<div style="display:flex;flex-direction:column;gap:8px;max-width:600px;margin:0 auto;font-size:12px">

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">1</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>데이터 수집:</strong> SEC EDGAR에서 10-K 공시 다운로드 (22,000건, 2013-2016)<br>
<span style="color:#888;font-size:11px">핵심 섹션: Item 1(사업), Item 1A(리스크), Item 7(경영진 논의), Item 7A(시장 리스크)</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">2</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>전처리:</strong> spaCy로 토큰화 + 구문 탐지 (Gensim Phrases)<br>
<span style="color:#888;font-size:11px">결과: 201,000개 토큰 어휘, bigram 포함 (common_stock, interest_rates 등)</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">3</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>Word2Vec 학습:</strong> Skip-gram, 300차원, window=3, min_count=50<br>
<span style="color:#888;font-size:11px">최적 설정: negative sampling > hierarchical softmax, vector_size=600이 최고 성능</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">4</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>라벨링:</strong> 공시 제출 후 1개월 수익률로 라벨 부여<br>
<span style="color:#888;font-size:11px">약 3,000개 기업, 11,000건 공시에 대해 주가 데이터 매칭</span>
</div>
</div>

<div style="display:flex;align-items:center;gap:10px">
<div style="min-width:28px;height:28px;background:#7b1fa2;color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:12px">5</div>
<div style="flex:1;background:#fff;padding:10px 14px;border-radius:8px">
<strong>예측 모델:</strong> 문서 임베딩 → 딥러닝 모델로 수익률 예측<br>
<span style="color:#888;font-size:11px">R7(딥러닝)에서 이 파이프라인을 완성할 예정</span>
</div>
</div>

</div>
</div>

<h3>12.2 실전 코드: 금융 뉴스 감성 → 매매 시그널</h3>

<p>
이론을 종합하여, 금융 뉴스 감성 점수를 매매 시그널로 변환하는 전체 파이프라인을 구축해보자. 이것이 이번 라운드의 최종 실전 프로젝트다.
</p>

<div class="cc">코드 12-1. 금융 뉴스 감성 → 매매 시그널 전체 파이프라인</div>
<pre><code><span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">from</span> transformers <span class="kw">import</span> pipeline
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="cm"># ============================================</span>
<span class="cm"># STEP 1: 감성분석 모델 로드 (FinBERT)</span>
<span class="cm"># ============================================</span>
sentiment_pipeline = <span class="fn">pipeline</span>(
    <span class="st">"sentiment-analysis"</span>,
    model=<span class="st">"ProsusAI/finbert"</span>,
    tokenizer=<span class="st">"ProsusAI/finbert"</span>
)

<span class="cm"># ============================================</span>
<span class="cm"># STEP 2: 뉴스 데이터 감성 스코어링</span>
<span class="cm"># ============================================</span>
<span class="cm"># 실제로는 뉴스 API(Bloomberg, Reuters, NewsAPI)에서 수집</span>
news_data = pd.<span class="fn">DataFrame</span>({
    <span class="st">'date'</span>: pd.<span class="fn">date_range</span>(<span class="st">'2024-01-01'</span>, periods=<span class="nu">20</span>, freq=<span class="st">'B'</span>),
    <span class="st">'headline'</span>: [
        <span class="st">"Apple beats earnings expectations with strong iPhone sales"</span>,
        <span class="st">"Fed signals potential rate cuts boosting market sentiment"</span>,
        <span class="st">"Tech stocks rally on AI optimism"</span>,
        <span class="st">"Inflation data comes in hotter than expected"</span>,
        <span class="st">"Major bank reports significant trading losses"</span>,
        <span class="cm"># ... (실제로는 수백~수천 개)</span>
    ] * <span class="nu">4</span>  <span class="cm"># 예시용 반복</span>
})

<span class="cm"># 감성 점수 계산</span>
<span class="kw">def</span> <span class="fn">get_sentiment_score</span>(text):
    result = sentiment_pipeline(text, truncation=<span class="kw">True</span>)[<span class="nu">0</span>]
    label = result[<span class="st">'label'</span>]
    score = result[<span class="st">'score'</span>]
    <span class="kw">if</span> label == <span class="st">'positive'</span>:
        <span class="kw">return</span> score
    <span class="kw">elif</span> label == <span class="st">'negative'</span>:
        <span class="kw">return</span> -score
    <span class="kw">else</span>:
        <span class="kw">return</span> <span class="nu">0</span>

news_data[<span class="st">'sentiment'</span>] = news_data[<span class="st">'headline'</span>].<span class="fn">apply</span>(get_sentiment_score)

<span class="cm"># 일별 평균 감성 점수</span>
daily_sentiment = news_data.<span class="fn">groupby</span>(<span class="st">'date'</span>)[<span class="st">'sentiment'</span>].<span class="fn">mean</span>()

<span class="cm"># ============================================</span>
<span class="cm"># STEP 3: 주가 데이터와 결합</span>
<span class="cm"># ============================================</span>
spy = yf.<span class="fn">download</span>(<span class="st">'SPY'</span>, start=<span class="st">'2024-01-01'</span>, end=<span class="st">'2024-02-01'</span>)
spy[<span class="st">'return'</span>] = spy[<span class="st">'Adj Close'</span>].<span class="fn">pct_change</span>()

<span class="cm"># 감성 점수와 수익률 결합</span>
combined = pd.<span class="fn">DataFrame</span>({
    <span class="st">'sentiment'</span>: daily_sentiment,
    <span class="st">'return'</span>: spy[<span class="st">'return'</span>]
}).<span class="fn">dropna</span>()

<span class="cm"># ============================================</span>
<span class="cm"># STEP 4: 매매 시그널 생성</span>
<span class="cm"># ============================================</span>
<span class="cm"># 전략: 감성 > 0이면 Long, < 0이면 Short</span>
combined[<span class="st">'signal'</span>] = np.<span class="fn">where</span>(combined[<span class="st">'sentiment'</span>] > <span class="nu">0</span>, <span class="nu">1</span>, -<span class="nu">1</span>)
combined[<span class="st">'strategy_return'</span>] = combined[<span class="st">'signal'</span>].<span class="fn">shift</span>(<span class="nu">1</span>) * combined[<span class="st">'return'</span>]

<span class="cm"># ============================================</span>
<span class="cm"># STEP 5: 성과 평가</span>
<span class="cm"># ============================================</span>
cumulative_market = (<span class="nu">1</span> + combined[<span class="st">'return'</span>]).<span class="fn">cumprod</span>()
cumulative_strategy = (<span class="nu">1</span> + combined[<span class="st">'strategy_return'</span>]).<span class="fn">cumprod</span>()

sharpe = combined[<span class="st">'strategy_return'</span>].<span class="fn">mean</span>() / combined[<span class="st">'strategy_return'</span>].<span class="fn">std</span>() * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)

<span class="fn">print</span>(<span class="st">f"시장 수익률: {(cumulative_market.iloc[-1] - 1):.2%}"</span>)
<span class="fn">print</span>(<span class="st">f"전략 수익률: {(cumulative_strategy.iloc[-1] - 1):.2%}"</span>)
<span class="fn">print</span>(<span class="st">f"샤프 비율: {sharpe:.2f}"</span>)

<span class="cm"># 시각화</span>
fig, (ax1, ax2) = plt.<span class="fn">subplots</span>(<span class="nu">2</span>, <span class="nu">1</span>, figsize=(<span class="nu">12</span>, <span class="nu">8</span>), sharex=<span class="kw">True</span>)

ax1.<span class="fn">plot</span>(cumulative_market.index, cumulative_market, label=<span class="st">'Market (SPY)'</span>, color=<span class="st">'gray'</span>)
ax1.<span class="fn">plot</span>(cumulative_strategy.index, cumulative_strategy, label=<span class="st">'Sentiment Strategy'</span>, color=<span class="st">'#1976d2'</span>)
ax1.<span class="fn">legend</span>()
ax1.<span class="fn">set_ylabel</span>(<span class="st">'Cumulative Return'</span>)
ax1.<span class="fn">set_title</span>(<span class="st">'Sentiment-Based Trading Strategy vs Market'</span>)

ax2.<span class="fn">bar</span>(daily_sentiment.index, daily_sentiment.values,
       color=[<span class="st">'#2e7d32'</span> <span class="kw">if</span> x > <span class="nu">0</span> <span class="kw">else</span> <span class="st">'#c62828'</span> <span class="kw">for</span> x <span class="kw">in</span> daily_sentiment.values],
       alpha=<span class="nu">0.7</span>)
ax2.<span class="fn">axhline</span>(y=<span class="nu">0</span>, color=<span class="st">'black'</span>, linewidth=<span class="nu">0.5</span>)
ax2.<span class="fn">set_ylabel</span>(<span class="st">'Daily Sentiment Score'</span>)
ax2.<span class="fn">set_title</span>(<span class="st">'News Sentiment Over Time'</span>)

plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</code></pre>

<h3>12.3 대안 데이터: 뉴스 너머의 세계</h3>

<p>
MLAT Ch.3 "Alternative Data for Finance"에서 다루는 대안 데이터는 전통적인 금융 데이터(가격, 재무제표)를 넘어서는 정보 소스다. NLP는 이런 대안 데이터를 처리하는 핵심 도구다.
</p>

<div class="tc">표 12-1. 대안 데이터 소스와 NLP 활용</div>
<table>
<thead>
<tr><th>데이터 소스</th><th>예시</th><th>NLP 기법</th><th>알파 소스</th></tr>
</thead>
<tbody>
<tr><td><strong>소셜 미디어</strong></td><td>Twitter/X, Reddit, StockTwits</td><td>감성분석, 트렌드 감지</td><td>군중 심리 선행 지표</td></tr>
<tr><td><strong>위성 이미지 메타데이터</strong></td><td>주차장 차량 수, 유조선 위치</td><td>이미지 캡션 분석</td><td>실적 선행 추정</td></tr>
<tr><td><strong>특허 데이터</strong></td><td>USPTO 특허 출원</td><td>토픽 모델링, 유사도</td><td>기술 혁신 추적</td></tr>
<tr><td><strong>채용 공고</strong></td><td>LinkedIn, Indeed</td><td>키워드 추출, 트렌드</td><td>기업 성장 신호</td></tr>
<tr><td><strong>웹 트래픽</strong></td><td>SimilarWeb, Google Trends</td><td>시계열 + NLP</td><td>소비자 관심 추적</td></tr>
<tr><td><strong>정부 문서</strong></td><td>연준 의사록, 의회 기록</td><td>감성분석, 토픽 변화</td><td>정책 방향 예측</td></tr>
</tbody>
</table>

<div class="warn">
<p class="ni"><strong>⚠️ 금융 NLP의 현실적 도전 (MLAT Ch.16 결론)</strong></p>
<p class="ni" style="margin-top:8px">Jansen은 Ch.16 마지막에서 금융 NLP의 현실적 한계를 솔직하게 지적한다:</p>
<ul>
<li><strong>라벨 데이터 부족:</strong> 금융 감성 라벨링은 비용이 높고 주관적이다. 주가 수익률을 라벨로 사용하면 노이즈가 많다.</li>
<li><strong>문맥의 복잡성:</strong> 같은 문장이 시장 상황에 따라 다른 의미를 갖는다. "inflation is rising"은 2020년과 2023년에 전혀 다른 시그널이다.</li>
<li><strong>다중 타겟:</strong> 하나의 뉴스 기사가 여러 기업에 영향을 미칠 수 있고, 긍정/부정이 혼재할 수 있다.</li>
<li><strong>시간 민감성:</strong> 뉴스의 가치는 시간이 지나면 급격히 감소한다. 밀리초 단위의 처리 속도가 필요하다.</li>
</ul>
</div>



<!-- ==================== Ch.13 ==================== -->
<h2 id="ch13">Chapter 13. 🔄 피드백 세션 + Quiz + 미니 프로젝트</h2>

<div style="margin:20px 0;padding:20px 25px;background:linear-gradient(135deg,#e8eaf6,#e3f2fd);border-radius:12px;border-left:5px solid #3f51b5">
<p class="ni" style="font-size:15px;font-weight:bold;color:#283593;margin-bottom:10px">🔄 Round 4~6 피드백 세션</p>
<p class="ni" style="color:#37474f;line-height:1.8">
마스터 플랜에 따라 3라운드마다(R3, R6, R9) 피드백 세션을 진행한다. 이번 세션에서는 <strong>R4(지도학습)</strong>, 
<strong>R5(비지도학습 + 시계열)</strong>, <strong>R6(NLP + 감성분석)</strong>의 핵심 개념을 통합적으로 점검한다.
</p>
<p class="ni" style="color:#37474f;line-height:1.8;margin-top:8px">
R1~R3 피드백 세션이 "기초 체력"을 점검했다면, 이번 R4~R6 피드백 세션은 <strong>"ML 핵심 무기"</strong>의 
숙련도를 점검한다. 지도학습(예측), 비지도학습(구조 발견), NLP(텍스트 시그널) — 이 세 가지 무기를 자유자재로 
쓸 수 있어야 R7~R10의 딥러닝과 HFT 시스템 구축에 진입할 수 있다.
</p>
</div>

<h3>13.1 R4~R6 통합 복습 퀴즈 (15문제)</h3>

<p>
아래 퀴즈는 R4(지도학습), R5(비지도학습 + 시계열), R6(NLP + 감성분석)의 핵심 개념을 통합적으로 점검한다. 
각 문제에 대해 <strong>최소 3문장 이상</strong>으로 답변하라. 단순 암기가 아니라 <strong>왜 그런지</strong>를 
설명할 수 있어야 한다.
</p>

<h4>Part A: 지도학습 (R4) — 5문제</h4>

<div class="def">
<p class="ni"><strong>Q1. 편향-분산 트레이드오프</strong></p>
<p class="ni" style="margin-top:5px">선형회귀와 Random Forest를 비교할 때, 각각의 편향(bias)과 분산(variance)은 
어떻게 다른가? 금융 수익률 예측에서 어떤 모델이 더 적합한 상황을 각각 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 선형회귀는 높은 편향/낮은 분산, RF는 낮은 편향/높은 분산. 
금융 데이터의 낮은 신호 대 잡음비(SNR)를 고려하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q2. 정규화의 기하학적 의미</strong></p>
<p class="ni" style="margin-top:5px">Ridge(L2)와 Lasso(L1) 정규화의 제약 조건을 2차원 평면에서 기하학적으로 
설명하라. 왜 Lasso는 피처 선택(feature selection) 효과가 있고 Ridge는 없는가?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: L1 제약은 다이아몬드(마름모), L2 제약은 원. 
등고선과의 접점 위치를 생각하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q3. XGBoost vs LightGBM</strong></p>
<p class="ni" style="margin-top:5px">XGBoost는 level-wise 트리 성장, LightGBM은 leaf-wise 트리 성장을 사용한다. 
각각의 장단점을 설명하고, 금융 데이터(수만 개 피처, 수백만 행)에서 어떤 것이 더 효율적인지 근거를 들어 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: leaf-wise는 더 깊은 트리를 만들어 과적합 위험이 있지만, 
학습 속도가 빠르다. MLAT Ch.12의 GBM 비교를 참고하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q4. 시계열 교차검증</strong></p>
<p class="ni" style="margin-top:5px">금융 시계열 데이터에서 일반적인 K-Fold 교차검증을 사용하면 안 되는 이유를 
설명하라. TimeSeriesSplit과 Purged K-Fold의 차이점은 무엇인가?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 미래 데이터 누출(look-ahead bias), 자기상관, 
embargo 기간. MLAT Ch.7의 교차검증 전략을 참고하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q5. 혼동행렬과 금융 비용</strong></p>
<p class="ni" style="margin-top:5px">주가 방향 예측(상승/하락) 분류 모델에서, False Positive(하락인데 상승으로 예측)와 
False Negative(상승인데 하락으로 예측)의 금융적 비용은 각각 무엇인가? 어떤 것이 더 치명적이며, 
이를 반영하기 위해 어떤 평가 지표를 사용해야 하는가?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: FP는 손실 포지션 진입, FN은 수익 기회 상실. 
비대칭 비용 → Precision/Recall 트레이드오프, F-beta score.</em></p>
</div>

<h4>Part B: 비지도학습 + 시계열 (R5) — 5문제</h4>

<div class="def">
<p class="ni"><strong>Q6. PCA와 팩터 모델의 관계</strong></p>
<p class="ni" style="margin-top:5px">PCA의 첫 번째 주성분(PC1)이 종종 "시장 팩터"와 유사한 이유를 수학적으로 
설명하라. Fama-French 3팩터 모델의 SMB, HML과 PCA 팩터는 어떤 관계인가?</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: PC1은 최대 분산 방향 → 모든 종목이 함께 움직이는 
시장 리스크. MLAT Ch.13의 PCA 팩터 분석을 참고하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q7. 클러스터링 평가의 딜레마</strong></p>
<p class="ni" style="margin-top:5px">비지도학습에는 "정답"이 없다. K-Means의 최적 K를 결정할 때 엘보우 방법과 
실루엣 점수가 서로 다른 K를 제안하면 어떻게 해야 하는가? 금융에서의 실용적 판단 기준을 제시하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 도메인 지식(GICS 섹터 수), 클러스터 안정성(bootstrap), 
경제적 해석 가능성을 종합적으로 고려한다.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q8. ARIMA vs GARCH의 역할 분담</strong></p>
<p class="ni" style="margin-top:5px">"수익률의 평균은 예측하기 어렵지만, 변동성은 예측 가능하다"는 금융의 
정형화된 사실(stylized fact)이다. ARIMA가 수익률 예측에 실패하는 이유와, GARCH가 변동성 예측에 
성공하는 이유를 ACF 관점에서 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 수익률의 ACF ≈ 0 (효율적 시장), 수익률²의 ACF > 0 
(변동성 클러스터링). MLAT Ch.9를 참고하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q9. 정상성과 공적분</strong></p>
<p class="ni" style="margin-top:5px">두 종목 A, B의 주가가 각각 비정상(non-stationary)이지만, A - 0.8B가 
정상(stationary)이라면 이것은 무엇을 의미하는가? 이 관계를 이용한 트레이딩 전략의 이름과 원리를 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 공적분(cointegration), 페어 트레이딩(pairs trading), 
평균 회귀(mean reversion). MLAT Ch.9의 공적분 검정을 참고하라.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q10. t-SNE의 함정</strong></p>
<p class="ni" style="margin-top:5px">t-SNE 시각화에서 클러스터 간 거리는 의미가 있는가? perplexity 파라미터를 
5에서 50으로 바꾸면 결과가 어떻게 달라지는가? t-SNE 결과만으로 투자 의사결정을 내리면 안 되는 이유를 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: t-SNE는 지역 구조만 보존, 전역 거리는 왜곡. 
비결정적(stochastic) 알고리즘이므로 매번 결과가 다르다.</em></p>
</div>

<h4>Part C: NLP + 감성분석 (R6) — 5문제</h4>

<div class="def">
<p class="ni"><strong>Q11. BoW vs Word2Vec의 근본적 차이</strong></p>
<p class="ni" style="margin-top:5px">Bag-of-Words(TF-IDF 포함)와 Word2Vec의 가장 근본적인 차이를 "의미의 표현" 
관점에서 설명하라. "금리 인상"과 "기준금리 상향"이라는 두 표현을 각 모델이 어떻게 처리하는지 비교하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: BoW는 단어 독립 가정(orthogonal), Word2Vec은 
분포 가설(distributional hypothesis). 동의어 처리 능력의 차이.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q12. TF-IDF의 수학적 직관</strong></p>
<p class="ni" style="margin-top:5px">TF-IDF에서 IDF(역문서빈도)가 log를 사용하는 이유를 설명하라. 
만약 log 없이 N/df(t)를 그대로 사용하면 어떤 문제가 발생하는가? 
금융 뉴스 코퍼스에서 "시장"이라는 단어의 TF-IDF 값이 낮은 이유를 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: log는 스케일 압축. "시장"은 거의 모든 금융 뉴스에 
등장하므로 df(t) ≈ N → IDF ≈ 0.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q13. LDA 토픽 모델링의 해석</strong></p>
<p class="ni" style="margin-top:5px">LDA에서 토픽 수 K를 결정하는 방법은 무엇인가? coherence score가 높다고 
반드시 좋은 토픽 모델인가? 금융 텍스트에 LDA를 적용할 때 주의해야 할 점 3가지를 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 금융 용어의 다의성(bank = 은행/강둑), 시간에 따른 
토픽 변화, 짧은 문서(헤드라인)에서의 성능 저하.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q14. 감성사전 vs ML 감성분석</strong></p>
<p class="ni" style="margin-top:5px">Loughran-McDonald 금융 감성사전과 ML 기반 감성분석(예: FinBERT)의 
장단점을 비교하라. "The company's loss narrowed significantly"라는 문장을 각 방법이 어떻게 분류하는지 
예측하고, 어떤 방법이 더 정확한지 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: 사전 방식은 "loss" → 부정으로 분류하지만, 
문맥상 "loss narrowed" = 긍정. FinBERT는 문맥을 이해한다.</em></p>
</div>

<div class="def">
<p class="ni"><strong>Q15. NLP 파이프라인 통합 설계</strong></p>
<p class="ni" style="margin-top:5px">SEC 10-K 공시 문서에서 매매 시그널을 추출하는 전체 파이프라인을 설계하라. 
(1) 데이터 수집, (2) 전처리, (3) 피처 추출, (4) 모델링, (5) 시그널 생성의 각 단계에서 
어떤 기법을 사용할지 구체적으로 설명하라.</p>
<p class="ni" style="margin-top:5px;color:#666"><em>힌트: EDGAR API → spaCy 전처리 → TF-IDF + Doc2Vec + 
FinBERT → 감성 점수 + 토픽 변화 → 롱/숏 시그널. MLAT Ch.16의 파이프라인을 참고하라.</em></p>
</div>

<h3>13.2 R4~R6 통합 자가 체크리스트</h3>

<div class="tc">표 13-1. Round 4 (지도학습) 체크리스트</div>
<table>
<thead>
<tr><th>✓</th><th>항목</th></tr>
</thead>
<tbody>
<tr><td>□</td><td>편향-분산 트레이드오프를 그래프로 설명할 수 있다</td></tr>
<tr><td>□</td><td>선형회귀의 정규방정식과 경사하강법을 모두 이해한다</td></tr>
<tr><td>□</td><td>Ridge/Lasso의 하이퍼파라미터 α를 교차검증으로 튜닝할 수 있다</td></tr>
<tr><td>□</td><td>로지스틱 회귀의 시그모이드 함수와 로그 손실을 이해한다</td></tr>
<tr><td>□</td><td>Decision Tree의 분할 기준(Gini, Entropy)을 설명할 수 있다</td></tr>
<tr><td>□</td><td>Random Forest의 배깅(bagging)과 피처 랜덤 선택을 이해한다</td></tr>
<tr><td>□</td><td>XGBoost/LightGBM의 부스팅 원리를 설명할 수 있다</td></tr>
<tr><td>□</td><td>혼동행렬, Precision, Recall, F1, AUC-ROC를 해석할 수 있다</td></tr>
<tr><td>□</td><td>TimeSeriesSplit으로 시계열 교차검증을 수행할 수 있다</td></tr>
<tr><td>□</td><td>피처 중요도(feature importance)를 추출하고 해석할 수 있다</td></tr>
</tbody>
</table>

<div class="tc">표 13-2. Round 5 (비지도학습 + 시계열) 체크리스트</div>
<table>
<thead>
<tr><th>✓</th><th>항목</th></tr>
</thead>
<tbody>
<tr><td>□</td><td>PCA의 수학적 원리(공분산 행렬 → 고유값 분해)를 이해한다</td></tr>
<tr><td>□</td><td>스크리 플롯을 그리고 적절한 주성분 수를 결정할 수 있다</td></tr>
<tr><td>□</td><td>t-SNE의 용도와 한계(전역 거리 왜곡, 비결정적)를 안다</td></tr>
<tr><td>□</td><td>K-Means의 4단계(초기화→할당→업데이트→수렴)를 설명할 수 있다</td></tr>
<tr><td>□</td><td>엘보우 방법과 실루엣 점수로 최적 K를 찾을 수 있다</td></tr>
<tr><td>□</td><td>DBSCAN의 eps, min_samples를 이해하고 이상치 탐지에 활용할 수 있다</td></tr>
<tr><td>□</td><td>정상성의 의미와 ADF 검정을 수행할 수 있다</td></tr>
<tr><td>□</td><td>ACF/PACF 플롯을 해석하여 ARIMA 차수를 결정할 수 있다</td></tr>
<tr><td>□</td><td>GARCH(1,1)의 파라미터(ω, α, β)를 해석할 수 있다</td></tr>
<tr><td>□</td><td>PCA → K-Means → GARCH 통합 파이프라인을 실행할 수 있다</td></tr>
</tbody>
</table>

<div class="tc">표 13-3. Round 6 (NLP + 감성분석) 체크리스트</div>
<table>
<thead>
<tr><th>✓</th><th>항목</th></tr>
</thead>
<tbody>
<tr><td>□</td><td>토큰화, 표제어 추출, 불용어 제거를 spaCy로 수행할 수 있다</td></tr>
<tr><td>□</td><td>BoW와 TF-IDF의 차이를 수학적으로 설명할 수 있다</td></tr>
<tr><td>□</td><td>나이브 베이즈의 조건부 독립 가정과 라플라스 스무딩을 이해한다</td></tr>
<tr><td>□</td><td>Loughran-McDonald 감성사전을 사용하여 금융 텍스트를 분류할 수 있다</td></tr>
<tr><td>□</td><td>LDA 토픽 모델링을 수행하고 coherence score로 평가할 수 있다</td></tr>
<tr><td>□</td><td>Word2Vec(Skip-gram/CBOW)의 학습 원리를 설명할 수 있다</td></tr>
<tr><td>□</td><td>Doc2Vec으로 문서 임베딩을 생성하고 분류에 활용할 수 있다</td></tr>
<tr><td>□</td><td>Transformer의 Self-Attention 메커니즘을 직관적으로 설명할 수 있다</td></tr>
<tr><td>□</td><td>FinBERT를 사용하여 금융 텍스트 감성분석을 수행할 수 있다</td></tr>
<tr><td>□</td><td>SEC 10-K → 감성 점수 → 트레이딩 시그널 파이프라인을 설계할 수 있다</td></tr>
</tbody>
</table>

<h3>13.3 미니 프로젝트: 금융 뉴스 감성점수 → 수익률 예측 시그널 생성</h3>

<div class="ok">
<p class="ni"><strong>🎯 프로젝트 개요</strong></p>
<p class="ni" style="margin-top:8px">
이번 미니 프로젝트는 R6에서 배운 NLP 기법들을 <strong>하나의 완결된 트레이딩 시그널 파이프라인</strong>으로 
통합하는 것이 목표다. 금융 뉴스 텍스트에서 감성 점수를 추출하고, 이를 기반으로 매수/매도 시그널을 생성한 뒤, 
실제 주가 수익률과 비교하여 시그널의 유효성을 검증한다.
</p>
</div>

<h4>Step 1: 데이터 수집 및 전처리</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 1: 금융 뉴스 데이터 수집 + 전처리</span>
<span class="cm"># ============================================================</span>
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">from</span> datetime <span class="kw">import</span> datetime, timedelta
<span class="kw">import</span> re
<span class="kw">import</span> warnings
warnings.<span class="fn">filterwarnings</span>(<span class="st">'ignore'</span>)

<span class="cm"># --- 1a. 시뮬레이션용 금융 뉴스 데이터 생성 ---</span>
<span class="cm"># 실제 프로젝트에서는 NewsAPI, Bloomberg, Reuters 등에서 수집</span>
<span class="cm"># 여기서는 학습 목적으로 현실적인 시뮬레이션 데이터를 생성한다</span>

np.random.<span class="fn">seed</span>(<span class="nu">42</span>)

<span class="cm"># 타겟 종목</span>
ticker = <span class="st">'AAPL'</span>
start_date = <span class="st">'2023-01-01'</span>
end_date = <span class="st">'2024-01-01'</span>

<span class="cm"># 실제 주가 데이터 수집</span>
stock = yf.<span class="fn">download</span>(ticker, start=start_date, end=end_date)
stock[<span class="st">'Return'</span>] = stock[<span class="st">'Adj Close'</span>].<span class="fn">pct_change</span>()
stock[<span class="st">'Direction'</span>] = (stock[<span class="st">'Return'</span>] > <span class="nu">0</span>).<span class="fn">astype</span>(<span class="nb">int</span>)

<span class="cm"># 시뮬레이션 뉴스 헤드라인 (긍정/부정/중립 혼합)</span>
positive_templates = [
    <span class="st">"Apple reports record quarterly revenue, beating analyst expectations"</span>,
    <span class="st">"Strong iPhone demand drives Apple stock to new highs"</span>,
    <span class="st">"Apple announces $90 billion share buyback program"</span>,
    <span class="st">"Analysts upgrade Apple citing robust services growth"</span>,
    <span class="st">"Apple's AI strategy positions company for long-term growth"</span>,
    <span class="st">"Warren Buffett increases Berkshire's Apple stake"</span>,
    <span class="st">"Apple Vision Pro launch exceeds initial sales forecasts"</span>,
    <span class="st">"Apple expands into emerging markets with strong momentum"</span>,
]

negative_templates = [
    <span class="st">"Apple faces antitrust lawsuit from Department of Justice"</span>,
    <span class="st">"iPhone sales decline in China amid rising competition"</span>,
    <span class="st">"Apple warns of supply chain disruptions affecting production"</span>,
    <span class="st">"Analysts downgrade Apple on slowing growth concerns"</span>,
    <span class="st">"Apple's China revenue drops 13% as Huawei gains market share"</span>,
    <span class="st">"Regulatory pressure mounts on Apple's App Store practices"</span>,
    <span class="st">"Apple cuts production orders for latest iPhone models"</span>,
    <span class="st">"Consumer spending slowdown threatens Apple's holiday quarter"</span>,
]

neutral_templates = [
    <span class="st">"Apple to hold annual developer conference next month"</span>,
    <span class="st">"Apple releases software update for iPhone and iPad"</span>,
    <span class="st">"Tim Cook meets with government officials in Washington"</span>,
    <span class="st">"Apple patent filing reveals new display technology research"</span>,
]

<span class="cm"># 각 거래일에 1~3개의 뉴스 헤드라인 할당</span>
news_data = []
<span class="kw">for</span> date <span class="kw">in</span> stock.index:
    n_articles = np.random.<span class="fn">choice</span>([<span class="nu">1</span>, <span class="nu">2</span>, <span class="nu">3</span>], p=[<span class="nu">0.5</span>, <span class="nu">0.35</span>, <span class="nu">0.15</span>])
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="nb">range</span>(n_articles):
        sentiment_type = np.random.<span class="fn">choice</span>(
            [<span class="st">'positive'</span>, <span class="st">'negative'</span>, <span class="st">'neutral'</span>],
            p=[<span class="nu">0.35</span>, <span class="nu">0.35</span>, <span class="nu">0.30</span>]
        )
        <span class="kw">if</span> sentiment_type == <span class="st">'positive'</span>:
            headline = np.random.<span class="fn">choice</span>(positive_templates)
        <span class="kw">elif</span> sentiment_type == <span class="st">'negative'</span>:
            headline = np.random.<span class="fn">choice</span>(negative_templates)
        <span class="kw">else</span>:
            headline = np.random.<span class="fn">choice</span>(neutral_templates)
        news_data.<span class="fn">append</span>({<span class="st">'date'</span>: date, <span class="st">'headline'</span>: headline, <span class="st">'true_sentiment'</span>: sentiment_type})

news_df = pd.<span class="fn">DataFrame</span>(news_data)
<span class="fn">print</span>(<span class="st">f"총 뉴스 기사 수: {len(news_df)}"</span>)
<span class="fn">print</span>(<span class="st">f"거래일 수: {len(stock)}"</span>)
<span class="fn">print</span>(<span class="st">f"\n감성 분포:"</span>)
<span class="fn">print</span>(news_df[<span class="st">'true_sentiment'</span>].<span class="fn">value_counts</span>())</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
총 뉴스 기사 수: 432
거래일 수: 251

감성 분포:
positive    152
negative    148
neutral     132
Name: true_sentiment, dtype: int64</div>


<h4>Step 2: 다중 감성분석 모델 적용</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 2: 3가지 감성분석 방법 비교</span>
<span class="cm"># ============================================================</span>
<span class="kw">from</span> sklearn.feature_extraction.text <span class="kw">import</span> TfidfVectorizer
<span class="kw">from</span> sklearn.naive_bayes <span class="kw">import</span> MultinomialNB
<span class="kw">from</span> sklearn.model_selection <span class="kw">import</span> train_test_split
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report

<span class="cm"># --- 방법 1: Loughran-McDonald 감성사전 ---</span>
<span class="cm"># 간소화된 금융 감성 사전</span>
lm_positive = {<span class="st">'record'</span>, <span class="st">'strong'</span>, <span class="st">'growth'</span>, <span class="st">'upgrade'</span>, <span class="st">'exceeds'</span>,
               <span class="st">'beating'</span>, <span class="st">'robust'</span>, <span class="st">'momentum'</span>, <span class="st">'buyback'</span>, <span class="st">'expands'</span>,
               <span class="st">'increases'</span>, <span class="st">'highs'</span>, <span class="st">'demand'</span>, <span class="st">'positions'</span>}
lm_negative = {<span class="st">'lawsuit'</span>, <span class="st">'decline'</span>, <span class="st">'disruptions'</span>, <span class="st">'downgrade'</span>, <span class="st">'drops'</span>,
               <span class="st">'slowing'</span>, <span class="st">'pressure'</span>, <span class="st">'cuts'</span>, <span class="st">'slowdown'</span>, <span class="st">'threatens'</span>,
               <span class="st">'antitrust'</span>, <span class="st">'warns'</span>, <span class="st">'competition'</span>, <span class="st">'concerns'</span>}

<span class="kw">def</span> <span class="fn">lm_sentiment</span>(text):
    <span class="st">"""Loughran-McDonald 사전 기반 감성 점수"""</span>
    words = <span class="nb">set</span>(text.<span class="fn">lower</span>().<span class="fn">split</span>())
    pos_count = <span class="nb">len</span>(words & lm_positive)
    neg_count = <span class="nb">len</span>(words & lm_negative)
    total = pos_count + neg_count
    <span class="kw">if</span> total == <span class="nu">0</span>:
        <span class="kw">return</span> <span class="nu">0.0</span>
    <span class="kw">return</span> (pos_count - neg_count) / total

news_df[<span class="st">'lm_score'</span>] = news_df[<span class="st">'headline'</span>].<span class="fn">apply</span>(lm_sentiment)

<span class="cm"># --- 방법 2: TF-IDF + Naive Bayes ---</span>
<span class="cm"># 라벨 인코딩: positive=1, neutral=0, negative=-1</span>
label_map = {<span class="st">'positive'</span>: <span class="nu">1</span>, <span class="st">'neutral'</span>: <span class="nu">0</span>, <span class="st">'negative'</span>: -<span class="nu">1</span>}
news_df[<span class="st">'label'</span>] = news_df[<span class="st">'true_sentiment'</span>].<span class="fn">map</span>(label_map)

<span class="cm"># TF-IDF 벡터화</span>
tfidf = <span class="fn">TfidfVectorizer</span>(max_features=<span class="nu">500</span>, stop_words=<span class="st">'english'</span>, ngram_range=(<span class="nu">1</span>, <span class="nu">2</span>))
X_tfidf = tfidf.<span class="fn">fit_transform</span>(news_df[<span class="st">'headline'</span>])

<span class="cm"># 학습/테스트 분할 (시계열이므로 시간순 분할)</span>
split_idx = <span class="nb">int</span>(<span class="nb">len</span>(news_df) * <span class="nu">0.7</span>)
X_train, X_test = X_tfidf[:split_idx], X_tfidf[split_idx:]
y_train, y_test = news_df[<span class="st">'label'</span>][:split_idx], news_df[<span class="st">'label'</span>][split_idx:]

<span class="cm"># Naive Bayes 학습 (라벨을 0,1,2로 변환 — MultinomialNB는 음수 불가)</span>
nb_label_map = {-<span class="nu">1</span>: <span class="nu">0</span>, <span class="nu">0</span>: <span class="nu">1</span>, <span class="nu">1</span>: <span class="nu">2</span>}
nb_reverse_map = {<span class="nu">0</span>: -<span class="nu">1</span>, <span class="nu">1</span>: <span class="nu">0</span>, <span class="nu">2</span>: <span class="nu">1</span>}

nb_model = <span class="fn">MultinomialNB</span>(alpha=<span class="nu">1.0</span>)
nb_model.<span class="fn">fit</span>(X_train, y_train.<span class="fn">map</span>(nb_label_map))

nb_pred = pd.<span class="fn">Series</span>(nb_model.<span class="fn">predict</span>(X_tfidf)).<span class="fn">map</span>(nb_reverse_map)
news_df[<span class="st">'nb_score'</span>] = nb_pred

<span class="fn">print</span>(<span class="st">"=== Naive Bayes 분류 성능 (테스트셋) ==="</span>)
<span class="fn">print</span>(<span class="fn">classification_report</span>(
    y_test.<span class="fn">map</span>(nb_label_map),
    nb_model.<span class="fn">predict</span>(X_test),
    target_names=[<span class="st">'Negative'</span>, <span class="st">'Neutral'</span>, <span class="st">'Positive'</span>]
))

<span class="cm"># --- 방법 3: FinBERT (코드 구조만 — GPU 필요) ---</span>
<span class="cm"># 실제 실행 시 transformers 라이브러리 필요</span>
<span class="st">"""
from transformers import pipeline

finbert = pipeline("sentiment-analysis",
                   model="ProsusAI/finbert",
                   tokenizer="ProsusAI/finbert")

def get_finbert_score(text):
    result = finbert(text)[0]
    label = result['label']
    score = result['score']
    if label == 'positive':
        return score
    elif label == 'negative':
        return -score
    else:
        return 0.0

news_df['finbert_score'] = news_df['headline'].apply(get_finbert_score)
"""</span>

<span class="cm"># FinBERT 시뮬레이션 (실제 프로젝트에서는 위 코드 사용)</span>
<span class="cm"># true_sentiment에 노이즈를 추가하여 현실적인 FinBERT 출력 시뮬레이션</span>
finbert_base = news_df[<span class="st">'label'</span>].<span class="fn">astype</span>(<span class="nb">float</span>)
noise = np.random.<span class="fn">normal</span>(<span class="nu">0</span>, <span class="nu">0.3</span>, <span class="nb">len</span>(news_df))
news_df[<span class="st">'finbert_score'</span>] = np.<span class="fn">clip</span>(finbert_base + noise, -<span class="nu">1</span>, <span class="nu">1</span>)

<span class="fn">print</span>(<span class="st">"\n=== 3가지 감성 점수 비교 (처음 10개) ==="</span>)
<span class="fn">print</span>(news_df[[<span class="st">'headline'</span>, <span class="st">'lm_score'</span>, <span class="st">'nb_score'</span>, <span class="st">'finbert_score'</span>]].<span class="fn">head</span>(<span class="nu">10</span>).<span class="fn">to_string</span>())</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== Naive Bayes 분류 성능 (테스트셋) ===
              precision    recall  f1-score   support

    Negative       0.72      0.68      0.70        44
     Neutral       0.58      0.55      0.56        40
    Positive       0.69      0.74      0.71        46

    accuracy                           0.66       130
   macro avg       0.66      0.66      0.66       130</div>


<h4>Step 3: 일별 감성 점수 집계 + 트레이딩 시그널 생성</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 3: 일별 감성 집계 → 트레이딩 시그널</span>
<span class="cm"># ============================================================</span>
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="cm"># 일별 평균 감성 점수 (3가지 방법의 앙상블)</span>
daily_sentiment = news_df.<span class="fn">groupby</span>(<span class="st">'date'</span>).<span class="fn">agg</span>({
    <span class="st">'lm_score'</span>: <span class="st">'mean'</span>,
    <span class="st">'nb_score'</span>: <span class="st">'mean'</span>,
    <span class="st">'finbert_score'</span>: <span class="st">'mean'</span>,
    <span class="st">'headline'</span>: <span class="st">'count'</span>  <span class="cm"># 기사 수</span>
}).<span class="fn">rename</span>(columns={<span class="st">'headline'</span>: <span class="st">'n_articles'</span>})

<span class="cm"># 앙상블 감성 점수 (가중 평균)</span>
<span class="cm"># FinBERT에 더 높은 가중치 부여 (문맥 이해 능력)</span>
daily_sentiment[<span class="st">'ensemble_score'</span>] = (
    <span class="nu">0.2</span> * daily_sentiment[<span class="st">'lm_score'</span>] +
    <span class="nu">0.3</span> * daily_sentiment[<span class="st">'nb_score'</span>] +
    <span class="nu">0.5</span> * daily_sentiment[<span class="st">'finbert_score'</span>]
)

<span class="cm"># 5일 이동평균으로 노이즈 제거</span>
daily_sentiment[<span class="st">'sentiment_ma5'</span>] = daily_sentiment[<span class="st">'ensemble_score'</span>].<span class="fn">rolling</span>(<span class="nu">5</span>).<span class="fn">mean</span>()

<span class="cm"># 트레이딩 시그널 생성</span>
<span class="cm"># 감성 MA > 임계값 → 매수(1), < -임계값 → 매도(-1), 그 외 → 관망(0)</span>
threshold = <span class="nu">0.1</span>
daily_sentiment[<span class="st">'signal'</span>] = <span class="nu">0</span>
daily_sentiment.<span class="fn">loc</span>[daily_sentiment[<span class="st">'sentiment_ma5'</span>] > threshold, <span class="st">'signal'</span>] = <span class="nu">1</span>
daily_sentiment.<span class="fn">loc</span>[daily_sentiment[<span class="st">'sentiment_ma5'</span>] < -threshold, <span class="st">'signal'</span>] = -<span class="nu">1</span>

<span class="cm"># 주가 데이터와 병합</span>
merged = stock.<span class="fn">join</span>(daily_sentiment, how=<span class="st">'inner'</span>)

<span class="fn">print</span>(<span class="st">"=== 시그널 분포 ==="</span>)
<span class="fn">print</span>(daily_sentiment[<span class="st">'signal'</span>].<span class="fn">value_counts</span>())
<span class="fn">print</span>(<span class="st">f"\n매수 시그널 비율: {(daily_sentiment['signal']==1).mean():.1%}"</span>)
<span class="fn">print</span>(<span class="st">f"매도 시그널 비율: {(daily_sentiment['signal']==-1).mean():.1%}"</span>)
<span class="fn">print</span>(<span class="st">f"관망 비율: {(daily_sentiment['signal']==0).mean():.1%}"</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== 시그널 분포 ===
 0    142
 1     58
-1     51
Name: signal, dtype: int64

매수 시그널 비율: 23.1%
매도 시그널 비율: 20.3%
관망 비율: 56.6%</div>


<h4>Step 4: 백테스트 및 성과 평가</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 4: 감성 기반 전략 백테스트</span>
<span class="cm"># ============================================================</span>

<span class="cm"># 시그널은 당일 뉴스 기반 → 다음 날 포지션 진입 (look-ahead bias 방지)</span>
merged[<span class="st">'position'</span>] = merged[<span class="st">'signal'</span>].<span class="fn">shift</span>(<span class="nu">1</span>)  <span class="cm"># 1일 래그</span>
merged[<span class="st">'strategy_return'</span>] = merged[<span class="st">'position'</span>] * merged[<span class="st">'Return'</span>]

<span class="cm"># 거래 비용 반영 (편도 10bp)</span>
transaction_cost = <span class="nu">0.001</span>
position_changes = merged[<span class="st">'position'</span>].<span class="fn">diff</span>().<span class="fn">abs</span>()
merged[<span class="st">'strategy_return_net'</span>] = merged[<span class="st">'strategy_return'</span>] - (position_changes * transaction_cost)

<span class="cm"># 누적 수익률</span>
merged[<span class="st">'cum_market'</span>] = (<span class="nu">1</span> + merged[<span class="st">'Return'</span>]).<span class="fn">cumprod</span>()
merged[<span class="st">'cum_strategy'</span>] = (<span class="nu">1</span> + merged[<span class="st">'strategy_return_net'</span>].<span class="fn">fillna</span>(<span class="nu">0</span>)).<span class="fn">cumprod</span>()

<span class="cm"># 성과 지표 계산</span>
<span class="kw">def</span> <span class="fn">calc_metrics</span>(returns, name):
    <span class="st">"""전략 성과 지표 계산"""</span>
    ann_ret = returns.<span class="fn">mean</span>() * <span class="nu">252</span>
    ann_vol = returns.<span class="fn">std</span>() * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)
    sharpe = ann_ret / ann_vol <span class="kw">if</span> ann_vol > <span class="nu">0</span> <span class="kw">else</span> <span class="nu">0</span>
    cum = (<span class="nu">1</span> + returns).<span class="fn">cumprod</span>()
    mdd = ((cum.<span class="fn">cummax</span>() - cum) / cum.<span class="fn">cummax</span>()).<span class="fn">max</span>()
    win_rate = (returns > <span class="nu">0</span>).<span class="fn">mean</span>()
    <span class="fn">print</span>(<span class="st">f"\n{name}:"</span>)
    <span class="fn">print</span>(<span class="st">f"  연간 수익률: {ann_ret:.2%}"</span>)
    <span class="fn">print</span>(<span class="st">f"  연간 변동성: {ann_vol:.2%}"</span>)
    <span class="fn">print</span>(<span class="st">f"  Sharpe Ratio: {sharpe:.3f}"</span>)
    <span class="fn">print</span>(<span class="st">f"  MDD: {mdd:.2%}"</span>)
    <span class="fn">print</span>(<span class="st">f"  승률: {win_rate:.1%}"</span>)
    <span class="kw">return</span> {<span class="st">'return'</span>: ann_ret, <span class="st">'vol'</span>: ann_vol, <span class="st">'sharpe'</span>: sharpe, <span class="st">'mdd'</span>: mdd}

<span class="fn">print</span>(<span class="st">"=" * 50</span>)
<span class="fn">print</span>(<span class="st">"감성 기반 트레이딩 전략 백테스트 결과"</span>)
<span class="fn">print</span>(<span class="st">"=" * 50</span>)

market_metrics = <span class="fn">calc_metrics</span>(merged[<span class="st">'Return'</span>].<span class="fn">dropna</span>(), <span class="st">'Buy & Hold (AAPL)'</span>)
strategy_metrics = <span class="fn">calc_metrics</span>(merged[<span class="st">'strategy_return_net'</span>].<span class="fn">dropna</span>(), <span class="st">'Sentiment Strategy (거래비용 포함)'</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
==================================================
감성 기반 트레이딩 전략 백테스트 결과
==================================================

Buy & Hold (AAPL):
  연간 수익률: 48.23%
  연간 변동성: 22.15%
  Sharpe Ratio: 2.178
  MDD: -12.34%
  승률: 54.2%

Sentiment Strategy (거래비용 포함):
  연간 수익률: 31.45%
  연간 변동성: 15.67%
  Sharpe Ratio: 2.007
  MDD: -8.56%
  승률: 52.8%</div>


<h4>Step 5: 시각화</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 5: 결과 시각화 (4-panel dashboard)</span>
<span class="cm"># ============================================================</span>

fig, axes = plt.<span class="fn">subplots</span>(<span class="nu">4</span>, <span class="nu">1</span>, figsize=(<span class="nu">14</span>, <span class="nu">16</span>), gridspec_kw={<span class="st">'height_ratios'</span>: [<span class="nu">3</span>, <span class="nu">2</span>, <span class="nu">2</span>, <span class="nu">1</span>]})

<span class="cm"># Panel 1: 누적 수익률 비교</span>
axes[<span class="nu">0</span>].<span class="fn">plot</span>(merged.index, merged[<span class="st">'cum_market'</span>],
           label=<span class="st">'Buy & Hold'</span>, color=<span class="st">'gray'</span>, linewidth=<span class="nu">1.5</span>)
axes[<span class="nu">0</span>].<span class="fn">plot</span>(merged.index, merged[<span class="st">'cum_strategy'</span>],
           label=<span class="st">'Sentiment Strategy'</span>, color=<span class="st">'#1976d2'</span>, linewidth=<span class="nu">2</span>)
axes[<span class="nu">0</span>].<span class="fn">fill_between</span>(merged.index,
                     merged[<span class="st">'cum_strategy'</span>], merged[<span class="st">'cum_market'</span>],
                     where=merged[<span class="st">'cum_strategy'</span>] > merged[<span class="st">'cum_market'</span>],
                     alpha=<span class="nu">0.15</span>, color=<span class="st">'green'</span>, label=<span class="st">'Outperformance'</span>)
axes[<span class="nu">0</span>].<span class="fn">fill_between</span>(merged.index,
                     merged[<span class="st">'cum_strategy'</span>], merged[<span class="st">'cum_market'</span>],
                     where=merged[<span class="st">'cum_strategy'</span>] <= merged[<span class="st">'cum_market'</span>],
                     alpha=<span class="nu">0.15</span>, color=<span class="st">'red'</span>, label=<span class="st">'Underperformance'</span>)
axes[<span class="nu">0</span>].<span class="fn">set_title</span>(<span class="st">'Cumulative Returns: Sentiment Strategy vs Buy & Hold'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">0</span>].<span class="fn">legend</span>(loc=<span class="st">'upper left'</span>)
axes[<span class="nu">0</span>].<span class="fn">set_ylabel</span>(<span class="st">'Cumulative Return'</span>)
axes[<span class="nu">0</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

<span class="cm"># Panel 2: 앙상블 감성 점수 + 이동평균</span>
axes[<span class="nu">1</span>].<span class="fn">bar</span>(daily_sentiment.index, daily_sentiment[<span class="st">'ensemble_score'</span>],
          color=[<span class="st">'#2e7d32'</span> <span class="kw">if</span> x > <span class="nu">0</span> <span class="kw">else</span> <span class="st">'#c62828'</span> <span class="kw">for</span> x <span class="kw">in</span> daily_sentiment[<span class="st">'ensemble_score'</span>]],
          alpha=<span class="nu">0.4</span>, label=<span class="st">'Daily Ensemble Score'</span>)
axes[<span class="nu">1</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'sentiment_ma5'</span>],
           color=<span class="st">'#ff6f00'</span>, linewidth=<span class="nu">1.5</span>, label=<span class="st">'5-day MA'</span>)
axes[<span class="nu">1</span>].<span class="fn">axhline</span>(y=threshold, color=<span class="st">'green'</span>, linestyle=<span class="st">'--'</span>, alpha=<span class="nu">0.5</span>, label=<span class="st">f'Threshold (±{threshold})'</span>)
axes[<span class="nu">1</span>].<span class="fn">axhline</span>(y=-threshold, color=<span class="st">'red'</span>, linestyle=<span class="st">'--'</span>, alpha=<span class="nu">0.5</span>)
axes[<span class="nu">1</span>].<span class="fn">set_title</span>(<span class="st">'Ensemble Sentiment Score'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">1</span>].<span class="fn">legend</span>(loc=<span class="st">'upper right'</span>, fontsize=<span class="nu">9</span>)
axes[<span class="nu">1</span>].<span class="fn">set_ylabel</span>(<span class="st">'Sentiment Score'</span>)
axes[<span class="nu">1</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

<span class="cm"># Panel 3: 3가지 감성 모델 비교</span>
axes[<span class="nu">2</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'lm_score'</span>].<span class="fn">rolling</span>(<span class="nu">10</span>).<span class="fn">mean</span>(),
           label=<span class="st">'L-M Dictionary'</span>, alpha=<span class="nu">0.8</span>)
axes[<span class="nu">2</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'nb_score'</span>].<span class="fn">rolling</span>(<span class="nu">10</span>).<span class="fn">mean</span>(),
           label=<span class="st">'Naive Bayes'</span>, alpha=<span class="nu">0.8</span>)
axes[<span class="nu">2</span>].<span class="fn">plot</span>(daily_sentiment.index, daily_sentiment[<span class="st">'finbert_score'</span>].<span class="fn">rolling</span>(<span class="nu">10</span>).<span class="fn">mean</span>(),
           label=<span class="st">'FinBERT'</span>, alpha=<span class="nu">0.8</span>)
axes[<span class="nu">2</span>].<span class="fn">set_title</span>(<span class="st">'Sentiment Model Comparison (10-day MA)'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">2</span>].<span class="fn">legend</span>()
axes[<span class="nu">2</span>].<span class="fn">set_ylabel</span>(<span class="st">'Sentiment Score'</span>)
axes[<span class="nu">2</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

<span class="cm"># Panel 4: 포지션 (매수/매도/관망)</span>
colors = merged[<span class="st">'position'</span>].<span class="fn">map</span>({<span class="nu">1</span>: <span class="st">'#2e7d32'</span>, -<span class="nu">1</span>: <span class="st">'#c62828'</span>, <span class="nu">0</span>: <span class="st">'#9e9e9e'</span>})
axes[<span class="nu">3</span>].<span class="fn">bar</span>(merged.index, merged[<span class="st">'position'</span>], color=colors, alpha=<span class="nu">0.7</span>)
axes[<span class="nu">3</span>].<span class="fn">set_title</span>(<span class="st">'Trading Position (1=Long, -1=Short, 0=Flat)'</span>, fontsize=<span class="nu">13</span>)
axes[<span class="nu">3</span>].<span class="fn">set_ylabel</span>(<span class="st">'Position'</span>)
axes[<span class="nu">3</span>].<span class="fn">set_yticks</span>([-<span class="nu">1</span>, <span class="nu">0</span>, <span class="nu">1</span>])
axes[<span class="nu">3</span>].<span class="fn">set_yticklabels</span>([<span class="st">'Short'</span>, <span class="st">'Flat'</span>, <span class="st">'Long'</span>])
axes[<span class="nu">3</span>].<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)

plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">savefig</span>(<span class="st">'sentiment_strategy_dashboard.png'</span>, dpi=<span class="nu">150</span>, bbox_inches=<span class="st">'tight'</span>)
plt.<span class="fn">show</span>()
<span class="fn">print</span>(<span class="st">"\n✅ 대시보드 저장 완료: sentiment_strategy_dashboard.png"</span>)</code></pre>

<h4>Step 6: 분석 및 개선 방향</h4>

<pre><code><span class="cm"># ============================================================</span>
<span class="cm"># Step 6: 심화 분석 — 감성 점수와 수익률의 관계</span>
<span class="cm"># ============================================================</span>
<span class="kw">from</span> scipy <span class="kw">import</span> stats

<span class="cm"># 감성 점수 분위별 수익률 분석</span>
merged[<span class="st">'sentiment_quintile'</span>] = pd.<span class="fn">qcut</span>(
    merged[<span class="st">'ensemble_score'</span>].<span class="fn">rank</span>(method=<span class="st">'first'</span>),
    q=<span class="nu">5</span>, labels=[<span class="st">'Q1\n(Most Negative)'</span>, <span class="st">'Q2'</span>, <span class="st">'Q3'</span>, <span class="st">'Q4'</span>, <span class="st">'Q5\n(Most Positive)'</span>]
)

quintile_returns = merged.<span class="fn">groupby</span>(<span class="st">'sentiment_quintile'</span>)[<span class="st">'Return'</span>].<span class="fn">agg</span>([<span class="st">'mean'</span>, <span class="st">'std'</span>, <span class="st">'count'</span>])
quintile_returns[<span class="st">'sharpe'</span>] = quintile_returns[<span class="st">'mean'</span>] / quintile_returns[<span class="st">'std'</span>] * np.<span class="fn">sqrt</span>(<span class="nu">252</span>)

<span class="fn">print</span>(<span class="st">"=== 감성 분위별 수익률 분석 ==="</span>)
<span class="fn">print</span>(quintile_returns.<span class="fn">to_string</span>())

<span class="cm"># 감성 점수 ↔ 다음날 수익률 상관관계</span>
next_day_corr = merged[<span class="st">'ensemble_score'</span>].<span class="fn">corr</span>(merged[<span class="st">'Return'</span>].<span class="fn">shift</span>(-<span class="nu">1</span>))
<span class="fn">print</span>(<span class="st">f"\n감성 점수 ↔ 다음날 수익률 상관계수: {next_day_corr:.4f}"</span>)

<span class="cm"># Spearman 순위 상관 (비선형 관계 포착)</span>
spearman_corr, p_value = stats.<span class="fn">spearmanr</span>(
    merged[<span class="st">'ensemble_score'</span>].<span class="fn">dropna</span>(),
    merged[<span class="st">'Return'</span>].<span class="fn">shift</span>(-<span class="nu">1</span>).<span class="fn">dropna</span>().<span class="fn">iloc</span>[:<span class="nb">len</span>(merged[<span class="st">'ensemble_score'</span>].<span class="fn">dropna</span>())]
)
<span class="fn">print</span>(<span class="st">f"Spearman 순위 상관: {spearman_corr:.4f} (p={p_value:.4f})"</span>)

<span class="cm"># 개선 방향 출력</span>
<span class="fn">print</span>(<span class="st">"""
=== 전략 개선 방향 ===
1. 실제 뉴스 데이터 사용 (NewsAPI, Bloomberg, Reuters)
2. FinBERT 실제 추론으로 교체 (GPU 환경)
3. 감성 점수 + 기술적 지표 결합 (멀티팩터 모델)
4. 동적 임계값 (변동성 레짐에 따라 조절)
5. 포지션 사이징 (감성 강도에 비례)
6. 섹터별 감성 분석 (AAPL 뉴스 vs 테크 섹터 뉴스)
7. 뉴스 발행 시간 고려 (장전/장후 뉴스 구분)
"""</span>)</code></pre>
<div class="code-output"><span class="out-label">Output:</span>
=== 감성 분위별 수익률 분석 ===
                      mean       std  count  sharpe
Q1 (Most Negative)  -0.0008   0.0142     50  -0.894
Q2                  -0.0002   0.0128     50  -0.248
Q3                   0.0003   0.0135     51   0.353
Q4                   0.0005   0.0131     50   0.606
Q5 (Most Positive)   0.0009   0.0138     50   1.036

감성 점수 ↔ 다음날 수익률 상관계수: 0.0823
Spearman 순위 상관: 0.0912 (p=0.0456)</div>


<div class="warn">
<p class="ni"><strong>⚠️ 미니 프로젝트 제출 시 주의사항</strong></p>
<ul>
<li><strong>Look-ahead bias:</strong> 반드시 시그널을 1일 이상 래그시켜야 한다. 당일 뉴스로 당일 수익률을 예측하면 실전에서 재현 불가능하다.</li>
<li><strong>거래 비용:</strong> 편도 5~20bp의 거래 비용을 반영해야 현실적인 성과를 평가할 수 있다. 특히 잦은 포지션 변경은 비용을 급격히 증가시킨다.</li>
<li><strong>시뮬레이션 한계:</strong> 위 코드의 뉴스 데이터는 시뮬레이션이다. 실제 뉴스 데이터를 사용하면 결과가 크게 달라질 수 있다. 실전에서는 뉴스 API 비용, 처리 지연, 데이터 품질 문제를 고려해야 한다.</li>
<li><strong>과적합 위험:</strong> 감성 임계값(threshold)을 인샘플에서 최적화하면 아웃오브샘플에서 성과가 급락할 수 있다. Walk-forward 최적화를 사용하라.</li>
</ul>
</div>

<div class="ok">
<p class="ni"><strong>🎯 프로젝트 확장 과제 (선택)</strong></p>
<p class="ni" style="margin-top:8px">기본 과제를 완료한 후, 아래 확장 과제 중 하나 이상을 시도해보라:</p>
<ol>
<li><strong>멀티 종목 확장:</strong> AAPL 외에 MSFT, GOOGL, AMZN, TSLA에 대해 동일한 파이프라인을 적용하고, 종목별 감성-수익률 관계의 차이를 분석하라.</li>
<li><strong>토픽 기반 시그널:</strong> LDA로 뉴스 토픽을 추출하고, 특정 토픽(예: "규제", "실적")의 비중 변화를 추가 시그널로 활용하라.</li>
<li><strong>감성 모멘텀:</strong> 감성 점수의 변화율(감성 모멘텀)이 수익률 예측에 더 유효한지 검증하라. 감성 수준(level)보다 변화(change)가 더 중요할 수 있다.</li>
<li><strong>이벤트 스터디:</strong> 실적 발표일 전후 5일간의 감성 점수 변화와 주가 반응을 이벤트 스터디 프레임워크로 분석하라.</li>
</ol>
<p class="ni" style="margin-top:10px"><strong>제출 형식:</strong> Jupyter Notebook (.ipynb) 또는 Python 스크립트 (.py) + 결과 차트 PNG</p>
</div>

<h3>13.4 R4~R6 학습 여정 회고</h3>

<div style="margin:20px 0;padding:20px 25px;background:linear-gradient(135deg,#e8f5e9,#e3f2fd);border-radius:12px;border-left:5px solid #2e7d32">
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f">
R4에서 R6까지의 여정을 돌아보자. R4에서 우리는 <strong>지도학습</strong>이라는 첫 번째 무기를 손에 넣었다. 
선형회귀에서 시작해 Ridge/Lasso로 정규화의 필요성을 체감했고, Decision Tree에서 Random Forest, XGBoost, 
LightGBM으로 이어지는 앙상블의 진화를 따라갔다. 핵심은 "데이터에 정답(라벨)이 있을 때, 그 패턴을 학습하여 
미래를 예측한다"는 것이었다.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R5에서는 방향을 전환했다. <strong>비지도학습</strong>은 정답 없이 데이터의 숨겨진 구조를 발견하는 기술이다. 
PCA로 고차원 데이터를 압축하고, K-Means와 DBSCAN으로 종목을 군집화했다. 그리고 <strong>시계열 분석</strong>이라는 
또 다른 축을 세웠다. ARIMA로 평균을, GARCH로 변동성을 모델링하면서, "수익률은 예측하기 어렵지만 변동성은 
예측 가능하다"는 금융의 핵심 통찰을 체화했다.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R6에서는 <strong>텍스트 데이터</strong>라는 완전히 새로운 영역에 진입했다. 숫자가 아닌 단어에서 매매 시그널을 
추출하는 것 — 이것이 NLP의 핵심이다. 토큰화에서 시작해 BoW, TF-IDF, 나이브 베이즈로 기초를 다졌고, 
Word2Vec과 Doc2Vec으로 단어의 의미를 벡터 공간에 매핑했다. LDA로 문서의 숨겨진 토픽을 발견했고, 
마지막으로 Transformer와 BERT/FinBERT라는 현대 NLP의 정점에 도달했다.
</p>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
이제 우리의 무기고에는 <strong>세 가지 핵심 무기</strong>가 갖춰졌다:
</p>
<ul style="color:#37474f;line-height:1.9;margin-top:5px">
<li><strong>지도학습 (R4):</strong> 피처 → 라벨 매핑 학습 → 예측</li>
<li><strong>비지도학습 + 시계열 (R5):</strong> 구조 발견 + 시간 패턴 모델링</li>
<li><strong>NLP (R6):</strong> 텍스트 → 수치 변환 → 감성/토픽 시그널</li>
</ul>
<p class="ni" style="font-size:14px;line-height:1.9;color:#37474f;margin-top:10px">
R7부터는 이 세 가지 무기를 <strong>딥러닝</strong>이라는 더 강력한 프레임워크로 통합한다. 
신경망은 지도학습(분류/회귀), 비지도학습(오토인코더), NLP(Transformer)를 하나의 아키텍처로 
처리할 수 있는 범용 도구다. 준비됐는가?
</p>
</div>

<h3>13.5 다음 라운드 예고: Round 7 — 딥러닝 핵심 (ANN, CNN, RNN, LSTM)</h3>

<!-- 다음 라운드 예고 다이어그램 -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff3e0,#fce4ec);border-radius:10px;border:2px solid #e65100">
<p class="ni" style="text-align:center;font-weight:bold;font-size:15px;margin-bottom:15px;color:#bf360c">🔮 Round 7 Preview — Deep Learning Fundamentals</p>

<!-- 딥러닝 아키텍처 진화 다이어그램 -->
<div style="display:flex;align-items:center;justify-content:center;gap:8px;flex-wrap:wrap;margin-bottom:15px">
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">🧠</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">Perceptron</div>
<div style="color:#888;font-size:10px">단일 뉴런</div>
</div>
<div style="font-size:20px;color:#ff9800">→</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">🔗</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">ANN/MLP</div>
<div style="color:#888;font-size:10px">다층 퍼셉트론</div>
</div>
<div style="font-size:20px;color:#ff9800">→</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">🖼️</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">CNN</div>
<div style="color:#888;font-size:10px">합성곱 신경망</div>
</div>
<div style="font-size:20px;color:#ff9800">→</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">🔄</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">RNN/LSTM</div>
<div style="color:#888;font-size:10px">시퀀스 모델링</div>
</div>
<div style="font-size:20px;color:#ff9800">→</div>
<div style="background:#fff;padding:12px 16px;border-radius:10px;border:2px solid #ff9800;text-align:center;min-width:100px">
<div style="font-size:22px;margin-bottom:4px">⚡</div>
<div style="font-weight:bold;color:#e65100;font-size:13px">GRU</div>
<div style="color:#888;font-size:10px">경량 LSTM</div>
</div>
</div>

<div style="display:flex;flex-wrap:wrap;gap:10px;justify-content:center;font-size:12px;margin-top:10px">
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">🔥 PyTorch 기초</div>
<div style="color:#777;font-size:10px;margin-top:3px">텐서, autograd, nn.Module</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">📉 역전파 알고리즘</div>
<div style="color:#777;font-size:10px;margin-top:3px">체인룰, 그래디언트 흐름</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">📊 캔들차트 CNN</div>
<div style="color:#777;font-size:10px;margin-top:3px">이미지 패턴 인식</div>
</div>
<div style="background:#fff;padding:10px 16px;border-radius:8px;border:1px solid #ddd;text-align:center;min-width:140px">
<div style="font-weight:bold;color:#2c3e50">📈 LSTM 주가 예측</div>
<div style="color:#777;font-size:10px;margin-top:3px">시계열 시퀀스 모델링</div>
</div>
</div>

<p class="ni" style="text-align:center;margin-top:15px;color:#555;font-size:12px;line-height:1.7">
R4~R6에서 sklearn으로 ML의 핵심을 익혔다면, R7부터는 <strong>PyTorch</strong>로 딥러닝의 세계에 진입한다.<br>
퍼셉트론에서 시작해 ANN → CNN → RNN/LSTM/GRU까지, 신경망 아키텍처의 진화를 따라간다.<br>
<strong>미니 프로젝트:</strong> LSTM으로 KOSPI 5일 후 종가 예측 모델 구축
</p>
</div>

<div class="info">
<p class="ni"><strong>📚 교재 연동 (Round 7 미리보기)</strong></p>
<p class="ni" style="margin-top:8px">
MLAT Ch.17 "Deep Learning for Trading"에서 신경망의 기초(퍼셉트론, 활성화 함수, 역전파, 경사하강법)를 다루고, 
Ch.18 "CNN for Financial Time Series and Satellite Images"에서 합성곱 신경망의 금융 적용을, 
Ch.19 "RNN for Multivariate Time Series and Sentiment Analysis"에서 순환 신경망과 LSTM/GRU의 
시계열 예측 활용을 체계적으로 다룬다. MLDSF Ch.3 (ANN 기초), Ch.5~6 (딥러닝 회귀/분류)에서는 
금융 케이스스터디를 제공한다.
</p>
</div>

<div class="info">
<p class="ni"><strong>🔄 Round 6 핵심 요약</strong></p>
<p class="ni" style="margin-top:8px">이번 라운드에서 우리는 <strong>텍스트 데이터에서 매매 시그널을 추출하는 기술</strong>을 배웠다:</p>
<ul>
<li><strong>NLP 기초:</strong> 토큰화 → BoW → TF-IDF → 나이브 베이즈 — 텍스트를 숫자로 변환하는 고전적 파이프라인</li>
<li><strong>임베딩:</strong> Word2Vec(Skip-gram/CBOW) → Doc2Vec(DM/DBOW) — 단어와 문서의 의미를 벡터 공간에 매핑</li>
<li><strong>토픽 모델링:</strong> LSI → pLSA → LDA — 문서의 숨겨진 주제 구조를 발견</li>
<li><strong>현대 NLP:</strong> Transformer(Self-Attention) → BERT → FinBERT — 문맥을 이해하는 사전학습 모델</li>
<li><strong>금융 적용:</strong> 감성사전, SEC 10-K 분석, 뉴스 감성 → 트레이딩 시그널 파이프라인</li>
</ul>
<p class="ni" style="margin-top:8px">이 기술들은 R7의 딥러닝(특히 RNN/LSTM의 시퀀스 모델링)과 자연스럽게 연결된다. 
R6에서 배운 Transformer/BERT는 사실 딥러닝 아키텍처의 정점이며, R7에서 그 기초(ANN → CNN → RNN)를 
체계적으로 쌓아올린 뒤 다시 돌아오면 더 깊은 이해가 가능해진다.</p>
</div>

</div><!-- paper-content -->
</div><!-- container -->
</div><!-- main-wrapper -->

</body>
</html>
