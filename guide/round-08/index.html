<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Round 8 - Convex Optimization + Portfolio Optimization + Transformer</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300;400;500&family=Space+Mono:wght@400&family=Inter:wght@300;400&display=swap" rel="stylesheet">
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:'Inter',sans-serif;background:#fafaf8;color:#1a1a1a;line-height:1.7;overflow-x:hidden}
.sidebar{position:fixed;left:0;top:0;width:260px;height:100vh;background:rgba(255,255,255,.97);border-right:1px solid rgba(0,0,0,.06);padding:32px 24px;z-index:100;overflow-y:auto;display:flex;flex-direction:column}
.sidebar-profile{text-align:center;margin-bottom:28px;padding-bottom:24px;border-bottom:1px solid rgba(0,0,0,.08)}
.profile-icon{font-size:48px;margin-bottom:8px}
.profile-name{font-family:'Cormorant Garamond',serif;font-size:1.3rem;font-weight:500;margin-bottom:4px}
.profile-title{font-size:.68rem;color:#888;letter-spacing:.08em;text-transform:uppercase;margin-bottom:8px}
.profile-bio{font-size:.78rem;color:#666;line-height:1.5}
.sidebar-nav{flex:1;margin-top:16px}
.nav-section{margin-bottom:20px}
.nav-section-title{font-size:.6rem;font-weight:600;color:#aaa;letter-spacing:.15em;text-transform:uppercase;margin-bottom:10px}
.nav-list{list-style:none}
.nav-list li{margin-bottom:5px}
.nav-list a{font-size:.78rem;color:#555;text-decoration:none;transition:all .2s;display:block;padding:3px 0}
.nav-list a:hover{color:#0080c6;padding-left:4px}
.nav-list a.active{color:#0080c6;font-weight:500}
.nav-list a.done{color:#28a745}
.badge{display:inline-block;font-size:.5rem;background:#0080c6;color:#fff;padding:1px 5px;border-radius:8px;margin-left:3px;vertical-align:middle}
.badge-done{background:#28a745}
.sidebar-footer{padding-top:16px;border-top:1px solid rgba(0,0,0,.06);font-size:.65rem;color:#aaa;text-align:center}
.main-wrapper{margin-left:260px;min-height:100vh}
.container{max-width:1100px;margin:0 auto;padding:50px 40px 80px}
.paper-content{font-family:'Times New Roman','Nanum Myeongjo',serif;line-height:1.8;background:#fff;padding:40px;border-radius:8px;box-shadow:0 2px 20px rgba(0,0,0,.05)}
.paper-header{text-align:center;margin-bottom:40px;padding-bottom:30px;border-bottom:2px solid #333}
.paper-category{font-size:14px;color:#666;margin-bottom:10px}
.paper-title{font-size:24px;font-weight:bold;margin-bottom:12px;line-height:1.4}
.paper-subtitle{font-size:14px;color:#555;margin-bottom:8px}
.paper-team{font-size:13px;color:#444}
.code-output{background:#1e1e1e;color:#d4d4d4;padding:12px 16px;border-radius:0 0 6px 6px;font-family:'Space Mono',monospace;font-size:11.5px;line-height:1.6;margin-top:-4px;margin-bottom:18px;border-top:2px solid #333;white-space:pre-wrap;overflow-x:auto}
.code-output .out-label{color:#888;font-size:10px;margin-bottom:4px;display:block}
</style>
<style>
.abstract{background:#f8f9fa;padding:25px;margin:30px 0;border-left:4px solid #2c3e50}
.abstract-title{font-weight:bold;font-size:16px;margin-bottom:15px}
h2{font-size:18px;margin:35px 0 20px;padding-bottom:8px;border-bottom:1px solid #ddd;color:#2c3e50}
h3{font-size:15px;margin:25px 0 15px;color:#34495e}
h4{font-size:14px;margin:20px 0 12px;color:#34495e}
p{text-align:justify;margin-bottom:15px;text-indent:2em}
p.ni{text-indent:0}
table{width:100%;border-collapse:collapse;margin:20px 0;font-size:12px}
th,td{border:1px solid #ddd;padding:10px 8px;text-align:center}
th{background:#2c3e50;color:white;font-weight:bold}
tr:nth-child(even){background:#f8f9fa}
tr:hover{background:#e8f4f8}
.tc{font-size:13px;font-weight:bold;margin:15px 0 10px;text-align:center}
.eq{text-align:center;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:4px;overflow-x:auto}
ul,ol{margin-left:2em;margin-bottom:15px}
li{margin-bottom:6px}
.def{background:#fff9e6;border:1px solid #ffc107;border-radius:4px;padding:20px;margin:20px 0}
.info{background:#e8f4f8;border-left:4px solid #3498db;padding:20px;margin:20px 0}
.warn{background:#fff3cd;border-left:4px solid #f39c12;padding:20px;margin:20px 0}
.ok{background:#d4edda;border-left:4px solid #28a745;padding:20px;margin:20px 0}
pre{background:#1e1e1e;color:#d4d4d4;padding:20px;border-radius:6px;overflow-x:auto;margin:20px 0;font-family:'Space Mono','Consolas',monospace;font-size:13px;line-height:1.6}
code{font-family:'Space Mono','Consolas',monospace;font-size:13px}
p code,li code,td code{background:#f0f0f0;padding:2px 6px;border-radius:3px;color:#c7254e;font-size:12px}
.cc{font-size:12px;font-weight:bold;color:#2c3e50;margin-top:15px;margin-bottom:4px}
.cm{color:#6a9955}.kw{color:#569cd6}.st{color:#ce9178}.fn{color:#dcdcaa}.nb{color:#4ec9b0}.nu{color:#b5cea8}
.progress-bar{width:100%;height:6px;background:#e0e0e0;border-radius:3px;margin-top:16px}
.progress-fill{height:100%;background:linear-gradient(90deg,#0080c6,#00b894);border-radius:3px;width:80%}
.progress-label{font-size:11px;color:#888;margin-top:4px;text-align:center}
@media(max-width:1024px){
.sidebar{width:100%;height:auto;position:relative;border-right:none;border-bottom:1px solid rgba(0,0,0,.08);padding:16px}
.sidebar-profile{margin-bottom:10px;padding-bottom:10px;display:flex;align-items:center;gap:12px;text-align:left}
.profile-icon{font-size:32px;margin-bottom:0}.profile-bio{display:none}
.nav-section{display:inline-block;margin-right:16px;margin-bottom:8px}
.nav-list{display:flex;gap:10px;flex-wrap:wrap}.nav-list li{margin-bottom:0}
.sidebar-footer{display:none}
.main-wrapper{margin-left:0}
.container{padding:0}.paper-content{padding:20px 16px;border-radius:0;box-shadow:none}
.paper-title{font-size:18px}p{font-size:14px;text-indent:1.5em;text-align:left}
pre{font-size:11px;padding:14px}table{font-size:10px;display:block;overflow-x:auto}
}
</style>
</head>
<body>

<div class="sidebar">
<div class="sidebar-profile">
<div class="profile-icon">&#x1F9E0;</div>
<div class="profile-name">HFT ML Master Plan</div>
<div class="profile-title">Convex Opt + DL + HFT</div>
<div class="profile-bio">10 Rounds: Zero to HFT System Trading</div>
</div>
<div class="sidebar-nav">
<div class="nav-section">
<div class="nav-section-title">Curriculum</div>
<ul class="nav-list">
<li><a class="done" href="../round_01/lecture_01.html">R1. Python + Finance <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_02/lecture_02.html">R2. Linear Algebra + Stats <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_03/lecture_03.html">R3. Data / Feature Eng. <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_04/lecture_04.html">R4. Supervised Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_05/lecture_05.html">R5. Unsupervised + TS <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_06/lecture_06.html">R6. NLP + Sentiment <span class="badge badge-done">DONE</span></a></li>
<li><a class="done" href="../round_07/lecture_07.html">R7. Deep Learning <span class="badge badge-done">DONE</span></a></li>
<li><a class="active" href="#">R8. Convex Opt + Transformer <span class="badge">NOW</span></a></li>
<li><a href="#">R9. HFT + RL</a></li>
<li><a href="#">R10. Final Project</a></li>
</ul>
</div>
<div class="nav-section">
<div class="nav-section-title">This Lecture</div>
<ul class="nav-list">
<li><a href="#ch1">1. ì™œ Convex Optimizationì¸ê°€</a></li>
<li><a href="#ch2">2. ë³¼ë¡ì§‘í•©ê³¼ ë³¼ë¡í•¨ìˆ˜</a></li>
<li><a href="#ch3">3. KKT ì¡°ê±´ &amp; ë¼ê·¸ë‘ì£¼</a></li>
<li><a href="#ch4">4. CVXPY ì‹¤ìŠµ</a></li>
<li><a href="#ch5">5. Mean-Variance ìµœì í™”</a></li>
<li><a href="#ch6">6. Black-Litterman ëª¨ë¸</a></li>
<li><a href="#ch7">7. Risk Parity</a></li>
<li><a href="#ch8">8. HRP (ê³„ì¸µì  ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹°)</a></li>
<li><a href="#ch9">9. Kelly Criterion</a></li>
<li><a href="#ch10">10. Transformer: Self-Attention</a></li>
<li><a href="#ch11">11. Positional Encoding + MHA</a></li>
<li><a href="#ch12">12. ê¸ˆìœµ Transformer ì‹œê³„ì—´</a></li>
<li><a href="#ch13">13. Quiz + Mini Project</a></li>
</ul>
</div>
</div>
<div class="sidebar-footer">Round 8 of 10 Â· âš¡ Convex Opt + Transformer</div>
</div>

<div class="main-wrapper">
<div class="container">
<div class="paper-content">

<div class="paper-header">
<div class="paper-category">Round 8 / 10 Â· ë”¥ëŸ¬ë‹ + ìµœì í™”</div>
<h1 class="paper-title">Convex Optimization &amp; Portfolio Optimization<br>+ Transformer Architecture</h1>
<div class="paper-subtitle">ë³¼ë¡ ìµœì í™”ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ì„¤ê³„í•˜ê³ , Transformerë¡œ ì‹œì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•</div>
<div class="paper-team">Textbooks: MLAT Ch.5, Ch.16 (Transformer), Ch.20~21 / MLDSF Ch.7~8</div>
<div class="progress-bar"><div class="progress-fill"></div></div>
<div class="progress-label">Overall Progress: 80%</div>
</div>

<div class="abstract">
<div class="abstract-title">Abstract</div>
<p class="ni">
R7ê¹Œì§€ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ì•„í‚¤í…ì²˜(ANN, CNN, RNN, LSTM)ë¥¼ ëª¨ë‘ ìµí˜”ë‹¤. ì´ì œ ë‘ ê°€ì§€ ê°•ë ¥í•œ ë¬´ê¸°ë¥¼ ì¶”ê°€í•œë‹¤. ì²«ì§¸, <strong>Convex Optimization</strong>(ë³¼ë¡ ìµœì í™”) â€” í¬íŠ¸í´ë¦¬ì˜¤ì˜ ë¹„ì¤‘ì„ ìˆ˜í•™ì ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” ì´ë¡ ê³¼ ë„êµ¬ë‹¤. "ì–´ë–¤ ì¢…ëª©ì„ ì–¼ë§ˆë‚˜ ì‚´ ê²ƒì¸ê°€?"ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•´, ê°ì´ ì•„ë‹Œ ìˆ˜í•™ìœ¼ë¡œ ë‹µí•œë‹¤. Markowitzì˜ Mean-Varianceë¶€í„° Black-Litterman, Risk Parity, HRP, Kelly Criterionê¹Œì§€ â€” í˜„ëŒ€ ìì‚°ë°°ë¶„ì˜ í•µì‹¬ ê¸°ë²•ì„ ëª¨ë‘ ë‹¤ë£¬ë‹¤. ë‘˜ì§¸, <strong>Transformer</strong> â€” "Attention Is All You Need" ë…¼ë¬¸ìœ¼ë¡œ ì‹œì‘ëœ í˜ëª…ì  ì•„í‚¤í…ì²˜ë‹¤. RNN/LSTMì˜ ìˆœì°¨ ì²˜ë¦¬ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , Self-Attentionìœ¼ë¡œ ì‹œê³„ì—´ì˜ ëª¨ë“  ì‹œì ì„ ë™ì‹œì— ì°¸ì¡°í•œë‹¤. GPT, BERTì˜ ê¸°ë°˜ì´ ë˜ëŠ” ì´ êµ¬ì¡°ë¥¼ ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì ìš©í•œë‹¤.
</p>
<p class="ni" style="margin-top:10px">
R2ì˜ ì„ í˜•ëŒ€ìˆ˜(ê³µë¶„ì‚° í–‰ë ¬, ê³ ìœ ê°’)ê°€ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì˜ ìˆ˜í•™ì  ê¸°ë°˜ì´ ë˜ê³ , R7ì˜ ë”¥ëŸ¬ë‹ ê¸°ì´ˆ(ì—­ì „íŒŒ, ê²½ì‚¬í•˜ê°•ë²•)ê°€ Transformer ì´í•´ì˜ í† ëŒ€ê°€ ëœë‹¤. ì´ ë‘ ì¶•ì„ ê²°í•©í•˜ë©´ â€” Transformerê°€ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•˜ê³ , Convex Optimizationì´ ìµœì  ë¹„ì¤‘ì„ ê²°ì •í•˜ëŠ” â€” ì™„ì „í•œ íˆ¬ì íŒŒì´í”„ë¼ì¸ì´ ì™„ì„±ëœë‹¤.
</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 1: ì™œ Convex Optimizationì¸ê°€
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch1">Chapter 1. ì™œ Convex Optimizationì¸ê°€ â€” ìµœì í™”ì˜ ì„¸ê³„ë¡œ</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.5 "Portfolio Optimization and Performance Evaluation" ë„ì…ë¶€ / MLDSF Ch.7 "Portfolio Management" â€” í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ê°€ ì™œ í•„ìš”í•œì§€, ê·¸ë¦¬ê³  ê·¸ ìˆ˜í•™ì  ê¸°ë°˜ì´ Convex Optimizationì¸ ì´ìœ ë¥¼ ë‹¤ë£¬ë‹¤.</p>
</div>

<h3>1.1 íˆ¬ìì˜ ê·¼ë³¸ ì§ˆë¬¸</h3>

<p>
R4ì—ì„œ XGBoostë¡œ ì£¼ê°€ ë°©í–¥ì„ ì˜ˆì¸¡í–ˆê³ , R7ì—ì„œ LSTMìœ¼ë¡œ ì‹œê³„ì—´ì„ ì˜ˆì¸¡í–ˆë‹¤. í•˜ì§€ë§Œ "ë‚´ì¼ ì‚¼ì„±ì „ìê°€ ì˜¤ë¥¸ë‹¤"ëŠ” ì˜ˆì¸¡ë§Œìœ¼ë¡œëŠ” ëˆì„ ë²Œ ìˆ˜ ì—†ë‹¤. ì§„ì§œ ì§ˆë¬¸ì€ ì´ê²ƒì´ë‹¤: <strong>"ì‚¼ì„±ì „ìì— ìì‚°ì˜ ëª‡ %ë¥¼ íˆ¬ìí•  ê²ƒì¸ê°€?"</strong> 10%? 50%? ì˜¬ì¸? ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì´ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”(Portfolio Optimization)ì´ê³ , ê·¸ ìˆ˜í•™ì  ì—”ì§„ì´ ë°”ë¡œ Convex Optimizationì´ë‹¤.
</p>

<p>
ë¹„ìœ í•˜ìë©´, ML ëª¨ë¸ì€ "ì–´ë””ë¡œ ê°€ì•¼ í•˜ëŠ”ì§€"ë¥¼ ì•Œë ¤ì£¼ëŠ” <strong>ë‚´ë¹„ê²Œì´ì…˜</strong>ì´ê³ , í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ëŠ” "ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ, ì–´ë–¤ ê²½ë¡œë¡œ ê°ˆì§€"ë¥¼ ê²°ì •í•˜ëŠ” <strong>ìš´ì „ ì „ëµ</strong>ì´ë‹¤. ì•„ë¬´ë¦¬ ì¢‹ì€ ë‚´ë¹„ê²Œì´ì…˜ì´ ìˆì–´ë„ ìš´ì „ì„ ëª»í•˜ë©´ ì‚¬ê³ ê°€ ë‚œë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, ì•„ë¬´ë¦¬ ì¢‹ì€ ì˜ˆì¸¡ ëª¨ë¸ì´ ìˆì–´ë„ ìê¸ˆ ë°°ë¶„ì„ ì˜ëª»í•˜ë©´ íŒŒì‚°í•œë‹¤.
</p>

<h3>1.2 ì™œ "ë³¼ë¡"(Convex)ì´ì–´ì•¼ í•˜ëŠ”ê°€?</h3>

<p>
ìµœì í™”(Optimization)ë€ ì–´ë–¤ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì†Œí™”(ë˜ëŠ” ìµœëŒ€í™”)í•˜ëŠ” ë³€ìˆ˜ ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ë¬¸ì œëŠ” ì¼ë°˜ì ì¸ ìµœì í™” ë¬¸ì œëŠ” <strong>ë§¤ìš° ì–´ë µë‹¤</strong>ëŠ” ê²ƒì´ë‹¤. ì‚° ì†ì—ì„œ ê°€ì¥ ë‚®ì€ ê³¨ì§œê¸°ë¥¼ ì°¾ëŠ”ë‹¤ê³  ìƒìƒí•´ë³´ì. ì‚°ì´ ìš¸í‰ë¶ˆí‰í•˜ë©´(ë¹„ë³¼ë¡, non-convex) ìˆ˜ë§ì€ ê³¨ì§œê¸°(local minimum)ê°€ ìˆì–´ì„œ ì§„ì§œ ê°€ì¥ ë‚®ì€ ê³³(global minimum)ì„ ì°¾ê¸° ì–´ë µë‹¤. R7ì—ì„œ ë”¥ëŸ¬ë‹ í•™ìŠµì´ local minimumì— ë¹ ì§ˆ ìˆ˜ ìˆë‹¤ê³  í–ˆë˜ ê²ƒì„ ê¸°ì–µí•˜ì.
</p>

<p>
í•˜ì§€ë§Œ ì‚°ì´ ë§¤ëˆí•œ ê·¸ë¦‡ ëª¨ì–‘(ë³¼ë¡, convex)ì´ë¼ë©´? ì–´ë””ì„œ ì¶œë°œí•˜ë“  ë‚´ë¦¬ë§‰ì„ ë”°ë¼ê°€ë©´ ë°˜ë“œì‹œ ê°€ì¥ ë‚®ì€ ê³³ì— ë„ë‹¬í•œë‹¤. <strong>ë³¼ë¡ ìµœì í™” ë¬¸ì œëŠ” local minimum = global minimum</strong>ì´ë¼ëŠ” ë†€ë¼ìš´ ì„±ì§ˆì„ ê°€ì§„ë‹¤. ì´ê²ƒì´ Convex Optimizationì´ íŠ¹ë³„í•œ ì´ìœ ë‹¤ â€” íš¨ìœ¨ì ìœ¼ë¡œ í’€ ìˆ˜ ìˆê³ , í•´ê°€ ìœ ì¼í•˜ë©°, ìˆ˜í•™ì ìœ¼ë¡œ ë³´ì¥ëœë‹¤.
</p>

<!-- ë³¼ë¡ vs ë¹„ë³¼ë¡ ì‹œê° ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e8eaf6,#fce4ec);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:18px;color:#283593">ğŸ”ï¸ ë³¼ë¡(Convex) vs ë¹„ë³¼ë¡(Non-Convex) ìµœì í™”</p>
<div style="display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:260px;background:#fff;padding:18px;border-radius:10px;border:2px solid #4caf50;box-shadow:0 2px 8px rgba(76,175,80,.15)">
<p class="ni" style="font-weight:bold;color:#2e7d32;text-align:center;margin-bottom:12px">âœ… Convex (ë³¼ë¡)</p>
<div style="text-align:center;font-size:48px;margin:10px 0">ğŸ¥£</div>
<p class="ni" style="font-size:12px;text-align:center;color:#555">ê·¸ë¦‡ ëª¨ì–‘ â€” ì–´ë””ì„œ ì¶œë°œí•´ë„<br>ë°”ë‹¥(global minimum)ì— ë„ë‹¬</p>
<div style="margin-top:12px;padding:10px;background:#e8f5e9;border-radius:6px;font-size:11px">
<p class="ni">â€¢ Local min = Global min (ìœ ì¼í•œ í•´)</p>
<p class="ni">â€¢ íš¨ìœ¨ì  ì•Œê³ ë¦¬ì¦˜ ì¡´ì¬ (ë‹¤í•­ ì‹œê°„)</p>
<p class="ni">â€¢ í•´ì˜ ì¡´ì¬ì™€ ìœ ì¼ì„± ë³´ì¥</p>
<p class="ni">â€¢ ì˜ˆ: í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚° ìµœì†Œí™”</p>
</div>
</div>
<div style="flex:1;min-width:260px;background:#fff;padding:18px;border-radius:10px;border:2px solid #f44336;box-shadow:0 2px 8px rgba(244,67,54,.15)">
<p class="ni" style="font-weight:bold;color:#c62828;text-align:center;margin-bottom:12px">âŒ Non-Convex (ë¹„ë³¼ë¡)</p>
<div style="text-align:center;font-size:48px;margin:10px 0">ğŸ”ï¸</div>
<p class="ni" style="font-size:12px;text-align:center;color:#555">ìš¸í‰ë¶ˆí‰í•œ ì‚° â€” ìˆ˜ë§ì€ ê³¨ì§œê¸°ì—<br>ë¹ ì§ˆ ìˆ˜ ìˆìŒ (local minimum)</p>
<div style="margin-top:12px;padding:10px;background:#ffebee;border-radius:6px;font-size:11px">
<p class="ni">â€¢ Local min â‰  Global min (í•¨ì • ë‹¤ìˆ˜)</p>
<p class="ni">â€¢ NP-hard (í’€ê¸° ë§¤ìš° ì–´ë ¤ì›€)</p>
<p class="ni">â€¢ ì´ˆê¸°ê°’ì— ë”°ë¼ ê²°ê³¼ ë‹¬ë¼ì§</p>
<p class="ni">â€¢ ì˜ˆ: ë”¥ëŸ¬ë‹ ì†ì‹¤í•¨ìˆ˜ ìµœì í™”</p>
</div>
</div>
</div>
</div>

<h3>1.3 ê¸ˆìœµì—ì„œì˜ Convex Optimization</h3>

<p>
ë‹¤í–‰íˆë„ ê¸ˆìœµì˜ ë§ì€ í•µì‹¬ ë¬¸ì œê°€ ë³¼ë¡ ìµœì í™”ë¡œ ì •ì‹í™”(formulation)ëœë‹¤. í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚° ìµœì†Œí™”, ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™”, ë¦¬ìŠ¤í¬ ì œì•½ í•˜ì˜ ìˆ˜ìµ ê·¹ëŒ€í™” â€” ì´ ëª¨ë“  ê²ƒì´ ë³¼ë¡ ìµœì í™” ë¬¸ì œë‹¤. Harry Markowitzê°€ 1952ë…„ì— ì œì•ˆí•œ Mean-Variance Optimizationì´ ë°”ë¡œ ì´ì°¨ ë³¼ë¡ ìµœì í™”(Quadratic Convex Optimization)ì˜ ëŒ€í‘œì  ì‚¬ë¡€ì´ë©°, ì´ ì—…ì ìœ¼ë¡œ 1990ë…„ ë…¸ë²¨ ê²½ì œí•™ìƒì„ ìˆ˜ìƒí–ˆë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– Convex Optimization ë¬¸ì œì˜ í‘œì¤€í˜•</strong></p>
<div class="eq">
$$\min_{x} \; f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \;\; h_j(x) = 0$$
</div>
<p class="ni" style="margin-top:8px;font-size:13px">
ì—¬ê¸°ì„œ \(f(x)\)ëŠ” ë³¼ë¡ ëª©ì í•¨ìˆ˜, \(g_i(x)\)ëŠ” ë³¼ë¡ ë¶€ë“±ì‹ ì œì•½, \(h_j(x)\)ëŠ” ì•„í•€(affine) ë“±ì‹ ì œì•½ì´ë‹¤. ì´ ì„¸ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ "ë³¼ë¡ ìµœì í™” ë¬¸ì œ"ë¼ ë¶€ë¥´ë©°, ì „ì—­ ìµœì í•´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆë‹¤.
</p>
</div>

<p>
ì´ë²ˆ ë¼ìš´ë“œì˜ ì „ë°˜ë¶€(Ch.1~9)ì—ì„œëŠ” ì´ ì´ë¡ ì„ ë°°ìš°ê³  CVXPYë¡œ ì‹¤ìŠµí•œ ë’¤, ì‹¤ì œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì— ì ìš©í•œë‹¤. í›„ë°˜ë¶€(Ch.10~12)ì—ì„œëŠ” Transformer ì•„í‚¤í…ì²˜ë¥¼ ë°°ì›Œ ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì— í™œìš©í•œë‹¤. ìµœì¢…ì ìœ¼ë¡œ Ch.13ì˜ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ì—ì„œ ë‘ ì¶•ì„ ê²°í•©í•œë‹¤.
</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 2: ë³¼ë¡ì§‘í•©ê³¼ ë³¼ë¡í•¨ìˆ˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch2">Chapter 2. ë³¼ë¡ì§‘í•©ê³¼ ë³¼ë¡í•¨ìˆ˜ â€” Convexityì˜ ìˆ˜í•™</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> ì´ ë‚´ìš©ì€ Boyd &amp; Vandenbergheì˜ "Convex Optimization" (í‘œì¤€ êµê³¼ì„œ) Ch.2~3ì— í•´ë‹¹í•œë‹¤. MLAT Ch.5ì—ì„œëŠ” ì´ ì´ë¡ ì„ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì— ì§ì ‘ ì ìš©í•œë‹¤.</p>
</div>

<h3>2.1 ë³¼ë¡ì§‘í•© (Convex Set)</h3>

<p>
ë³¼ë¡ì§‘í•©ì˜ ì •ì˜ëŠ” ë†€ëë„ë¡ ì§ê´€ì ì´ë‹¤. ì§‘í•© ì•ˆì˜ ì„ì˜ì˜ ë‘ ì ì„ ì‡ëŠ” ì„ ë¶„ì´ ëª¨ë‘ ê·¸ ì§‘í•© ì•ˆì— ìˆìœ¼ë©´, ê·¸ ì§‘í•©ì€ ë³¼ë¡í•˜ë‹¤. ë¹„ìœ í•˜ìë©´, ë³¼ë¡ì§‘í•©ì€ "ì›€í‘¹ ë“¤ì–´ê°„ ê³³ì´ ì—†ëŠ”" ë„í˜•ì´ë‹¤. ì›, ì‚¼ê°í˜•, ì‚¬ê°í˜•ì€ ë³¼ë¡ì§‘í•©ì´ì§€ë§Œ, ë³„ ëª¨ì–‘ì´ë‚˜ ì´ˆìŠ¹ë‹¬ì€ ë³¼ë¡ì§‘í•©ì´ ì•„ë‹ˆë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– ë³¼ë¡ì§‘í•© (Convex Set) ì •ì˜</strong></p>
<p class="ni" style="margin-top:8px">ì§‘í•© \(C\)ê°€ ë³¼ë¡í•˜ë‹¤ âŸº ì„ì˜ì˜ \(x_1, x_2 \in C\)ì™€ \(0 \leq \theta \leq 1\)ì— ëŒ€í•´:</p>
<div class="eq">
$$\theta x_1 + (1-\theta) x_2 \in C$$
</div>
<p class="ni" style="font-size:12px;color:#666">ì¦‰, ë‘ ì ì˜ ë³¼ë¡ ê²°í•©(convex combination)ì´ í•­ìƒ ì§‘í•© ì•ˆì— ìˆë‹¤.</p>
</div>

<!-- ë³¼ë¡ì§‘í•© ì‹œê° ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e0f7fa,#f1f8e9);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#00695c">ğŸ“ ë³¼ë¡ì§‘í•© vs ë¹„ë³¼ë¡ì§‘í•©</p>
<div style="display:flex;gap:20px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;text-align:center">
<div style="width:100px;height:100px;background:linear-gradient(135deg,#a5d6a7,#66bb6a);border-radius:50%;margin:0 auto 10px;display:flex;align-items:center;justify-content:center;color:#fff;font-weight:bold;font-size:12px">ì› â­•</div>
<p class="ni" style="font-size:12px;color:#2e7d32;font-weight:bold">âœ… ë³¼ë¡ì§‘í•©</p>
<p class="ni" style="font-size:11px;color:#666">ë‘ ì ì„ ì´ìœ¼ë©´ í•­ìƒ ì› ì•ˆ</p>
</div>
<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;text-align:center">
<div style="width:100px;height:100px;background:linear-gradient(135deg,#a5d6a7,#66bb6a);margin:0 auto 10px;display:flex;align-items:center;justify-content:center;color:#fff;font-weight:bold;font-size:12px">â–² ì‚¼ê°í˜•</div>
<p class="ni" style="font-size:12px;color:#2e7d32;font-weight:bold">âœ… ë³¼ë¡ì§‘í•©</p>
<p class="ni" style="font-size:11px;color:#666">ë‘ ì ì„ ì´ìœ¼ë©´ í•­ìƒ ì‚¼ê°í˜• ì•ˆ</p>
</div>
<div style="flex:1;min-width:200px;background:#fff;padding:16px;border-radius:10px;text-align:center">
<div style="width:100px;height:100px;background:linear-gradient(135deg,#ef9a9a,#e57373);margin:0 auto 10px;display:flex;align-items:center;justify-content:center;color:#fff;font-weight:bold;font-size:28px">â˜†</div>
<p class="ni" style="font-size:12px;color:#c62828;font-weight:bold">âŒ ë¹„ë³¼ë¡ì§‘í•©</p>
<p class="ni" style="font-size:11px;color:#666">ë‘ ì ì„ ì´ìœ¼ë©´ ë³„ ë°–ìœ¼ë¡œ ë‚˜ê°</p>
</div>
</div>
</div>

<p>
ê¸ˆìœµì—ì„œ ë³¼ë¡ì§‘í•©ì˜ ëŒ€í‘œì  ì˜ˆëŠ” <strong>í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ì˜ ì œì•½ ì¡°ê±´</strong>ì´ë‹¤. "ëª¨ë“  ë¹„ì¤‘ì˜ í•© = 1"ì´ê³  "ê° ë¹„ì¤‘ â‰¥ 0" (ê³µë§¤ë„ ê¸ˆì§€)ì´ë¼ëŠ” ì¡°ê±´ì€ ì‹¬í”Œë ‰ìŠ¤(simplex)ë¼ëŠ” ë³¼ë¡ì§‘í•©ì„ í˜•ì„±í•œë‹¤. ì´ ì•ˆì—ì„œ ìµœì  ë¹„ì¤‘ì„ ì°¾ëŠ” ê²ƒì´ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ë‹¤.
</p>

<h3>2.2 ë³¼ë¡í•¨ìˆ˜ (Convex Function)</h3>

<p>
ë³¼ë¡í•¨ìˆ˜ëŠ” "ì•„ë˜ë¡œ ë³¼ë¡í•œ" í•¨ìˆ˜ë‹¤. ê·¸ë˜í”„ ìœ„ì˜ ì„ì˜ì˜ ë‘ ì ì„ ì‡ëŠ” ì„ ë¶„ì´ í•­ìƒ ê·¸ë˜í”„ ìœ„ì— ìˆìœ¼ë©´ ë³¼ë¡í•¨ìˆ˜ë‹¤. ì§ê´€ì ìœ¼ë¡œ, ê·¸ë¦‡(bowl) ëª¨ì–‘ì˜ í•¨ìˆ˜ê°€ ë³¼ë¡í•¨ìˆ˜ë‹¤.
</p>

<div class="def">
<p class="ni"><strong>ğŸ“– ë³¼ë¡í•¨ìˆ˜ (Convex Function) ì •ì˜</strong></p>
<p class="ni" style="margin-top:8px">í•¨ìˆ˜ \(f: \mathbb{R}^n \to \mathbb{R}\)ê°€ ë³¼ë¡í•˜ë‹¤ âŸº ì •ì˜ì—­ì´ ë³¼ë¡ì§‘í•©ì´ê³ , ì„ì˜ì˜ \(x, y\)ì™€ \(0 \leq \theta \leq 1\)ì— ëŒ€í•´:</p>
<div class="eq">
$$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$$
</div>
<p class="ni" style="font-size:12px;color:#666">Jensen's inequality â€” í•¨ìˆ˜ê°’ì˜ ë³¼ë¡ ê²°í•©ì´ ë³¼ë¡ ê²°í•©ì˜ í•¨ìˆ˜ê°’ë³´ë‹¤ í•­ìƒ í¬ê±°ë‚˜ ê°™ë‹¤.</p>
</div>

<h4>ë³¼ë¡í•¨ìˆ˜ì˜ ì˜ˆì‹œ</h4>

<table>
<tr><th>í•¨ìˆ˜</th><th>ìˆ˜ì‹</th><th>ë³¼ë¡?</th><th>ê¸ˆìœµ ì‘ìš©</th></tr>
<tr><td>ì´ì°¨í•¨ìˆ˜</td><td>\(f(x) = x^2\)</td><td>âœ… ë³¼ë¡</td><td>í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚°</td></tr>
<tr><td>ì ˆëŒ€ê°’</td><td>\(f(x) = |x|\)</td><td>âœ… ë³¼ë¡</td><td>L1 ì •ê·œí™” (Lasso)</td></tr>
<tr><td>ì§€ìˆ˜í•¨ìˆ˜</td><td>\(f(x) = e^x\)</td><td>âœ… ë³¼ë¡</td><td>ë¡œê·¸ìˆ˜ìµë¥  ë³€í™˜</td></tr>
<tr><td>ìŒì˜ ë¡œê·¸</td><td>\(f(x) = -\log x\)</td><td>âœ… ë³¼ë¡</td><td>Kelly criterion</td></tr>
<tr><td>ë…¸ë¦„</td><td>\(f(x) = \|x\|_2\)</td><td>âœ… ë³¼ë¡</td><td>ë¦¬ìŠ¤í¬ ì¸¡ì •</td></tr>
<tr><td>ì‚¬ì¸í•¨ìˆ˜</td><td>\(f(x) = \sin x\)</td><td>âŒ ë¹„ë³¼ë¡</td><td>â€”</td></tr>
</table>
<p class="ni tc">Table 2.1: ë³¼ë¡í•¨ìˆ˜ì™€ ë¹„ë³¼ë¡í•¨ìˆ˜ ì˜ˆì‹œ</p>

<h3>2.3 ë³¼ë¡ì„± íŒë³„ë²•</h3>

<p>
í•¨ìˆ˜ê°€ ë³¼ë¡í•œì§€ ì–´ë–»ê²Œ í™•ì¸í• ê¹Œ? ë‘ ê°€ì§€ ê°•ë ¥í•œ ë„êµ¬ê°€ ìˆë‹¤:
</p>

<p>
<strong>1ì°¨ ì¡°ê±´ (First-order condition):</strong> ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜ \(f\)ê°€ ë³¼ë¡í•˜ë‹¤ âŸº ëª¨ë“  ì ì—ì„œ ì ‘ì„ ì´ í•¨ìˆ˜ ì•„ë˜ì— ìˆë‹¤:
</p>

<div class="eq">
$$f(y) \geq f(x) + \nabla f(x)^T (y - x) \quad \forall x, y$$
</div>

<p>
<strong>2ì°¨ ì¡°ê±´ (Second-order condition):</strong> ë‘ ë²ˆ ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜ \(f\)ê°€ ë³¼ë¡í•˜ë‹¤ âŸº í—¤ì‹œì•ˆ í–‰ë ¬(Hessian matrix)ì´ ì–‘ë°˜ì •ì¹˜(positive semidefinite)ì´ë‹¤:
</p>

<div class="eq">
$$\nabla^2 f(x) \succeq 0 \quad \forall x$$
</div>

<p>
R2ì—ì„œ ë°°ìš´ ê³ ìœ ê°’ì„ ê¸°ì–µí•˜ì. í–‰ë ¬ì´ ì–‘ë°˜ì •ì¹˜ë¼ëŠ” ê²ƒì€ ëª¨ë“  ê³ ìœ ê°’ì´ 0 ì´ìƒì´ë¼ëŠ” ëœ»ì´ë‹¤. í¬íŠ¸í´ë¦¬ì˜¤ì˜ ê³µë¶„ì‚° í–‰ë ¬ \(\Sigma\)ëŠ” í•­ìƒ ì–‘ë°˜ì •ì¹˜ì´ë¯€ë¡œ, í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚° \(w^T \Sigma w\)ëŠ” í•­ìƒ ë³¼ë¡í•¨ìˆ˜ë‹¤. ì´ê²ƒì´ Mean-Variance ìµœì í™”ê°€ ë³¼ë¡ ìµœì í™”ì¸ ì´ìœ ë‹¤.
</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 3: KKT ì¡°ê±´, ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch3">Chapter 3. KKT ì¡°ê±´ê³¼ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²• â€” ì œì•½ ì¡°ê±´ í•˜ì˜ ìµœì í™”</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> Boyd &amp; Vandenberghe "Convex Optimization" Ch.5 (Duality) â€” KKT ì¡°ê±´ì€ ì œì•½ ìˆëŠ” ìµœì í™”ì˜ í•µì‹¬ ì´ë¡ ì´ë‹¤. MLAT Ch.5ì—ì„œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì˜ ìˆ˜í•™ì  ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.</p>
</div>

<h3>3.1 ì™œ ì œì•½ ì¡°ê±´ì´ í•„ìš”í•œê°€?</h3>

<p>
í˜„ì‹¤ì˜ íˆ¬ìì—ëŠ” í•­ìƒ ì œì•½ì´ ìˆë‹¤. "ë¹„ì¤‘ì˜ í•© = 100%", "ê³µë§¤ë„ ê¸ˆì§€ (ê° ë¹„ì¤‘ â‰¥ 0)", "í•œ ì¢…ëª©ì— 30% ì´ìƒ íˆ¬ì ê¸ˆì§€", "ì„¹í„°ë³„ ë¹„ì¤‘ ì œí•œ" ë“±. ì´ëŸ° ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ìµœì í•´ë¥¼ ì°¾ëŠ” ê²ƒì´ <strong>ì œì•½ ìµœì í™”</strong>(Constrained Optimization)ì´ë‹¤.
</p>

<p>
ë¹„ìœ í•˜ìë©´, ì œì•½ ì—†ëŠ” ìµœì í™”ëŠ” "ì§€ë„ ìœ„ ì–´ë””ë“  ê°ˆ ìˆ˜ ìˆì„ ë•Œ ê°€ì¥ ë‚®ì€ ê³³ ì°¾ê¸°"ì´ê³ , ì œì•½ ìˆëŠ” ìµœì í™”ëŠ” "ë„ë¡œ ìœ„ì—ì„œë§Œ ì´ë™í•  ìˆ˜ ìˆì„ ë•Œ ê°€ì¥ ë‚®ì€ ê³³ ì°¾ê¸°"ë‹¤. ë„ë¡œë¼ëŠ” ì œì•½ì´ ìˆìœ¼ë©´ ë¬¸ì œê°€ ë” ì–´ë ¤ì›Œì§€ì§€ë§Œ, ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì´ ì´ë¥¼ ìš°ì•„í•˜ê²Œ í•´ê²°í•œë‹¤.
</p>

<h3>3.2 ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²• (Method of Lagrange Multipliers)</h3>

<p>
ë“±ì‹ ì œì•½ \(h(x) = 0\) í•˜ì—ì„œ \(f(x)\)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¥¼ ìƒê°í•˜ì. ë¼ê·¸ë‘ì£¼ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì œì•½ ì¡°ê±´ì„ ëª©ì í•¨ìˆ˜ì— "í¡ìˆ˜"ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤. ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜(Lagrangian)ë¥¼ ì •ì˜í•œë‹¤:
</p>

<div class="eq">
$$\mathcal{L}(x, \nu) = f(x) + \nu \cdot h(x)$$
</div>

<p>
ì—¬ê¸°ì„œ \(\nu\)ëŠ” <strong>ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜</strong>(Lagrange multiplier)ë‹¤. ìµœì í•´ì—ì„œëŠ” \(\nabla_x \mathcal{L} = 0\)ì´ê³  \(h(x) = 0\)ì´ë‹¤. ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ \(\nu\)ì˜ ê²½ì œì  ì˜ë¯¸ëŠ” "ì œì•½ì„ í•œ ë‹¨ìœ„ ì™„í™”í–ˆì„ ë•Œ ëª©ì í•¨ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ê°œì„ ë˜ëŠ”ê°€"ì´ë‹¤. ê¸ˆìœµì—ì„œ ì´ê²ƒì€ <strong>ê·¸ë¦¼ì ê°€ê²©</strong>(shadow price)ì´ë¼ ë¶ˆë¦°ë‹¤ â€” ì˜ˆë¥¼ ë“¤ì–´, "ê³µë§¤ë„ ì œì•½ì„ í’€ë©´ ìƒ¤í”„ë¹„ìœ¨ì´ ì–¼ë§ˆë‚˜ ì˜¬ë¼ê°€ëŠ”ê°€?"ë¥¼ ì•Œë ¤ì¤€ë‹¤.
</p>

<h3>3.3 KKT ì¡°ê±´ (Karush-Kuhn-Tucker Conditions)</h3>

<p>
ë¶€ë“±ì‹ ì œì•½ \(g_i(x) \leq 0\)ê¹Œì§€ í¬í•¨í•˜ë©´ ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ê°€ í™•ì¥ëœë‹¤:
</p>

<div class="eq">
$$\mathcal{L}(x, \lambda, \nu) = f(x) + \sum_{i} \lambda_i g_i(x) + \sum_{j} \nu_j h_j(x)$$
</div>

<p>
KKT ì¡°ê±´ì€ ë³¼ë¡ ìµœì í™”ì—ì„œ ìµœì í•´ì˜ <strong>í•„ìš”ì¶©ë¶„ì¡°ê±´</strong>ì´ë‹¤. ë„¤ ê°€ì§€ ì¡°ê±´ìœ¼ë¡œ êµ¬ì„±ëœë‹¤:
</p>

<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff3e0,#fce4ec);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#e65100">ğŸ“‹ KKT ì¡°ê±´ (4ê°€ì§€)</p>
<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(220px,1fr));gap:12px">
<div style="background:#fff;padding:14px;border-radius:8px;border-left:4px solid #ff9800">
<p class="ni" style="font-weight:bold;font-size:12px;color:#e65100;margin-bottom:6px">â‘  ì •ìƒì„± (Stationarity)</p>
<p class="ni" style="font-size:12px">\(\nabla f(x^*) + \sum \lambda_i \nabla g_i(x^*) + \sum \nu_j \nabla h_j(x^*) = 0\)</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:4px">ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° = 0</p>
</div>
<div style="background:#fff;padding:14px;border-radius:8px;border-left:4px solid #ff9800">
<p class="ni" style="font-weight:bold;font-size:12px;color:#e65100;margin-bottom:6px">â‘¡ ì›ì‹œ ì‹¤í–‰ê°€ëŠ¥ì„± (Primal Feasibility)</p>
<p class="ni" style="font-size:12px">\(g_i(x^*) \leq 0, \;\; h_j(x^*) = 0\)</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:4px">ì œì•½ ì¡°ê±´ì„ ë§Œì¡±</p>
</div>
<div style="background:#fff;padding:14px;border-radius:8px;border-left:4px solid #ff9800">
<p class="ni" style="font-weight:bold;font-size:12px;color:#e65100;margin-bottom:6px">â‘¢ ìŒëŒ€ ì‹¤í–‰ê°€ëŠ¥ì„± (Dual Feasibility)</p>
<p class="ni" style="font-size:12px">\(\lambda_i \geq 0\)</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:4px">ë¶€ë“±ì‹ ì œì•½ì˜ ìŠ¹ìˆ˜ëŠ” ìŒì´ ì•„ë‹˜</p>
</div>
<div style="background:#fff;padding:14px;border-radius:8px;border-left:4px solid #ff9800">
<p class="ni" style="font-weight:bold;font-size:12px;color:#e65100;margin-bottom:6px">â‘£ ìƒë³´ ì´ì™„ (Complementary Slackness)</p>
<p class="ni" style="font-size:12px">\(\lambda_i \cdot g_i(x^*) = 0\)</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:4px">ì œì•½ì´ í™œì„±ì´ ì•„ë‹ˆë©´ ìŠ¹ìˆ˜ = 0</p>
</div>
</div>
</div>

<p>
â‘£ë²ˆ ìƒë³´ ì´ì™„ ì¡°ê±´ì´ íŠ¹íˆ ì¤‘ìš”í•˜ë‹¤. ì´ê²ƒì€ "ì œì•½ì´ ë“±í˜¸ë¡œ ì„±ë¦½í•˜ì§€ ì•Šìœ¼ë©´(\(g_i(x^*) < 0\)), í•´ë‹¹ ìŠ¹ìˆ˜ëŠ” 0ì´ë‹¤(\(\lambda_i = 0\))"ë¼ëŠ” ëœ»ì´ë‹¤. í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ ì´ê²ƒì€ "ë¹„ì¤‘ í•˜í•œ ì œì•½ì— ê±¸ë¦¬ì§€ ì•Šì€ ì¢…ëª©ì€ ê·¸ ì œì•½ì´ ìµœì í•´ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤"ëŠ” ì˜ë¯¸ë‹¤.
</p>

<h3>3.4 í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì—ì„œì˜ KKT</h3>

<p>
êµ¬ì²´ì  ì˜ˆë¥¼ ë³´ì. 2ì¢…ëª© í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ë˜, ë¹„ì¤‘ì˜ í•© = 1ì´ê³  ê¸°ëŒ€ìˆ˜ìµë¥  â‰¥ ëª©í‘œìˆ˜ìµë¥ ì¸ ë¬¸ì œ:
</p>

<div class="eq">
$$\min_{w_1, w_2} \; w^T \Sigma w \quad \text{s.t.} \quad w_1 + w_2 = 1, \;\; \mu^T w \geq r_{\text{target}}, \;\; w_i \geq 0$$
</div>

<p>
ì´ ë¬¸ì œì˜ KKT ì¡°ê±´ì„ í’€ë©´ ìµœì  ë¹„ì¤‘ì´ ë‚˜ì˜¨ë‹¤. í•˜ì§€ë§Œ ì¢…ëª©ì´ ìˆ˜ë°± ê°œê°€ ë˜ë©´ ì†ìœ¼ë¡œ í’€ ìˆ˜ ì—†ë‹¤. ì´ë•Œ CVXPY ê°™ì€ ì†”ë²„ê°€ KKT ì¡°ê±´ì„ ìë™ìœ¼ë¡œ í’€ì–´ì¤€ë‹¤. ë‹¤ìŒ ì±•í„°ì—ì„œ ë°”ë¡œ ì‹¤ìŠµí•œë‹¤.
</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 4: CVXPY ì‹¤ìŠµ
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch4">Chapter 4. CVXPY ì‹¤ìŠµ â€” íŒŒì´ì¬ìœ¼ë¡œ ë³¼ë¡ ìµœì í™” í’€ê¸°</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> CVXPYëŠ” Stanfordì˜ Stephen Boyd êµìˆ˜ ì—°êµ¬ì‹¤ì—ì„œ ê°œë°œí•œ íŒŒì´ì¬ ë³¼ë¡ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤. MLAT Ch.5ì—ì„œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” êµ¬í˜„ì— ì‚¬ìš©ëœë‹¤.</p>
</div>

<h3>4.1 CVXPYë€?</h3>

<p>
CVXPYëŠ” ë³¼ë¡ ìµœì í™” ë¬¸ì œë¥¼ íŒŒì´ì¬ ì½”ë“œë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•˜ê³  í’€ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤. ìˆ˜í•™ ê³µì‹ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì½”ë“œë¡œ ì˜®ê¸¸ ìˆ˜ ìˆì–´ì„œ, ìµœì í™” ì´ë¡ ì„ ëª¨ë¥´ëŠ” ì‚¬ëŒë„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œëŠ” ECOS, SCS, OSQP ê°™ì€ ê³ ì„±ëŠ¥ ì†”ë²„ë¥¼ í˜¸ì¶œí•˜ì—¬ KKT ì¡°ê±´ì„ í’€ì–´ì¤€ë‹¤.
</p>

<p class="cc">â–¼ CVXPY ì„¤ì¹˜</p>
<pre>
<span class="cm"># pipìœ¼ë¡œ ì„¤ì¹˜</span>
pip install cvxpy
</pre>

<h3>4.2 CVXPY ê¸°ë³¸ ë¬¸ë²•</h3>

<p>
CVXPYì˜ ì›Œí¬í”Œë¡œìš°ëŠ” ì„¸ ë‹¨ê³„ë‹¤: (1) ë³€ìˆ˜ ì •ì˜, (2) ëª©ì í•¨ìˆ˜ì™€ ì œì•½ ì¡°ê±´ ì •ì˜, (3) ë¬¸ì œ í’€ê¸°. ê°„ë‹¨í•œ ì˜ˆì œë¡œ ì‹œì‘í•˜ì.
</p>

<p class="cc">â–¼ CVXPY ê¸°ë³¸ ì˜ˆì œ: ì´ì°¨í•¨ìˆ˜ ìµœì†Œí™”</p>
<pre>
<span class="kw">import</span> cvxpy <span class="kw">as</span> cp
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># 1ë‹¨ê³„: ë³€ìˆ˜ ì •ì˜</span>
x = cp.<span class="fn">Variable</span>(<span class="nu">2</span>)  <span class="cm"># 2ì°¨ì› ë³€ìˆ˜</span>

<span class="cm"># 2ë‹¨ê³„: ëª©ì í•¨ìˆ˜ì™€ ì œì•½ ì¡°ê±´</span>
objective = cp.<span class="fn">Minimize</span>(cp.<span class="fn">sum_squares</span>(x))  <span class="cm"># min ||x||^2</span>
constraints = [
    x[<span class="nu">0</span>] + x[<span class="nu">1</span>] == <span class="nu">1</span>,   <span class="cm"># ë“±ì‹ ì œì•½: x1 + x2 = 1</span>
    x >= <span class="nu">0</span>                <span class="cm"># ë¶€ë“±ì‹ ì œì•½: x >= 0 (ì›ì†Œë³„)</span>
]

<span class="cm"># 3ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ë° í’€ê¸°</span>
problem = cp.<span class="fn">Problem</span>(objective, constraints)
problem.<span class="fn">solve</span>()

<span class="fn">print</span>(<span class="st">f"ìµœì ê°’: {problem.value:.4f}"</span>)
<span class="fn">print</span>(<span class="st">f"ìµœì í•´: x = {x.value}"</span>)
<span class="fn">print</span>(<span class="st">f"ìƒíƒœ: {problem.status}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output:</span>
ìµœì ê°’: 0.5000
ìµœì í•´: x = [0.5 0.5]
ìƒíƒœ: optimal</div>

<p>
ê²°ê³¼ê°€ ì§ê´€ì ìœ¼ë¡œ ë§ë‹¤. \(x_1 + x_2 = 1\)ì´ê³  \(x_1, x_2 \geq 0\)ì¼ ë•Œ \(x_1^2 + x_2^2\)ë¥¼ ìµœì†Œí™”í•˜ë©´, ë‘ ê°’ì´ ê°™ì„ ë•Œ (0.5, 0.5) ìµœì†Œê°€ ëœë‹¤. ì´ê²ƒì€ "ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ë ¤ë©´ ê· ë“± ë°°ë¶„í•˜ë¼"ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ì´ë¡ ì˜ ì§ê´€ê³¼ ì¼ì¹˜í•œë‹¤.
</p>

<h3>4.3 í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ë§›ë³´ê¸°</h3>

<p>
ì´ì œ ì‹¤ì œ ì£¼ì‹ ë°ì´í„°ë¡œ ìµœì†Œ ë¶„ì‚° í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬í•´ë³´ì. 3ì¢…ëª©(AAPL, MSFT, GOOGL)ì˜ ê³µë¶„ì‚° í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤.
</p>

<p class="cc">â–¼ CVXPYë¡œ ìµœì†Œ ë¶„ì‚° í¬íŠ¸í´ë¦¬ì˜¤</p>
<pre>
<span class="kw">import</span> cvxpy <span class="kw">as</span> cp
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf

<span class="cm"># ì£¼ê°€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ</span>
tickers = [<span class="st">'AAPL'</span>, <span class="st">'MSFT'</span>, <span class="st">'GOOGL'</span>]
data = yf.<span class="fn">download</span>(tickers, start=<span class="st">'2020-01-01'</span>, end=<span class="st">'2024-01-01'</span>)
prices = data[<span class="st">'Close'</span>]

<span class="cm"># ì¼ê°„ ìˆ˜ìµë¥  â†’ ì—°ê°„ ê³µë¶„ì‚° í–‰ë ¬</span>
returns = prices.<span class="fn">pct_change</span>().<span class="fn">dropna</span>()
mu = returns.<span class="fn">mean</span>() * <span class="nu">252</span>          <span class="cm"># ì—°ê°„ ê¸°ëŒ€ìˆ˜ìµë¥ </span>
Sigma = returns.<span class="fn">cov</span>() * <span class="nu">252</span>        <span class="cm"># ì—°ê°„ ê³µë¶„ì‚° í–‰ë ¬</span>
n = <span class="fn">len</span>(tickers)

<span class="cm"># CVXPYë¡œ ìµœì†Œ ë¶„ì‚° í¬íŠ¸í´ë¦¬ì˜¤</span>
w = cp.<span class="fn">Variable</span>(n)                  <span class="cm"># ë¹„ì¤‘ ë³€ìˆ˜</span>
portfolio_var = cp.<span class="fn">quad_form</span>(w, Sigma.values)  <span class="cm"># w^T Î£ w</span>

objective = cp.<span class="fn">Minimize</span>(portfolio_var)
constraints = [
    cp.<span class="fn">sum</span>(w) == <span class="nu">1</span>,    <span class="cm"># ë¹„ì¤‘ í•© = 1</span>
    w >= <span class="nu">0</span>              <span class="cm"># ê³µë§¤ë„ ê¸ˆì§€</span>
]

problem = cp.<span class="fn">Problem</span>(objective, constraints)
problem.<span class="fn">solve</span>()

<span class="fn">print</span>(<span class="st">"=== ìµœì†Œ ë¶„ì‚° í¬íŠ¸í´ë¦¬ì˜¤ ==="</span>)
<span class="kw">for</span> i, ticker <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {ticker}: {w.value[i]*100:.1f}%"</span>)

port_std = np.<span class="fn">sqrt</span>(problem.value)
port_ret = mu.values @ w.value
<span class="fn">print</span>(<span class="st">f"\nì—°ê°„ ìˆ˜ìµë¥ : {port_ret*100:.1f}%"</span>)
<span class="fn">print</span>(<span class="st">f"ì—°ê°„ ë³€ë™ì„±: {port_std*100:.1f}%"</span>)
<span class="fn">print</span>(<span class="st">f"ìƒ¤í”„ë¹„ìœ¨: {port_ret/port_std:.2f}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output (ì˜ˆì‹œ):</span>
=== ìµœì†Œ ë¶„ì‚° í¬íŠ¸í´ë¦¬ì˜¤ ===
  AAPL: 28.3%
  MSFT: 48.5%
  GOOGL: 23.2%

ì—°ê°„ ìˆ˜ìµë¥ : 25.7%
ì—°ê°„ ë³€ë™ì„±: 24.1%
ìƒ¤í”„ë¹„ìœ¨: 1.07</div>

<div class="ok">
<p class="ni"><strong>ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:</strong> CVXPYëŠ” ìˆ˜í•™ ê³µì‹ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì½”ë“œë¡œ ì˜®ê¸¸ ìˆ˜ ìˆë‹¤. <code>cp.quad_form(w, Sigma)</code>ê°€ \(w^T \Sigma w\)ì´ê³ , <code>cp.sum(w) == 1</code>ì´ \(\sum w_i = 1\)ì´ë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ KKT ì¡°ê±´ì„ í’€ì–´ ì „ì—­ ìµœì í•´ë¥¼ ë³´ì¥í•œë‹¤.</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 5: Mean-Variance í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch5">Chapter 5. Mean-Variance í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” â€” Markowitzì˜ ìœ ì‚°</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.5 "Mean-variance optimization" ì„¹ì…˜ â€” Markowitz ì´ë¡ , íš¨ìœ¨ì  í”„ë¡ í‹°ì–´, ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™”ë¥¼ ìƒì„¸íˆ ë‹¤ë£¬ë‹¤. MLDSF Ch.7 "Portfolio Management" â€” íŒŒì´ì¬ êµ¬í˜„ ì¼€ì´ìŠ¤ìŠ¤í„°ë””.</p>
</div>

<h3>5.1 Markowitzì˜ í˜ëª… (1952)</h3>

<p>
1952ë…„, Harry MarkowitzëŠ” "Portfolio Selection"ì´ë¼ëŠ” 14í˜ì´ì§€ì§œë¦¬ ë…¼ë¬¸ìœ¼ë¡œ í˜„ëŒ€ í¬íŠ¸í´ë¦¬ì˜¤ ì´ë¡ (Modern Portfolio Theory, MPT)ì„ ì°½ì‹œí–ˆë‹¤. í•µì‹¬ í†µì°°ì€ ë‹¨ìˆœí•˜ì§€ë§Œ í˜ëª…ì ì´ì—ˆë‹¤: <strong>ê°œë³„ ìì‚°ì˜ ìˆ˜ìµë¥ ë§Œ ë³´ì§€ ë§ê³ , ìì‚° ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ì˜ ìœ„í—˜-ìˆ˜ìµ í”„ë¡œíŒŒì¼ì„ ìµœì í™”í•˜ë¼.</strong>
</p>

<p>
ë¹„ìœ í•˜ìë©´, ì¶•êµ¬íŒ€ì„ êµ¬ì„±í•  ë•Œ ê°œì¸ ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì„ ìˆ˜ 11ëª…ì„ ëª¨ìœ¼ëŠ” ê²ƒë³´ë‹¤, ì„œë¡œ ë³´ì™„í•˜ëŠ” ì„ ìˆ˜ë“¤ì„ ì¡°í•©í•˜ëŠ” ê²ƒì´ ë” ê°•í•œ íŒ€ì„ ë§Œë“ ë‹¤. ê³µê²©ìˆ˜ë§Œ 11ëª…ì´ë©´ ìˆ˜ë¹„ê°€ ë¬´ë„ˆì§„ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, ìˆ˜ìµë¥ ì´ ë†’ì€ ì¢…ëª©ë§Œ ëª¨ìœ¼ë©´ ë¦¬ìŠ¤í¬ê°€ ì§‘ì¤‘ëœë‹¤. ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ì¢…ëª©ì„ ì¡°í•©í•˜ë©´ ê°™ì€ ìˆ˜ìµë¥ ì—ì„œ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤ â€” ì´ê²ƒì´ <strong>ë¶„ì‚° íˆ¬ì</strong>(diversification)ì˜ ìˆ˜í•™ì  ê·¼ê±°ë‹¤.
</p>

<h3>5.2 ìˆ˜í•™ì  ì •ì‹í™”</h3>

<p>
\(n\)ê°œ ìì‚°ì˜ ë¹„ì¤‘ ë²¡í„°ë¥¼ \(w = (w_1, \ldots, w_n)^T\), ê¸°ëŒ€ìˆ˜ìµë¥  ë²¡í„°ë¥¼ \(\mu\), ê³µë¶„ì‚° í–‰ë ¬ì„ \(\Sigma\)ë¼ í•˜ë©´:
</p>

<div class="eq">
$$\text{í¬íŠ¸í´ë¦¬ì˜¤ ê¸°ëŒ€ìˆ˜ìµë¥ :} \quad \mu_p = w^T \mu = \sum_{i=1}^{n} w_i \mu_i$$
</div>

<div class="eq">
$$\text{í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚°:} \quad \sigma_p^2 = w^T \Sigma w = \sum_{i=1}^{n} \sum_{j=1}^{n} w_i w_j \sigma_{ij}$$
</div>

<p>
Mean-Variance ìµœì í™”ëŠ” ë‘ ê°€ì§€ í˜•íƒœë¡œ ì •ì‹í™”í•  ìˆ˜ ìˆë‹¤:
</p>

<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e3f2fd,#e8eaf6);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#1565c0">ğŸ“Š Mean-Variance ìµœì í™”ì˜ ë‘ ê°€ì§€ í˜•íƒœ</p>
<div style="display:flex;gap:16px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:280px;background:#fff;padding:16px;border-radius:10px;border:2px solid #1976d2">
<p class="ni" style="font-weight:bold;color:#1565c0;text-align:center;margin-bottom:10px">í˜•íƒœ 1: ìµœì†Œ ë¶„ì‚°</p>
<p class="ni" style="font-size:12px;text-align:center">"ëª©í‘œ ìˆ˜ìµë¥ ì„ ë‹¬ì„±í•˜ë©´ì„œ ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”"</p>
<div class="eq" style="font-size:14px;background:transparent;padding:8px">
$$\min_w \; w^T \Sigma w$$
$$\text{s.t.} \; w^T \mu \geq r_{\text{target}}, \; \mathbf{1}^T w = 1$$
</div>
</div>
<div style="flex:1;min-width:280px;background:#fff;padding:16px;border-radius:10px;border:2px solid #7b1fa2">
<p class="ni" style="font-weight:bold;color:#7b1fa2;text-align:center;margin-bottom:10px">í˜•íƒœ 2: ìµœëŒ€ íš¨ìš©</p>
<p class="ni" style="font-size:12px;text-align:center">"ìˆ˜ìµê³¼ ë¦¬ìŠ¤í¬ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì§ì ‘ ì¡°ì ˆ"</p>
<div class="eq" style="font-size:14px;background:transparent;padding:8px">
$$\max_w \; w^T \mu - \frac{\gamma}{2} w^T \Sigma w$$
$$\text{s.t.} \; \mathbf{1}^T w = 1$$
</div>
<p class="ni" style="font-size:11px;color:#888;text-align:center">\(\gamma\) = ìœ„í—˜íšŒí”¼ê³„ìˆ˜ (í´ìˆ˜ë¡ ë³´ìˆ˜ì )</p>
</div>
</div>
</div>

<h3>5.3 íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ (Efficient Frontier)</h3>

<p>
ê° ëª©í‘œ ìˆ˜ìµë¥ ì— ëŒ€í•´ ìµœì†Œ ë¶„ì‚° í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬í•˜ë©´, ìˆ˜ìµë¥ -ë³€ë™ì„± í‰ë©´ì— ê³¡ì„ ì´ ê·¸ë ¤ì§„ë‹¤. ì´ê²ƒì´ <strong>íš¨ìœ¨ì  í”„ë¡ í‹°ì–´</strong>(Efficient Frontier)ë‹¤. ì´ ê³¡ì„  ìœ„ì˜ í¬íŠ¸í´ë¦¬ì˜¤ë§Œì´ "ê°™ì€ ë¦¬ìŠ¤í¬ì—ì„œ ìµœëŒ€ ìˆ˜ìµ" ë˜ëŠ” "ê°™ì€ ìˆ˜ìµì—ì„œ ìµœì†Œ ë¦¬ìŠ¤í¬"ë¥¼ ë‹¬ì„±í•˜ëŠ” íš¨ìœ¨ì  í¬íŠ¸í´ë¦¬ì˜¤ë‹¤.
</p>

<p class="cc">â–¼ íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê·¸ë¦¬ê¸° (CVXPY + Matplotlib)</p>
<pre>
<span class="kw">import</span> cvxpy <span class="kw">as</span> cp
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="cm"># 5ì¢…ëª© ë°ì´í„°</span>
tickers = [<span class="st">'AAPL'</span>, <span class="st">'MSFT'</span>, <span class="st">'GOOGL'</span>, <span class="st">'AMZN'</span>, <span class="st">'TSLA'</span>]
data = yf.<span class="fn">download</span>(tickers, start=<span class="st">'2020-01-01'</span>, end=<span class="st">'2024-01-01'</span>)
prices = data[<span class="st">'Close'</span>]
returns = prices.<span class="fn">pct_change</span>().<span class="fn">dropna</span>()

mu = returns.<span class="fn">mean</span>().<span class="fn">values</span> * <span class="nu">252</span>
Sigma = returns.<span class="fn">cov</span>().<span class="fn">values</span> * <span class="nu">252</span>
n = <span class="fn">len</span>(tickers)

<span class="cm"># íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ê³„ì‚°</span>
target_returns = np.<span class="fn">linspace</span>(mu.<span class="fn">min</span>(), mu.<span class="fn">max</span>(), <span class="nu">50</span>)
frontier_risk = []
frontier_ret = []

<span class="kw">for</span> target <span class="kw">in</span> target_returns:
    w = cp.<span class="fn">Variable</span>(n)
    objective = cp.<span class="fn">Minimize</span>(cp.<span class="fn">quad_form</span>(w, Sigma))
    constraints = [
        cp.<span class="fn">sum</span>(w) == <span class="nu">1</span>,
        w >= <span class="nu">0</span>,
        mu @ w >= target
    ]
    prob = cp.<span class="fn">Problem</span>(objective, constraints)
    prob.<span class="fn">solve</span>()
    <span class="kw">if</span> prob.status == <span class="st">'optimal'</span>:
        frontier_risk.<span class="fn">append</span>(np.<span class="fn">sqrt</span>(prob.value))
        frontier_ret.<span class="fn">append</span>(target)

<span class="cm"># ì‹œê°í™”</span>
plt.<span class="fn">figure</span>(figsize=(<span class="nu">10</span>, <span class="nu">6</span>))
plt.<span class="fn">plot</span>(frontier_risk, frontier_ret, <span class="st">'b-'</span>, linewidth=<span class="nu">2</span>,
         label=<span class="st">'Efficient Frontier'</span>)

<span class="cm"># ê°œë³„ ì¢…ëª© í‘œì‹œ</span>
<span class="kw">for</span> i, ticker <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    std_i = np.<span class="fn">sqrt</span>(Sigma[i, i])
    plt.<span class="fn">scatter</span>(std_i, mu[i], s=<span class="nu">80</span>, zorder=<span class="nu">5</span>)
    plt.<span class="fn">annotate</span>(ticker, (std_i, mu[i]),
                fontsize=<span class="nu">10</span>, fontweight=<span class="st">'bold'</span>,
                xytext=(<span class="nu">5</span>, <span class="nu">5</span>), textcoords=<span class="st">'offset points'</span>)

plt.<span class="fn">xlabel</span>(<span class="st">'Annual Volatility (Ïƒ)'</span>)
plt.<span class="fn">ylabel</span>(<span class="st">'Annual Return (Î¼)'</span>)
plt.<span class="fn">title</span>(<span class="st">'Efficient Frontier â€” Markowitz Mean-Variance'</span>)
plt.<span class="fn">legend</span>()
plt.<span class="fn">grid</span>(<span class="kw">True</span>, alpha=<span class="nu">0.3</span>)
plt.<span class="fn">tight_layout</span>()
plt.<span class="fn">show</span>()
</pre>

<h3>5.4 ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” í¬íŠ¸í´ë¦¬ì˜¤</h3>

<p>
íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ ìœ„ì—ì„œ ê°€ì¥ "íš¨ìœ¨ì ì¸" í¬íŠ¸í´ë¦¬ì˜¤ëŠ” <strong>ìƒ¤í”„ë¹„ìœ¨ì´ ìµœëŒ€ì¸ í¬íŠ¸í´ë¦¬ì˜¤</strong>ë‹¤. ì´ê²ƒì€ ì›ì ì—ì„œ íš¨ìœ¨ì  í”„ë¡ í‹°ì–´ì— ì ‘ì„ ì„ ê·¸ì—ˆì„ ë•Œì˜ ì ‘ì ì´ë©°, <strong>ì ‘ì„  í¬íŠ¸í´ë¦¬ì˜¤</strong>(Tangency Portfolio)ë¼ ë¶ˆë¦°ë‹¤. R1ì—ì„œ ë°°ìš´ ìƒ¤í”„ë¹„ìœ¨ì´ ì—¬ê¸°ì„œ í•µì‹¬ ì—­í• ì„ í•œë‹¤.
</p>

<p>
ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™”ëŠ” ì§ì ‘ì ìœ¼ë¡œëŠ” ë¹„ë³¼ë¡ ë¬¸ì œì§€ë§Œ, ë³€ìˆ˜ ë³€í™˜ íŠ¸ë¦­ìœ¼ë¡œ ë³¼ë¡ ë¬¸ì œë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤:
</p>

<p class="cc">â–¼ ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” (CVXPY)</p>
<pre>
<span class="cm"># ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” â€” ë³€ìˆ˜ ë³€í™˜ íŠ¸ë¦­</span>
<span class="cm"># y = w/k, k = 1/(1^T Î£^{-1} (Î¼ - rf))</span>
rf = <span class="nu">0.05</span>  <span class="cm"># ë¬´ìœ„í—˜ ì´ììœ¨ 5%</span>

y = cp.<span class="fn">Variable</span>(n)
kappa = cp.<span class="fn">Variable</span>()  <span class="cm"># ìŠ¤ì¼€ì¼ë§ ë³€ìˆ˜</span>

objective = cp.<span class="fn">Minimize</span>(cp.<span class="fn">quad_form</span>(y, Sigma))
constraints = [
    (mu - rf) @ y == <span class="nu">1</span>,  <span class="cm"># ì´ˆê³¼ìˆ˜ìµë¥  ì •ê·œí™”</span>
    cp.<span class="fn">sum</span>(y) == kappa,
    y >= <span class="nu">0</span>,
    kappa >= <span class="nu">0</span>
]

prob = cp.<span class="fn">Problem</span>(objective, constraints)
prob.<span class="fn">solve</span>()

<span class="cm"># ì›ë˜ ë¹„ì¤‘ìœ¼ë¡œ ë³µì›</span>
w_tangency = y.value / kappa.value

<span class="fn">print</span>(<span class="st">"=== ìµœëŒ€ ìƒ¤í”„ë¹„ìœ¨ í¬íŠ¸í´ë¦¬ì˜¤ ==="</span>)
<span class="kw">for</span> i, ticker <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {ticker}: {w_tangency[i]*100:.1f}%"</span>)

ret_t = mu @ w_tangency
vol_t = np.<span class="fn">sqrt</span>(w_tangency @ Sigma @ w_tangency)
sr_t = (ret_t - rf) / vol_t
<span class="fn">print</span>(<span class="st">f"\nìƒ¤í”„ë¹„ìœ¨: {sr_t:.2f}"</span>)
</pre>

<h3>5.5 Markowitzì˜ í•œê³„</h3>

<p>
Mean-Variance ìµœì í™”ëŠ” ìš°ì•„í•˜ì§€ë§Œ ì‹¤ì „ì—ì„œ ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆë‹¤:
</p>

<ul>
<li><strong>ì¶”ì • ì˜¤ì°¨ì— ê·¹ë„ë¡œ ë¯¼ê°:</strong> ê¸°ëŒ€ìˆ˜ìµë¥  \(\mu\)ì˜ ì‘ì€ ë³€í™”ê°€ ë¹„ì¤‘ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ë°”ê¾¼ë‹¤. "Garbage In, Garbage Out"ì˜ ì „í˜•ì  ì‚¬ë¡€ë‹¤.</li>
<li><strong>ì§‘ì¤‘ íˆ¬ì ê²½í–¥:</strong> ì†Œìˆ˜ ì¢…ëª©ì— ë¹„ì¤‘ì´ ëª°ë¦¬ëŠ” ê²½í–¥ì´ ìˆì–´ ë¶„ì‚° íˆ¬ìì˜ ì·¨ì§€ì— ì–´ê¸‹ë‚œë‹¤.</li>
<li><strong>ê¸°ëŒ€ìˆ˜ìµë¥  ì¶”ì •ì˜ ì–´ë ¤ì›€:</strong> ê³¼ê±° ìˆ˜ìµë¥ ì´ ë¯¸ë˜ë¥¼ ëŒ€í‘œí•œë‹¤ëŠ” ë³´ì¥ì´ ì—†ë‹¤.</li>
</ul>

<p>
ì´ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ Black-Litterman, Risk Parity, HRP ë“±ì˜ ëŒ€ì•ˆì´ ë“±ì¥í–ˆë‹¤. ë‹¤ìŒ ì±•í„°ë“¤ì—ì„œ í•˜ë‚˜ì”© ë‹¤ë£¬ë‹¤.
</p>

<div class="warn">
<p class="ni"><strong>âš ï¸ ì‹¤ì „ ì£¼ì˜:</strong> Markowitz ìµœì í™”ë¥¼ ê·¸ëŒ€ë¡œ ì‹¤ì „ì— ì ìš©í•˜ë©´ ì•ˆ ëœë‹¤. ì¶”ì • ì˜¤ì°¨ ë•Œë¬¸ì— ë°±í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì¢‹ì•„ ë³´ì´ì§€ë§Œ ì‹¤ì „ì—ì„œëŠ” ì„±ê³¼ê°€ ë‚˜ìœ ê²½ìš°ê°€ ë§ë‹¤. ë°˜ë“œì‹œ ì •ê·œí™”(shrinkage), ë¦¬ìƒ˜í”Œë§, ë˜ëŠ” ì•„ë˜ì—ì„œ ë°°ìš¸ ëŒ€ì•ˆ ê¸°ë²•ê³¼ ê²°í•©í•´ì•¼ í•œë‹¤.</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 6: Black-Litterman ëª¨ë¸
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch6">Chapter 6. Black-Litterman ëª¨ë¸ â€” ì‹œì¥ ê· í˜• + íˆ¬ìì ì „ë§</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.5 "The Black-Litterman approach" ì„¹ì…˜ â€” ì‹œì¥ ê· í˜• ìˆ˜ìµë¥ ì—ì„œ ì¶œë°œí•˜ì—¬ íˆ¬ììì˜ ì£¼ê´€ì  ì „ë§ì„ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¬ë‹¤.</p>
</div>

<h3>6.1 Markowitzì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì•„ì´ë””ì–´</h3>

<p>
Markowitzì˜ ê°€ì¥ í° ë¬¸ì œëŠ” ê¸°ëŒ€ìˆ˜ìµë¥  \(\mu\)ë¥¼ ì–´ë–»ê²Œ ì¶”ì •í•˜ëŠëƒì˜€ë‹¤. ê³¼ê±° í‰ê·  ìˆ˜ìµë¥ ? ë„ˆë¬´ ë¶ˆì•ˆì •í•˜ë‹¤. ì• ë„ë¦¬ìŠ¤íŠ¸ ì „ë§? ì£¼ê´€ì ì´ë‹¤. 1992ë…„, Goldman Sachsì˜ Fischer Blackê³¼ Robert Littermanì´ ìš°ì•„í•œ í•´ê²°ì±…ì„ ì œì‹œí–ˆë‹¤.
</p>

<p>
Black-Littermanì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‘ ë‹¨ê³„ë‹¤:
</p>

<ol>
<li><strong>ì‹œì¥ ê· í˜• ìˆ˜ìµë¥ ì—ì„œ ì¶œë°œ:</strong> í˜„ì¬ ì‹œì¥ì˜ ì‹œê°€ì´ì•¡ ë¹„ì¤‘ì´ "ìµœì "ì´ë¼ê³  ê°€ì •í•˜ê³ , ì´ë¥¼ ì—­ìœ¼ë¡œ í’€ì–´ ì‹œì¥ì´ ì•”ë¬µì ìœ¼ë¡œ ê¸°ëŒ€í•˜ëŠ” ìˆ˜ìµë¥ (implied equilibrium returns)ì„ êµ¬í•œë‹¤.</li>
<li><strong>íˆ¬ììì˜ ì „ë§ì„ ê²°í•©:</strong> "ì‚¼ì„±ì „ìê°€ ì‹œì¥ ëŒ€ë¹„ 2% ì´ˆê³¼ ìˆ˜ìµì„ ë‚¼ ê²ƒì´ë‹¤" ê°™ì€ ì£¼ê´€ì  ì „ë§ì„ ë² ì´ì¦ˆ ì •ë¦¬ë¡œ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ê¸°ëŒ€ìˆ˜ìµë¥ ì„ ë§Œë“ ë‹¤.</li>
</ol>

<p>
ë¹„ìœ í•˜ìë©´, MarkowitzëŠ” "ë°±ì§€ì—ì„œ ì‹œì‘"í•˜ëŠ” ê²ƒì´ê³ , Black-Littermanì€ "ì‹œì¥ì˜ ì§€í˜œì—ì„œ ì‹œì‘í•˜ì—¬ ë‚´ ì˜ê²¬ì„ ë°˜ì˜"í•˜ëŠ” ê²ƒì´ë‹¤. ì¶œë°œì ì´ ì•ˆì •ì ì´ë¯€ë¡œ ê²°ê³¼ë„ í›¨ì”¬ ì•ˆì •ì ì´ë‹¤.
</p>

<h3>6.2 ìˆ˜í•™ì  êµ¬ì¡°</h3>

<p>
<strong>Step 1: ê· í˜• ìˆ˜ìµë¥  (Implied Equilibrium Returns)</strong>
</p>

<p>
ì‹œì¥ ì‹œê°€ì´ì•¡ ë¹„ì¤‘ \(w_{\text{mkt}}\)ì´ Mean-Variance ìµœì í•´ë¼ê³  ê°€ì •í•˜ë©´, 1ì°¨ ì¡°ê±´ì—ì„œ:
</p>

<div class="eq">
$$\Pi = \delta \Sigma w_{\text{mkt}}$$
</div>

<p>
ì—¬ê¸°ì„œ \(\Pi\)ëŠ” ê· í˜• ê¸°ëŒ€ìˆ˜ìµë¥ , \(\delta\)ëŠ” ìœ„í—˜íšŒí”¼ê³„ìˆ˜(ë³´í†µ ì‹œì¥ ìƒ¤í”„ë¹„ìœ¨ë¡œ ì¶”ì •), \(\Sigma\)ëŠ” ê³µë¶„ì‚° í–‰ë ¬ì´ë‹¤.
</p>

<p>
<strong>Step 2: íˆ¬ìì ì „ë§ ê²°í•©</strong>
</p>

<p>
íˆ¬ììì˜ ì „ë§ì„ í–‰ë ¬ í˜•íƒœë¡œ í‘œí˜„í•œë‹¤: \(P \cdot \mu = Q + \epsilon\), ì—¬ê¸°ì„œ \(P\)ëŠ” ì „ë§ í–‰ë ¬, \(Q\)ëŠ” ì „ë§ ìˆ˜ìµë¥ , \(\Omega\)ëŠ” ì „ë§ì˜ ë¶ˆí™•ì‹¤ì„±ì´ë‹¤. ë² ì´ì¦ˆ ê²°í•©ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸°ëŒ€ìˆ˜ìµë¥ ì„ êµ¬í•œë‹¤:
</p>

<div class="eq">
$$\mu_{\text{BL}} = [(\tau\Sigma)^{-1} + P^T \Omega^{-1} P]^{-1} [(\tau\Sigma)^{-1}\Pi + P^T \Omega^{-1} Q]$$
</div>

<p>
\(\tau\)ëŠ” ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°(ë³´í†µ 0.025~0.05)ë¡œ, ê· í˜• ìˆ˜ìµë¥ ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤.
</p>

<h3>6.3 íŒŒì´ì¬ êµ¬í˜„</h3>

<p class="cc">â–¼ Black-Litterman ëª¨ë¸ êµ¬í˜„</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> yfinance <span class="kw">as</span> yf

tickers = [<span class="st">'AAPL'</span>, <span class="st">'MSFT'</span>, <span class="st">'GOOGL'</span>, <span class="st">'AMZN'</span>, <span class="st">'TSLA'</span>]
data = yf.<span class="fn">download</span>(tickers, start=<span class="st">'2020-01-01'</span>, end=<span class="st">'2024-01-01'</span>)
prices = data[<span class="st">'Close'</span>]
returns = prices.<span class="fn">pct_change</span>().<span class="fn">dropna</span>()
Sigma = returns.<span class="fn">cov</span>().<span class="fn">values</span> * <span class="nu">252</span>
n = <span class="fn">len</span>(tickers)

<span class="cm"># Step 1: ì‹œì¥ ê· í˜• ìˆ˜ìµë¥ </span>
w_mkt = np.<span class="fn">array</span>([<span class="nu">0.30</span>, <span class="nu">0.25</span>, <span class="nu">0.20</span>, <span class="nu">0.15</span>, <span class="nu">0.10</span>])  <span class="cm"># ì‹œê°€ì´ì•¡ ë¹„ì¤‘ (ì˜ˆì‹œ)</span>
delta = <span class="nu">2.5</span>  <span class="cm"># ìœ„í—˜íšŒí”¼ê³„ìˆ˜</span>
Pi = delta * Sigma @ w_mkt  <span class="cm"># ê· í˜• ê¸°ëŒ€ìˆ˜ìµë¥ </span>

<span class="fn">print</span>(<span class="st">"ê· í˜• ê¸°ëŒ€ìˆ˜ìµë¥ :"</span>)
<span class="kw">for</span> i, t <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {t}: {Pi[i]*100:.1f}%"</span>)

<span class="cm"># Step 2: íˆ¬ìì ì „ë§</span>
<span class="cm"># ì „ë§ 1: AAPLì´ TSLAë³´ë‹¤ 3% ë†’ì€ ìˆ˜ìµë¥ </span>
<span class="cm"># ì „ë§ 2: MSFT ì ˆëŒ€ ìˆ˜ìµë¥  15%</span>
tau = <span class="nu">0.05</span>
P = np.<span class="fn">array</span>([
    [<span class="nu">1</span>, <span class="nu">0</span>, <span class="nu">0</span>, <span class="nu">0</span>, <span class="nu">-1</span>],  <span class="cm"># AAPL - TSLA</span>
    [<span class="nu">0</span>, <span class="nu">1</span>, <span class="nu">0</span>, <span class="nu">0</span>,  <span class="nu">0</span>],  <span class="cm"># MSFT ì ˆëŒ€</span>
])
Q = np.<span class="fn">array</span>([<span class="nu">0.03</span>, <span class="nu">0.15</span>])  <span class="cm"># ì „ë§ ìˆ˜ìµë¥ </span>

<span class="cm"># ì „ë§ ë¶ˆí™•ì‹¤ì„± (He & Litterman ë°©ì‹)</span>
Omega = np.<span class="fn">diag</span>(np.<span class="fn">diag</span>(tau * P @ Sigma @ P.T))

<span class="cm"># Black-Litterman ê²°í•©</span>
tau_Sigma_inv = np.linalg.<span class="fn">inv</span>(tau * Sigma)
M = np.linalg.<span class="fn">inv</span>(tau_Sigma_inv + P.T @ np.linalg.<span class="fn">inv</span>(Omega) @ P)
mu_BL = M @ (tau_Sigma_inv @ Pi + P.T @ np.linalg.<span class="fn">inv</span>(Omega) @ Q)

<span class="fn">print</span>(<span class="st">"\nBlack-Litterman ê¸°ëŒ€ìˆ˜ìµë¥ :"</span>)
<span class="kw">for</span> i, t <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {t}: {mu_BL[i]*100:.1f}%"</span>)

<span class="cm"># BL ìˆ˜ìµë¥ ë¡œ ìµœì  ë¹„ì¤‘ ê³„ì‚°</span>
w_BL = np.linalg.<span class="fn">inv</span>(delta * Sigma) @ mu_BL
w_BL = w_BL / w_BL.<span class="fn">sum</span>()  <span class="cm"># ì •ê·œí™”</span>

<span class="fn">print</span>(<span class="st">"\nBL ìµœì  ë¹„ì¤‘:"</span>)
<span class="kw">for</span> i, t <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {t}: {w_BL[i]*100:.1f}%"</span>)
</pre>

<div class="ok">
<p class="ni"><strong>ğŸ’¡ Black-Littermanì˜ ì¥ì :</strong> (1) ì‹œì¥ ê· í˜•ì—ì„œ ì¶œë°œí•˜ë¯€ë¡œ ê²°ê³¼ê°€ ì•ˆì •ì ì´ë‹¤. (2) ì „ë§ì´ ì—†ìœ¼ë©´ ì‹œì¥ ë¹„ì¤‘ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤. (3) ì „ë§ì˜ í™•ì‹ ë„ë¥¼ \(\Omega\)ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. (4) ì‹¤ë¬´ì—ì„œ Goldman Sachs, BlackRock ë“± ëŒ€í˜• ê¸°ê´€ì´ ì‹¤ì œë¡œ ì‚¬ìš©í•œë‹¤.</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 7: Risk Parity
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch7">Chapter 7. Risk Parity â€” ë¦¬ìŠ¤í¬ ê· ë“± ë°°ë¶„</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.5 "Risk parity" ì„¹ì…˜ â€” ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ë¥¼ ê· ë“±í•˜ê²Œ ë§Œë“œëŠ” í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ë²•. Bridgewaterì˜ Ray Dalioê°€ ëŒ€ì¤‘í™”í•œ ì „ëµì´ë‹¤.</p>
</div>

<h3>7.1 ì™œ Risk Parityì¸ê°€?</h3>

<p>
ì „í†µì ì¸ 60/40 í¬íŠ¸í´ë¦¬ì˜¤(ì£¼ì‹ 60%, ì±„ê¶Œ 40%)ë¥¼ ìƒê°í•´ë³´ì. ê¸ˆì•¡ ê¸°ì¤€ìœ¼ë¡œëŠ” ê· í˜• ì¡í˜€ ë³´ì´ì§€ë§Œ, <strong>ë¦¬ìŠ¤í¬ ê¸°ì¤€ìœ¼ë¡œëŠ” ê·¹ë„ë¡œ ë¶ˆê· í˜•</strong>í•˜ë‹¤. ì£¼ì‹ì˜ ë³€ë™ì„±ì´ ì±„ê¶Œë³´ë‹¤ 3~4ë°° ë†’ìœ¼ë¯€ë¡œ, í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ì˜ 90% ì´ìƒì´ ì£¼ì‹ì—ì„œ ì˜¨ë‹¤. ì±„ê¶Œì€ ì‚¬ì‹¤ìƒ ì¥ì‹ì´ë‹¤.
</p>

<p>
Risk Parityì˜ ì•„ì´ë””ì–´ëŠ” ë‹¨ìˆœí•˜ë‹¤: <strong>ê° ìì‚°ì´ í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ë¦¬ìŠ¤í¬ì— ë™ì¼í•˜ê²Œ ê¸°ì—¬í•˜ë„ë¡ ë¹„ì¤‘ì„ ì¡°ì ˆí•œë‹¤.</strong> ê¸ˆì•¡ì´ ì•„ë‹Œ ë¦¬ìŠ¤í¬ë¥¼ ê· ë“± ë°°ë¶„í•˜ëŠ” ê²ƒì´ë‹¤.
</p>

<h3>7.2 ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ (Risk Contribution)</h3>

<p>
ìì‚° \(i\)ì˜ ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„(Risk Contribution, RC)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:
</p>

<div class="eq">
$$RC_i = w_i \cdot \frac{(\Sigma w)_i}{\sqrt{w^T \Sigma w}} = w_i \cdot \frac{\partial \sigma_p}{\partial w_i}$$
</div>

<p>
ëª¨ë“  ìì‚°ì˜ ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ì˜ í•©ì€ í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ë³€ë™ì„±ê³¼ ê°™ë‹¤: \(\sum RC_i = \sigma_p\). Risk ParityëŠ” ëª¨ë“  \(RC_i\)ë¥¼ ê°™ê²Œ ë§Œë“œëŠ” ë¹„ì¤‘ \(w\)ë¥¼ ì°¾ëŠ”ë‹¤.
</p>

<h3>7.3 íŒŒì´ì¬ êµ¬í˜„</h3>

<p class="cc">â–¼ Risk Parity í¬íŠ¸í´ë¦¬ì˜¤ (scipy ìµœì í™”)</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> scipy.optimize <span class="kw">import</span> minimize

<span class="kw">def</span> <span class="fn">risk_parity_objective</span>(w, Sigma):
    <span class="st">"""ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ í¸ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ëª©ì í•¨ìˆ˜"""</span>
    port_vol = np.<span class="fn">sqrt</span>(w @ Sigma @ w)
    marginal_risk = Sigma @ w / port_vol  <span class="cm"># í•œê³„ ë¦¬ìŠ¤í¬</span>
    risk_contrib = w * marginal_risk       <span class="cm"># ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„</span>
    target_rc = port_vol / <span class="fn">len</span>(w)          <span class="cm"># ëª©í‘œ: ê· ë“± ê¸°ì—¬</span>
    <span class="cm"># ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ í¸ì°¨ì˜ ì œê³±í•©</span>
    <span class="kw">return</span> np.<span class="fn">sum</span>((risk_contrib - target_rc) ** <span class="nu">2</span>)

<span class="kw">def</span> <span class="fn">get_risk_parity_weights</span>(Sigma):
    <span class="st">"""Risk Parity ë¹„ì¤‘ ê³„ì‚°"""</span>
    n = Sigma.shape[<span class="nu">0</span>]
    w0 = np.<span class="fn">ones</span>(n) / n  <span class="cm"># ì´ˆê¸°ê°’: ê· ë“± ë¹„ì¤‘</span>
    bounds = [(<span class="nu">0.01</span>, <span class="nu">1.0</span>)] * n
    constraints = {<span class="st">'type'</span>: <span class="st">'eq'</span>, <span class="st">'fun'</span>: <span class="kw">lambda</span> w: np.<span class="fn">sum</span>(w) - <span class="nu">1</span>}

    result = <span class="fn">minimize</span>(
        risk_parity_objective, w0,
        args=(Sigma,),
        method=<span class="st">'SLSQP'</span>,
        bounds=bounds,
        constraints=constraints
    )
    <span class="kw">return</span> result.x

<span class="cm"># ì‹¤í–‰</span>
w_rp = <span class="fn">get_risk_parity_weights</span>(Sigma)

<span class="fn">print</span>(<span class="st">"=== Risk Parity í¬íŠ¸í´ë¦¬ì˜¤ ==="</span>)
<span class="kw">for</span> i, t <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {t}: {w_rp[i]*100:.1f}%"</span>)

<span class="cm"># ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ í™•ì¸</span>
port_vol = np.<span class="fn">sqrt</span>(w_rp @ Sigma @ w_rp)
marginal = Sigma @ w_rp / port_vol
rc = w_rp * marginal
<span class="fn">print</span>(<span class="st">f"\në¦¬ìŠ¤í¬ ê¸°ì—¬ë„: {np.round(rc / rc.sum() * 100, 1)}%"</span>)
<span class="fn">print</span>(<span class="st">"â†’ ëª¨ë“  ìì‚°ì´ ì•½ 20%ì”© ê· ë“± ê¸°ì—¬!"</span>)
</pre>

<p>
Risk Parityì˜ ê²°ê³¼ë¥¼ ë³´ë©´, ë³€ë™ì„±ì´ ë†’ì€ TSLAì˜ ë¹„ì¤‘ì€ ë‚®ê³ , ë³€ë™ì„±ì´ ë‚®ì€ ì¢…ëª©ì˜ ë¹„ì¤‘ì€ ë†’ë‹¤. í•˜ì§€ë§Œ ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ëŠ” ëª¨ë‘ ì•½ 20%ë¡œ ê· ë“±í•˜ë‹¤. ì´ê²ƒì´ Risk Parityì˜ í•µì‹¬ì´ë‹¤.
</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 8: HRP (Hierarchical Risk Parity)
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch8">Chapter 8. Hierarchical Risk Parity (HRP) â€” ML ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.5 "Hierarchical Risk Parity" ì„¹ì…˜ â€” Marcos LÃ³pez de Pradoê°€ 2016ë…„ì— ì œì•ˆí•œ ML ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”. ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ì¶”ì • ì˜¤ì°¨ì— ê°•ê±´í•˜ë‹¤.</p>
</div>

<h3>8.1 ê³µë¶„ì‚° í–‰ë ¬ ì—­í–‰ë ¬ì˜ ì €ì£¼</h3>

<p>
Markowitzì™€ Black-Litterman ëª¨ë‘ ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ \(\Sigma^{-1}\)ì„ í•„ìš”ë¡œ í•œë‹¤. ë¬¸ì œëŠ” ì¢…ëª© ìˆ˜ê°€ ë§ì•„ì§€ë©´ ê³µë¶„ì‚° í–‰ë ¬ì˜ ì¶”ì • ì˜¤ì°¨ê°€ ì»¤ì§€ê³ , ì—­í–‰ë ¬ì´ ì´ ì˜¤ì°¨ë¥¼ <strong>ì¦í­</strong>ì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ë‹¤. R2ì—ì„œ ë°°ìš´ ì¡°ê±´ìˆ˜(condition number)ê°€ í¬ë©´ ì—­í–‰ë ¬ì´ ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì.
</p>

<p>
Marcos LÃ³pez de Prado(í€€íŠ¸ ê¸ˆìœµì˜ ëŒ€ê°€, Cornell êµìˆ˜)ëŠ” ì´ ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤: <strong>ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ì•„ì˜ˆ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”</strong> í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”. ëŒ€ì‹  R5ì—ì„œ ë°°ìš´ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì„ í™œìš©í•œë‹¤.
</p>

<h3>8.2 HRP ì•Œê³ ë¦¬ì¦˜ 3ë‹¨ê³„</h3>

<!-- HRP 3ë‹¨ê³„ ì‹œê° ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e8eaf6,#f3e5f5);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:18px;color:#4527a0">ğŸŒ³ HRP ì•Œê³ ë¦¬ì¦˜ 3ë‹¨ê³„</p>
<div style="display:flex;gap:12px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:180px;background:#fff;padding:16px;border-radius:10px;text-align:center;border-top:4px solid #7c4dff">
<div style="font-size:32px;margin-bottom:8px">ğŸ”—</div>
<p class="ni" style="font-weight:bold;font-size:13px;color:#4527a0;margin-bottom:6px">Step 1: Tree Clustering</p>
<p class="ni" style="font-size:11px;color:#666">ìƒê´€ê´€ê³„ ê¸°ë°˜ìœ¼ë¡œ ì¢…ëª©ì„ ê³„ì¸µì ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§. ìœ ì‚¬í•œ ì¢…ëª©ë¼ë¦¬ ê·¸ë£¹í™”.</p>
</div>
<div style="flex:0 0 30px;display:flex;align-items:center;justify-content:center;font-size:24px;color:#7c4dff">â†’</div>
<div style="flex:1;min-width:180px;background:#fff;padding:16px;border-radius:10px;text-align:center;border-top:4px solid #7c4dff">
<div style="font-size:32px;margin-bottom:8px">ğŸ”€</div>
<p class="ni" style="font-weight:bold;font-size:13px;color:#4527a0;margin-bottom:6px">Step 2: Quasi-Diagonalization</p>
<p class="ni" style="font-size:11px;color:#666">ê³µë¶„ì‚° í–‰ë ¬ì„ í´ëŸ¬ìŠ¤í„° ìˆœì„œë¡œ ì¬ë°°ì—´. ìœ ì‚¬ ì¢…ëª©ì´ ì¸ì ‘í•˜ë„ë¡.</p>
</div>
<div style="flex:0 0 30px;display:flex;align-items:center;justify-content:center;font-size:24px;color:#7c4dff">â†’</div>
<div style="flex:1;min-width:180px;background:#fff;padding:16px;border-radius:10px;text-align:center;border-top:4px solid #7c4dff">
<div style="font-size:32px;margin-bottom:8px">âš–ï¸</div>
<p class="ni" style="font-weight:bold;font-size:13px;color:#4527a0;margin-bottom:6px">Step 3: Recursive Bisection</p>
<p class="ni" style="font-size:11px;color:#666">íŠ¸ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì´ë“±ë¶„í•˜ë©° ë¶„ì‚°ì˜ ì—­ìˆ˜ ë¹„ë¡€ë¡œ ë¹„ì¤‘ ë°°ë¶„.</p>
</div>
</div>
</div>

<h3>8.3 íŒŒì´ì¬ êµ¬í˜„</h3>

<p class="cc">â–¼ HRP í¬íŠ¸í´ë¦¬ì˜¤ (scipy + ì§ì ‘ êµ¬í˜„)</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">from</span> scipy.cluster.hierarchy <span class="kw">import</span> linkage, leaves_list
<span class="kw">from</span> scipy.spatial.distance <span class="kw">import</span> squareform

<span class="kw">def</span> <span class="fn">get_hrp_weights</span>(returns):
    <span class="st">"""Hierarchical Risk Parity ë¹„ì¤‘ ê³„ì‚°"""</span>
    cov = returns.<span class="fn">cov</span>()
    corr = returns.<span class="fn">corr</span>()

    <span class="cm"># Step 1: ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§</span>
    dist = np.<span class="fn">sqrt</span>((<span class="nu">1</span> - corr) / <span class="nu">2</span>)  <span class="cm"># ìƒê´€â†’ê±°ë¦¬ ë³€í™˜</span>
    dist_condensed = <span class="fn">squareform</span>(dist, checks=<span class="kw">False</span>)
    link = <span class="fn">linkage</span>(dist_condensed, method=<span class="st">'single'</span>)

    <span class="cm"># Step 2: ì¤€ëŒ€ê°í™” (í´ëŸ¬ìŠ¤í„° ìˆœì„œë¡œ ì¬ë°°ì—´)</span>
    sort_idx = <span class="fn">leaves_list</span>(link).<span class="fn">tolist</span>()
    cov_sorted = cov.<span class="fn">iloc</span>[sort_idx, sort_idx]

    <span class="cm"># Step 3: ì¬ê·€ì  ì´ë“±ë¶„</span>
    weights = pd.<span class="fn">Series</span>(<span class="nu">1.0</span>, index=cov_sorted.index)
    clusters = [cov_sorted.index.<span class="fn">tolist</span>()]

    <span class="kw">while</span> <span class="fn">len</span>(clusters) > <span class="nu">0</span>:
        clusters_new = []
        <span class="kw">for</span> cluster <span class="kw">in</span> clusters:
            <span class="kw">if</span> <span class="fn">len</span>(cluster) <= <span class="nu">1</span>:
                <span class="kw">continue</span>
            <span class="cm"># ì´ë“±ë¶„</span>
            mid = <span class="fn">len</span>(cluster) // <span class="nu">2</span>
            left, right = cluster[:mid], cluster[mid:]

            <span class="cm"># ê° í´ëŸ¬ìŠ¤í„°ì˜ ë¶„ì‚° (ì—­ë¶„ì‚° ë¹„ì¤‘)</span>
            var_left = cov.<span class="fn">loc</span>[left, left].<span class="fn">values</span>.<span class="fn">diagonal</span>().<span class="fn">mean</span>()
            var_right = cov.<span class="fn">loc</span>[right, right].<span class="fn">values</span>.<span class="fn">diagonal</span>().<span class="fn">mean</span>()

            <span class="cm"># ë¶„ì‚°ì˜ ì—­ìˆ˜ ë¹„ë¡€ë¡œ ë°°ë¶„</span>
            alpha = <span class="nu">1</span> - var_left / (var_left + var_right)
            weights[left] *= alpha
            weights[right] *= (<span class="nu">1</span> - alpha)

            <span class="kw">if</span> <span class="fn">len</span>(left) > <span class="nu">1</span>: clusters_new.<span class="fn">append</span>(left)
            <span class="kw">if</span> <span class="fn">len</span>(right) > <span class="nu">1</span>: clusters_new.<span class="fn">append</span>(right)
        clusters = clusters_new

    <span class="kw">return</span> weights / weights.<span class="fn">sum</span>()

<span class="cm"># ì‹¤í–‰</span>
w_hrp = <span class="fn">get_hrp_weights</span>(returns)
<span class="fn">print</span>(<span class="st">"=== HRP í¬íŠ¸í´ë¦¬ì˜¤ ==="</span>)
<span class="kw">for</span> t, w <span class="kw">in</span> w_hrp.<span class="fn">items</span>():
    <span class="fn">print</span>(<span class="st">f"  {t}: {w*100:.1f}%"</span>)
</pre>

<div class="ok">
<p class="ni"><strong>ğŸ’¡ HRPì˜ ì¥ì :</strong> (1) ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ ë¶ˆí•„ìš” â†’ ì¶”ì • ì˜¤ì°¨ì— ê°•ê±´. (2) í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ìì‚° ê°„ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë°˜ì˜. (3) Markowitzë³´ë‹¤ out-of-sample ì„±ê³¼ê°€ ìš°ìˆ˜í•œ ê²½ìš°ê°€ ë§ìŒ (LÃ³pez de Prado, 2016). (4) R5ì—ì„œ ë°°ìš´ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì˜ ì‹¤ì „ ì‘ìš©.</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 9: Kelly Criterion
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch9">Chapter 9. Kelly Criterion â€” ìµœì  ë² íŒ… ì‚¬ì´ì¦ˆ</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.5 "The Kelly criterion" ì„¹ì…˜ â€” ì •ë³´ ì´ë¡ ì—ì„œ ì¶œë°œí•œ ìµœì  ë² íŒ… ì „ëµ. Edward Thorpê°€ ë¸”ë™ì­ê³¼ ì£¼ì‹ì‹œì¥ì— ì ìš©í•˜ì—¬ ìœ ëª…í•´ì¡Œë‹¤.</p>
</div>

<h3>9.1 ë„ë°•ì—ì„œ íˆ¬ìë¡œ</h3>

<p>
1956ë…„, Bell Labsì˜ ë¬¼ë¦¬í•™ì John Kelly Jr.ëŠ” ì •ë³´ ì´ë¡ ì„ ì´ìš©í•˜ì—¬ "ì¥ê¸°ì ìœ¼ë¡œ ìì‚°ì„ ìµœëŒ€í™”í•˜ëŠ” ìµœì  ë² íŒ… ë¹„ìœ¨"ì„ ë„ì¶œí–ˆë‹¤. ì´í›„ MIT ìˆ˜í•™ êµìˆ˜ Edward Thorpê°€ ì´ë¥¼ ë¸”ë™ì­ ì¹´ë“œ ì¹´ìš´íŒ…ì— ì ìš©í•˜ì—¬ ì¹´ì§€ë…¸ë¥¼ ì´ê²¼ê³ , ë‚˜ì•„ê°€ ì£¼ì‹ì‹œì¥ì— ì ìš©í•˜ì—¬ ì—° 20%+ ìˆ˜ìµë¥ ì„ ë‹¬ì„±í–ˆë‹¤.
</p>

<p>
Kellyì˜ í•µì‹¬ ì§ˆë¬¸: "ìŠ¹ë¥ ê³¼ ë°°ë‹¹ë¥ ì´ ì•Œë ¤ì§„ ê²Œì„ì—ì„œ, ë§¤ë²ˆ ìì‚°ì˜ ëª‡ %ë¥¼ ê±¸ì–´ì•¼ ì¥ê¸°ì ìœ¼ë¡œ ìì‚°ì´ ìµœëŒ€ê°€ ë˜ëŠ”ê°€?"
</p>

<h3>9.2 Kelly ê³µì‹</h3>

<p>
ì´ì§„ ë² íŒ…(ì´ê¸°ë©´ bë°°, ì§€ë©´ ì „ì•¡ ì†ì‹¤)ì—ì„œ ìŠ¹ë¥ ì´ \(p\)ì¼ ë•Œ, ìµœì  ë² íŒ… ë¹„ìœ¨ \(f^*\)ëŠ”:
</p>

<div class="eq">
$$f^* = \frac{p \cdot b - (1-p)}{b} = p - \frac{1-p}{b} = \frac{bp - q}{b}$$
</div>

<p>
ì—¬ê¸°ì„œ \(q = 1-p\)ëŠ” íŒ¨ë°° í™•ë¥ ì´ë‹¤. ì´ê²ƒì€ <strong>ê¸°ëŒ€ ë¡œê·¸ ìˆ˜ìµë¥ ì„ ìµœëŒ€í™”</strong>í•˜ëŠ” ë¹„ìœ¨ì´ë‹¤:
</p>

<div class="eq">
$$f^* = \arg\max_f \; E[\log(1 + f \cdot R)]$$
</div>

<p>
ì—°ì† ìˆ˜ìµë¥  ë¶„í¬(ì •ê·œë¶„í¬ ê°€ì •)ì—ì„œ ë‹¤ìì‚° Kelly ë¹„ì¤‘ì€:
</p>

<div class="eq">
$$f^* = \Sigma^{-1} \mu$$
</div>

<p>
ë†€ëê²Œë„ ì´ê²ƒì€ Markowitzì˜ ìµœëŒ€ íš¨ìš© í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ \(\gamma = 1\)ì¸ ê²½ìš°ì™€ ë™ì¼í•˜ë‹¤. KellyëŠ” ìˆ˜ìµë¥  ê·¹ëŒ€í™”ì— "ì˜¬ì¸"í•˜ëŠ” ê³µê²©ì  ì „ëµì´ë¯€ë¡œ, ì‹¤ì „ì—ì„œëŠ” <strong>Half-Kelly</strong>(\(f^*/2\))ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.
</p>

<h3>9.3 íŒŒì´ì¬ êµ¬í˜„</h3>

<p class="cc">â–¼ Kelly Criterion í¬íŠ¸í´ë¦¬ì˜¤</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ì—°ê°„ ê¸°ëŒ€ìˆ˜ìµë¥ ê³¼ ê³µë¶„ì‚° í–‰ë ¬ (ì´ì „ ì±•í„°ì—ì„œ ê³„ì‚°)</span>
<span class="cm"># mu, Sigma ì‚¬ìš©</span>

<span class="cm"># Full Kelly</span>
Sigma_inv = np.linalg.<span class="fn">inv</span>(Sigma)
f_kelly = Sigma_inv @ mu

<span class="cm"># Half Kelly (ì‹¤ì „ ê¶Œì¥)</span>
f_half = f_kelly / <span class="nu">2</span>

<span class="cm"># ì •ê·œí™” (ë¹„ì¤‘ í•© = 1, ë ˆë²„ë¦¬ì§€ ì—†ëŠ” ê²½ìš°)</span>
w_kelly = f_half / np.<span class="fn">abs</span>(f_half).<span class="fn">sum</span>()

<span class="fn">print</span>(<span class="st">"=== Half-Kelly í¬íŠ¸í´ë¦¬ì˜¤ ==="</span>)
<span class="kw">for</span> i, t <span class="kw">in</span> <span class="fn">enumerate</span>(tickers):
    <span class="fn">print</span>(<span class="st">f"  {t}: {w_kelly[i]*100:.1f}%"</span>)
</pre>

<div class="warn">
<p class="ni"><strong>âš ï¸ Kellyì˜ ìœ„í—˜:</strong> Full KellyëŠ” ì´ë¡ ì ìœ¼ë¡œ ìµœì ì´ì§€ë§Œ, ì‹¤ì „ì—ì„œëŠ” (1) ìˆ˜ìµë¥  ë¶„í¬ê°€ ì •ê·œë¶„í¬ê°€ ì•„ë‹ˆê³ , (2) íŒŒë¼ë¯¸í„° ì¶”ì •ì— ì˜¤ì°¨ê°€ ìˆìœ¼ë©°, (3) ë³€ë™ì„±ì´ ê·¹ë„ë¡œ í¬ë‹¤. ë”°ë¼ì„œ Half-Kelly ë˜ëŠ” Quarter-Kellyë¥¼ ì‚¬ìš©í•˜ê³ , ìµœëŒ€ ë¹„ì¤‘ ì œí•œì„ ë‘ëŠ” ê²ƒì´ í˜„ëª…í•˜ë‹¤.</p>
</div>

<h3>9.4 í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ê¸°ë²• ë¹„êµ</h3>

<table>
<tr><th>ê¸°ë²•</th><th>í•µì‹¬ ì•„ì´ë””ì–´</th><th>ì¥ì </th><th>ë‹¨ì </th><th>ì‹¤ì „ ì‚¬ìš©</th></tr>
<tr><td>Markowitz (MVO)</td><td>ìˆ˜ìµ-ë¦¬ìŠ¤í¬ íŠ¸ë ˆì´ë“œì˜¤í”„</td><td>ì´ë¡ ì  ìµœì </td><td>ì¶”ì • ì˜¤ì°¨ì— ë¯¼ê°</td><td>âš ï¸ ì •ê·œí™” í•„ìš”</td></tr>
<tr><td>Black-Litterman</td><td>ì‹œì¥ ê· í˜• + ì „ë§</td><td>ì•ˆì •ì , ì§ê´€ì </td><td>ì „ë§ ì„¤ì • ì£¼ê´€ì </td><td>âœ… ê¸°ê´€ í‘œì¤€</td></tr>
<tr><td>Risk Parity</td><td>ë¦¬ìŠ¤í¬ ê· ë“± ë°°ë¶„</td><td>ë¶„ì‚° íš¨ê³¼ ê·¹ëŒ€í™”</td><td>ìˆ˜ìµë¥  ë¬´ì‹œ</td><td>âœ… Bridgewater</td></tr>
<tr><td>HRP</td><td>ML í´ëŸ¬ìŠ¤í„°ë§</td><td>ì—­í–‰ë ¬ ë¶ˆí•„ìš”, ê°•ê±´</td><td>ì´ë¡ ì  ê·¼ê±° ì•½í•¨</td><td>âœ… í€€íŠ¸ í€ë“œ</td></tr>
<tr><td>Kelly</td><td>ë¡œê·¸ ìì‚° ê·¹ëŒ€í™”</td><td>ì¥ê¸° ì„±ì¥ ìµœì </td><td>ë³€ë™ì„± ê·¹ì‹¬</td><td>âš ï¸ Half-Kelly</td></tr>
</table>
<p class="ni tc">Table 9.1: í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ê¸°ë²• ë¹„êµ</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 10: Transformer â€” Self-Attention ë©”ì»¤ë‹ˆì¦˜
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch10">Chapter 10. Transformer: Self-Attention ë©”ì»¤ë‹ˆì¦˜</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.16 "New frontiers â€” pretrained transformer models" ì„¹ì…˜ â€” Attention ë©”ì»¤ë‹ˆì¦˜ê³¼ Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ì„ ë‹¤ë£¬ë‹¤. ì›ë³¸ ë…¼ë¬¸: Vaswani et al. (2017) "Attention Is All You Need".</p>
</div>

<h3>10.1 RNN/LSTMì˜ í•œê³„ â€” ì™œ Transformerê°€ í•„ìš”í•œê°€</h3>

<p>
R7ì—ì„œ ìš°ë¦¬ëŠ” LSTMìœ¼ë¡œ ì‹œê³„ì—´ì„ ì˜ˆì¸¡í–ˆë‹¤. LSTMì€ ê°•ë ¥í•˜ì§€ë§Œ ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆë‹¤:
</p>

<ul>
<li><strong>ìˆœì°¨ ì²˜ë¦¬:</strong> ì‹œì  1 â†’ 2 â†’ 3 â†’ ... ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ ë³‘ë ¬í™”ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ì‹œí€€ìŠ¤ê°€ ê¸¸ë©´ í•™ìŠµì´ ë§¤ìš° ëŠë¦¬ë‹¤.</li>
<li><strong>ì¥ê±°ë¦¬ ì˜ì¡´ì„±:</strong> LSTMì´ Vanishing Gradientë¥¼ ì™„í™”í–ˆì§€ë§Œ, ìˆ˜ë°±~ìˆ˜ì²œ ì‹œì  ë–¨ì–´ì§„ ì •ë³´ë¥¼ í¬ì°©í•˜ê¸°ëŠ” ì—¬ì „íˆ ì–´ë µë‹¤.</li>
<li><strong>ê³ ì • ê¸¸ì´ ì»¨í…ìŠ¤íŠ¸:</strong> ì€ë‹‰ ìƒíƒœ(hidden state)ë¼ëŠ” ê³ ì • í¬ê¸° ë²¡í„°ì— ëª¨ë“  ê³¼ê±° ì •ë³´ë¥¼ ì••ì¶•í•´ì•¼ í•œë‹¤. ì •ë³´ ë³‘ëª©ì´ ë°œìƒí•œë‹¤.</li>
</ul>

<p>
2017ë…„, Googleì˜ ì—°êµ¬íŒ€ì´ "Attention Is All You Need"ë¼ëŠ” ë…¼ë¬¸ìœ¼ë¡œ Transformerë¥¼ ë°œí‘œí–ˆë‹¤. ì´ ì•„í‚¤í…ì²˜ëŠ” RNNì„ ì™„ì „íˆ ì œê±°í•˜ê³ , <strong>Self-Attention</strong>ì´ë¼ëŠ” ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•œë‹¤. ê²°ê³¼ëŠ” í˜ëª…ì ì´ì—ˆë‹¤ â€” ë²ˆì—­ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆê³ , ì´í›„ GPT, BERT, ChatGPT ë“± í˜„ëŒ€ AIì˜ ê¸°ë°˜ì´ ë˜ì—ˆë‹¤.
</p>

<h3>10.2 Attentionì˜ ì§ê´€</h3>

<p>
Attentionì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ë¹„ìœ ë¡œ ì„¤ëª…í•˜ì. ë‹¹ì‹ ì´ ë„ì„œê´€ì—ì„œ "í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”"ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ì“´ë‹¤ê³  í•˜ì. ì±…ì¥ì— 100ê¶Œì˜ ì±…ì´ ìˆë‹¤. ëª¨ë“  ì±…ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì½ì„ ìˆ˜ëŠ” ì—†ë‹¤(RNN ë°©ì‹). ëŒ€ì‹ , ê° ì±…ì˜ ëª©ì°¨ë¥¼ í›‘ì–´ë³´ê³  "ì´ ì±…ì´ ë‚´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ê°€?"ë¥¼ ì ìˆ˜ë¡œ ë§¤ê¸´ ë’¤, ê´€ë ¨ë„ê°€ ë†’ì€ ì±…ì— ë” ë§ì€ ì‹œê°„ì„ íˆ¬ìí•œë‹¤. ì´ê²ƒì´ Attentionì´ë‹¤.
</p>

<p>
ìˆ˜í•™ì ìœ¼ë¡œ, Attentionì€ ì„¸ ê°€ì§€ ë²¡í„°ë¡œ ì‘ë™í•œë‹¤:
</p>

<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fff8e1,#fff3e0);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:15px;color:#e65100">ğŸ”‘ Query, Key, Value â€” Attentionì˜ 3ìš”ì†Œ</p>
<div style="display:flex;gap:12px;flex-wrap:wrap;justify-content:center">
<div style="flex:1;min-width:180px;background:#fff;padding:16px;border-radius:10px;text-align:center;border-top:4px solid #ff6f00">
<div style="font-size:32px;margin-bottom:8px">â“</div>
<p class="ni" style="font-weight:bold;font-size:13px;color:#e65100;margin-bottom:6px">Query (Q)</p>
<p class="ni" style="font-size:11px;color:#666">"ë‚´ê°€ ì°¾ê³  ìˆëŠ” ê²ƒ"<br>í˜„ì¬ ì‹œì ì˜ ì§ˆë¬¸</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:6px">ë„ì„œê´€ ë¹„ìœ : "í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì— ëŒ€í•´ ì•Œê³  ì‹¶ë‹¤"</p>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:16px;border-radius:10px;text-align:center;border-top:4px solid #ff6f00">
<div style="font-size:32px;margin-bottom:8px">ğŸ”‘</div>
<p class="ni" style="font-weight:bold;font-size:13px;color:#e65100;margin-bottom:6px">Key (K)</p>
<p class="ni" style="font-size:11px;color:#666">"ê° í•­ëª©ì˜ ë¼ë²¨"<br>ë‹¤ë¥¸ ì‹œì ë“¤ì˜ ì‹ë³„ì</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:6px">ë„ì„œê´€ ë¹„ìœ : ê° ì±…ì˜ ëª©ì°¨/ì œëª©</p>
</div>
<div style="flex:1;min-width:180px;background:#fff;padding:16px;border-radius:10px;text-align:center;border-top:4px solid #ff6f00">
<div style="font-size:32px;margin-bottom:8px">ğŸ“„</div>
<p class="ni" style="font-weight:bold;font-size:13px;color:#e65100;margin-bottom:6px">Value (V)</p>
<p class="ni" style="font-size:11px;color:#666">"ì‹¤ì œ ë‚´ìš©"<br>ë‹¤ë¥¸ ì‹œì ë“¤ì˜ ì •ë³´</p>
<p class="ni" style="font-size:11px;color:#888;margin-top:6px">ë„ì„œê´€ ë¹„ìœ : ê° ì±…ì˜ ì‹¤ì œ ë‚´ìš©</p>
</div>
</div>
</div>

<h3>10.3 Scaled Dot-Product Attention</h3>

<p>
Attentionì˜ ìˆ˜í•™ì  ê³µì‹ì€ ë†€ëë„ë¡ ê°„ê²°í•˜ë‹¤:
</p>

<div class="eq">
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
</div>

<p>
ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ë©´:
</p>

<ol>
<li>\(QK^T\): Queryì™€ Keyì˜ ë‚´ì  â†’ ìœ ì‚¬ë„ ì ìˆ˜ (ì–´ë–¤ ì‹œì ì´ í˜„ì¬ì™€ ê´€ë ¨ ìˆëŠ”ê°€?)</li>
<li>\(\div \sqrt{d_k}\): ìŠ¤ì¼€ì¼ë§ (ì°¨ì›ì´ í´ ë•Œ ë‚´ì ê°’ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€)</li>
<li>\(\text{softmax}\): ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (í•© = 1, ëª¨ë‘ ì–‘ìˆ˜)</li>
<li>\(\times V\): í™•ë¥  ê°€ì¤‘ í‰ê· ìœ¼ë¡œ Valueë¥¼ ê²°í•© â†’ ìµœì¢… ì¶œë ¥</li>
</ol>

<p class="cc">â–¼ Scaled Dot-Product Attention êµ¬í˜„ (PyTorch)</p>
<pre>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F
<span class="kw">import</span> math

<span class="kw">def</span> <span class="fn">scaled_dot_product_attention</span>(Q, K, V, mask=<span class="kw">None</span>):
    <span class="st">"""
    Q, K, V: (batch, seq_len, d_k)
    Returns: (batch, seq_len, d_k)
    """</span>
    d_k = Q.<span class="fn">size</span>(-<span class="nu">1</span>)

    <span class="cm"># Step 1: Qì™€ Kì˜ ë‚´ì  â†’ ìœ ì‚¬ë„ ì ìˆ˜</span>
    scores = torch.<span class="fn">matmul</span>(Q, K.<span class="fn">transpose</span>(-<span class="nu">2</span>, -<span class="nu">1</span>)) / math.<span class="fn">sqrt</span>(d_k)
    <span class="cm"># scores shape: (batch, seq_len, seq_len)</span>

    <span class="cm"># Step 2: ë§ˆìŠ¤í‚¹ (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨, ì„ íƒì )</span>
    <span class="kw">if</span> mask <span class="kw">is not None</span>:
        scores = scores.<span class="fn">masked_fill</span>(mask == <span class="nu">0</span>, <span class="nu">-1e9</span>)

    <span class="cm"># Step 3: Softmax â†’ ì–´í…ì…˜ ê°€ì¤‘ì¹˜</span>
    attn_weights = F.<span class="fn">softmax</span>(scores, dim=-<span class="nu">1</span>)

    <span class="cm"># Step 4: ê°€ì¤‘ í‰ê· </span>
    output = torch.<span class="fn">matmul</span>(attn_weights, V)
    <span class="kw">return</span> output, attn_weights

<span class="cm"># í…ŒìŠ¤íŠ¸</span>
batch, seq_len, d_k = <span class="nu">2</span>, <span class="nu">10</span>, <span class="nu">64</span>
Q = torch.<span class="fn">randn</span>(batch, seq_len, d_k)
K = torch.<span class="fn">randn</span>(batch, seq_len, d_k)
V = torch.<span class="fn">randn</span>(batch, seq_len, d_k)

output, weights = <span class="fn">scaled_dot_product_attention</span>(Q, K, V)
<span class="fn">print</span>(<span class="st">f"Output shape: {output.shape}"</span>)    <span class="cm"># (2, 10, 64)</span>
<span class="fn">print</span>(<span class="st">f"Weights shape: {weights.shape}"</span>)  <span class="cm"># (2, 10, 10)</span>
<span class="fn">print</span>(<span class="st">f"Weights sum: {weights.sum(dim=-1)}"</span>)  <span class="cm"># ê° í–‰ì˜ í•© = 1</span>
</pre>
<div class="code-output"><span class="out-label">Output:</span>
Output shape: torch.Size([2, 10, 64])
Weights shape: torch.Size([2, 10, 10])
Weights sum: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
                     [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</div>

<p>
<code>weights</code>ì˜ shapeì´ (batch, 10, 10)ì¸ ê²ƒì— ì£¼ëª©í•˜ì. ì´ê²ƒì€ 10ê°œ ì‹œì  ê°ê°ì´ ë‹¤ë¥¸ 10ê°œ ì‹œì ì— ì–¼ë§ˆë‚˜ "ì£¼ì˜"ë¥¼ ê¸°ìš¸ì´ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” <strong>ì–´í…ì…˜ ë§µ</strong>ì´ë‹¤. ê¸ˆìœµ ì‹œê³„ì—´ì—ì„œ ì´ ë§µì„ ì‹œê°í™”í•˜ë©´, ëª¨ë¸ì´ ì–´ë–¤ ê³¼ê±° ì‹œì ì„ ì¤‘ìš”í•˜ê²Œ ë³´ëŠ”ì§€ í•´ì„í•  ìˆ˜ ìˆë‹¤.
</p>

<h3>10.4 Self-Attentionì˜ ì˜ë¯¸</h3>

<p>
"Self"-Attentionì´ë¼ ë¶ˆë¦¬ëŠ” ì´ìœ ëŠ” Q, K, Vê°€ ëª¨ë‘ <strong>ê°™ì€ ì‹œí€€ìŠ¤</strong>ì—ì„œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì´ë‹¤. ì…ë ¥ ì‹œí€€ìŠ¤ \(X\)ì— ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•˜ì—¬ Q, K, Vë¥¼ ë§Œë“ ë‹¤:
</p>

<div class="eq">
$$Q = XW^Q, \quad K = XW^K, \quad V = XW^V$$
</div>

<p>
ì´ê²ƒì€ ì‹œí€€ìŠ¤ê°€ "ìê¸° ìì‹ ì„ ì°¸ì¡°"í•˜ì—¬ ê° ì‹œì ì˜ í‘œí˜„ì„ í’ë¶€í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤. RNNì´ ìˆœì°¨ì ìœ¼ë¡œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, Self-Attentionì€ <strong>ëª¨ë“  ì‹œì  ìŒì˜ ê´€ê³„ë¥¼ ë™ì‹œì—</strong> ê³„ì‚°í•œë‹¤. ì‹œì  1ê³¼ ì‹œì  100ì˜ ê´€ê³„ë„ í•œ ë²ˆì˜ ì—°ì‚°ìœ¼ë¡œ í¬ì°©í•  ìˆ˜ ìˆë‹¤.
</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 11: Positional Encoding + Multi-Head Attention
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch11">Chapter 11. Positional Encoding + Multi-Head Attention</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.16 "Attention is all you need" ì„¹ì…˜ â€” Transformerì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œì¸ Positional Encodingê³¼ Multi-Head Attentionì„ ë‹¤ë£¬ë‹¤. MLAT Ch.20~21ì—ì„œ Autoencoder/GANê³¼ ê²°í•©í•œ ê³ ê¸‰ ì•„í‚¤í…ì²˜ë„ ì†Œê°œëœë‹¤.</p>
</div>

<h3>11.1 Positional Encoding â€” ìˆœì„œ ì •ë³´ ì£¼ì…</h3>

<p>
Self-Attentionì—ëŠ” ì¹˜ëª…ì ì¸ ì•½ì ì´ í•˜ë‚˜ ìˆë‹¤: <strong>ìˆœì„œë¥¼ ëª¨ë¥¸ë‹¤.</strong> "ì›”ìš”ì¼ ìƒìŠ¹ â†’ í™”ìš”ì¼ í•˜ë½"ê³¼ "í™”ìš”ì¼ í•˜ë½ â†’ ì›”ìš”ì¼ ìƒìŠ¹"ì„ êµ¬ë¶„í•˜ì§€ ëª»í•œë‹¤. RNNì€ ìˆœì°¨ ì²˜ë¦¬ ìì²´ê°€ ìˆœì„œ ì •ë³´ë¥¼ ë‚´í¬í•˜ì§€ë§Œ, Attentionì€ ëª¨ë“  ì‹œì ì„ ë™ì‹œì— ë³´ë¯€ë¡œ ìˆœì„œ ê°œë…ì´ ì—†ë‹¤.
</p>

<p>
ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ TransformerëŠ” ì…ë ¥ì— <strong>ìœ„ì¹˜ ì •ë³´</strong>(Positional Encoding)ë¥¼ ë”í•œë‹¤. ì›ë³¸ ë…¼ë¬¸ì€ ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤:
</p>

<div class="eq">
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
</div>

<p>
ì—¬ê¸°ì„œ \(pos\)ëŠ” ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜, \(i\)ëŠ” ì°¨ì› ì¸ë±ìŠ¤, \(d_{\text{model}}\)ì€ ëª¨ë¸ ì°¨ì›ì´ë‹¤. ì™œ ì‚¬ì¸/ì½”ì‚¬ì¸ì¸ê°€? ë‘ ê°€ì§€ ì´ìœ ê°€ ìˆë‹¤:
</p>

<ul>
<li><strong>ìƒëŒ€ì  ìœ„ì¹˜ í‘œí˜„:</strong> \(PE_{pos+k}\)ë¥¼ \(PE_{pos}\)ì˜ ì„ í˜• ë³€í™˜ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì–´, ëª¨ë¸ì´ ìƒëŒ€ì  ê±°ë¦¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</li>
<li><strong>ì„ì˜ ê¸¸ì´ ì¼ë°˜í™”:</strong> í•™ìŠµ ë°ì´í„°ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë‹¤.</li>
</ul>

<p class="cc">â–¼ Positional Encoding êµ¬í˜„</p>
<pre>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> math

<span class="kw">class</span> <span class="nb">PositionalEncoding</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, d_model, max_len=<span class="nu">5000</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        pe = torch.<span class="fn">zeros</span>(max_len, d_model)
        position = torch.<span class="fn">arange</span>(<span class="nu">0</span>, max_len, dtype=torch.float).<span class="fn">unsqueeze</span>(<span class="nu">1</span>)
        div_term = torch.<span class="fn">exp</span>(
            torch.<span class="fn">arange</span>(<span class="nu">0</span>, d_model, <span class="nu">2</span>).float() * (-math.<span class="fn">log</span>(<span class="nu">10000.0</span>) / d_model)
        )
        pe[:, <span class="nu">0</span>::<span class="nu">2</span>] = torch.<span class="fn">sin</span>(position * div_term)  <span class="cm"># ì§ìˆ˜ ì°¨ì›</span>
        pe[:, <span class="nu">1</span>::<span class="nu">2</span>] = torch.<span class="fn">cos</span>(position * div_term)  <span class="cm"># í™€ìˆ˜ ì°¨ì›</span>
        pe = pe.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)  <span class="cm"># (1, max_len, d_model)</span>
        self.<span class="fn">register_buffer</span>(<span class="st">'pe'</span>, pe)

    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        <span class="cm"># x: (batch, seq_len, d_model)</span>
        <span class="kw">return</span> x + self.pe[:, :x.<span class="fn">size</span>(<span class="nu">1</span>), :]

<span class="cm"># í…ŒìŠ¤íŠ¸</span>
pe = <span class="fn">PositionalEncoding</span>(d_model=<span class="nu">64</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">2</span>, <span class="nu">20</span>, <span class="nu">64</span>)  <span class="cm"># batch=2, seq=20, dim=64</span>
out = <span class="fn">pe</span>(x)
<span class="fn">print</span>(<span class="st">f"Input shape: {x.shape} â†’ Output shape: {out.shape}"</span>)
</pre>
<div class="code-output"><span class="out-label">Output:</span>
Input shape: torch.Size([2, 20, 64]) â†’ Output shape: torch.Size([2, 20, 64])</div>

<h3>11.2 Multi-Head Attention â€” ë‹¤ì–‘í•œ ê´€ì ìœ¼ë¡œ ë³´ê¸°</h3>

<p>
í•˜ë‚˜ì˜ Attentionì€ í•˜ë‚˜ì˜ "ê´€ì "ìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ë³¸ë‹¤. í•˜ì§€ë§Œ ê¸ˆìœµ ì‹œê³„ì—´ì—ëŠ” ë‹¤ì–‘í•œ íŒ¨í„´ì´ ê³µì¡´í•œë‹¤ â€” ë‹¨ê¸° ëª¨ë©˜í…€, ì¥ê¸° ì¶”ì„¸, ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§, ì„¹í„° ìƒê´€ê´€ê³„ ë“±. <strong>Multi-Head Attention</strong>ì€ ì—¬ëŸ¬ ê°œì˜ Attentionì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì„ ë™ì‹œì— í¬ì°©í•œë‹¤.
</p>

<div class="eq">
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O$$
$$\text{where} \;\; \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$
</div>

<p>
ì˜ˆë¥¼ ë“¤ì–´ \(d_{\text{model}} = 64\)ì´ê³  \(h = 8\) í—¤ë“œë¼ë©´, ê° í—¤ë“œëŠ” \(d_k = 64/8 = 8\) ì°¨ì›ì—ì„œ ì‘ë™í•œë‹¤. 8ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ "ê´€ì "ì´ ë³‘ë ¬ë¡œ ê³„ì‚°ë˜ê³ , ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… ì¶œë ¥ì„ ë§Œë“ ë‹¤.
</p>

<p class="cc">â–¼ Multi-Head Attention êµ¬í˜„</p>
<pre>
<span class="kw">class</span> <span class="nb">MultiHeadAttention</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, d_model, n_heads):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        <span class="kw">assert</span> d_model % n_heads == <span class="nu">0</span>
        self.d_k = d_model // n_heads
        self.n_heads = n_heads

        self.W_q = nn.<span class="fn">Linear</span>(d_model, d_model)
        self.W_k = nn.<span class="fn">Linear</span>(d_model, d_model)
        self.W_v = nn.<span class="fn">Linear</span>(d_model, d_model)
        self.W_o = nn.<span class="fn">Linear</span>(d_model, d_model)

    <span class="kw">def</span> <span class="fn">forward</span>(self, Q, K, V, mask=<span class="kw">None</span>):
        batch = Q.<span class="fn">size</span>(<span class="nu">0</span>)

        <span class="cm"># ì„ í˜• ë³€í™˜ í›„ í—¤ë“œ ë¶„ë¦¬</span>
        Q = self.<span class="fn">W_q</span>(Q).<span class="fn">view</span>(batch, -<span class="nu">1</span>, self.n_heads, self.d_k).<span class="fn">transpose</span>(<span class="nu">1</span>, <span class="nu">2</span>)
        K = self.<span class="fn">W_k</span>(K).<span class="fn">view</span>(batch, -<span class="nu">1</span>, self.n_heads, self.d_k).<span class="fn">transpose</span>(<span class="nu">1</span>, <span class="nu">2</span>)
        V = self.<span class="fn">W_v</span>(V).<span class="fn">view</span>(batch, -<span class="nu">1</span>, self.n_heads, self.d_k).<span class="fn">transpose</span>(<span class="nu">1</span>, <span class="nu">2</span>)
        <span class="cm"># shape: (batch, n_heads, seq_len, d_k)</span>

        <span class="cm"># Scaled Dot-Product Attention</span>
        scores = torch.<span class="fn">matmul</span>(Q, K.<span class="fn">transpose</span>(-<span class="nu">2</span>, -<span class="nu">1</span>)) / math.<span class="fn">sqrt</span>(self.d_k)
        <span class="kw">if</span> mask <span class="kw">is not None</span>:
            scores = scores.<span class="fn">masked_fill</span>(mask == <span class="nu">0</span>, <span class="nu">-1e9</span>)
        attn = F.<span class="fn">softmax</span>(scores, dim=-<span class="nu">1</span>)
        context = torch.<span class="fn">matmul</span>(attn, V)

        <span class="cm"># í—¤ë“œ í•©ì¹˜ê¸°</span>
        context = context.<span class="fn">transpose</span>(<span class="nu">1</span>, <span class="nu">2</span>).<span class="fn">contiguous</span>().<span class="fn">view</span>(batch, -<span class="nu">1</span>, self.n_heads * self.d_k)
        output = self.<span class="fn">W_o</span>(context)
        <span class="kw">return</span> output

<span class="cm"># í…ŒìŠ¤íŠ¸</span>
mha = <span class="fn">MultiHeadAttention</span>(d_model=<span class="nu">64</span>, n_heads=<span class="nu">8</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">2</span>, <span class="nu">20</span>, <span class="nu">64</span>)
out = <span class="fn">mha</span>(x, x, x)  <span class="cm"># Self-Attention: Q=K=V=x</span>
<span class="fn">print</span>(<span class="st">f"MHA output: {out.shape}"</span>)  <span class="cm"># (2, 20, 64)</span>
</pre>
<div class="code-output"><span class="out-label">Output:</span>
MHA output: torch.Size([2, 20, 64])</div>

<h3>11.3 Transformer Encoder Block</h3>

<p>
Transformerì˜ EncoderëŠ” Multi-Head Attentionê³¼ Feed-Forward Networkë¥¼ ê²°í•©í•˜ê³ , ê°ê°ì— Residual Connectionê³¼ Layer Normalizationì„ ì ìš©í•œë‹¤. R7ì—ì„œ ë°°ìš´ ResNetì˜ skip connectionê³¼ ê°™ì€ ì›ë¦¬ë‹¤.
</p>

<!-- Transformer Encoder Block ì‹œê° ë‹¤ì´ì–´ê·¸ë¨ -->
<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#e0f2f1,#e8eaf6);border-radius:12px;box-shadow:0 4px 15px rgba(0,0,0,.08)">
<p class="ni" style="text-align:center;font-weight:bold;font-size:14px;margin-bottom:18px;color:#00695c">ğŸ—ï¸ Transformer Encoder Block</p>
<div style="max-width:350px;margin:0 auto">
<div style="background:#fff;padding:12px;border-radius:8px;text-align:center;border:2px solid #26a69a;margin-bottom:8px">
<p class="ni" style="font-size:12px;font-weight:bold;color:#00695c">Input Embedding + Positional Encoding</p>
</div>
<div style="text-align:center;font-size:18px;color:#26a69a">â†“</div>
<div style="background:#e8f5e9;padding:12px;border-radius:8px;text-align:center;border:2px solid #66bb6a;margin-bottom:4px">
<p class="ni" style="font-size:12px;font-weight:bold;color:#2e7d32">Multi-Head Self-Attention</p>
</div>
<div style="background:#fff;padding:6px;border-radius:6px;text-align:center;border:1px dashed #aaa;margin-bottom:8px">
<p class="ni" style="font-size:11px;color:#666">Add &amp; LayerNorm (Residual Connection)</p>
</div>
<div style="text-align:center;font-size:18px;color:#26a69a">â†“</div>
<div style="background:#e3f2fd;padding:12px;border-radius:8px;text-align:center;border:2px solid #42a5f5;margin-bottom:4px">
<p class="ni" style="font-size:12px;font-weight:bold;color:#1565c0">Feed-Forward Network (FFN)</p>
<p class="ni" style="font-size:10px;color:#666">Linear â†’ ReLU â†’ Linear</p>
</div>
<div style="background:#fff;padding:6px;border-radius:6px;text-align:center;border:1px dashed #aaa;margin-bottom:8px">
<p class="ni" style="font-size:11px;color:#666">Add &amp; LayerNorm (Residual Connection)</p>
</div>
<div style="text-align:center;font-size:18px;color:#26a69a">â†“</div>
<div style="background:#fff;padding:12px;border-radius:8px;text-align:center;border:2px solid #26a69a">
<p class="ni" style="font-size:12px;font-weight:bold;color:#00695c">Output â†’ ë‹¤ìŒ Encoder Block ë˜ëŠ” ì˜ˆì¸¡ í—¤ë“œ</p>
</div>
</div>
<p class="ni" style="text-align:center;font-size:11px;color:#888;margin-top:12px">ì´ ë¸”ë¡ì„ Në²ˆ ìŒ“ìœ¼ë©´ Transformer Encoderê°€ ì™„ì„±ëœë‹¤ (ì›ë³¸ ë…¼ë¬¸: N=6)</p>
</div>

<p class="cc">â–¼ Transformer Encoder Block êµ¬í˜„</p>
<pre>
<span class="kw">class</span> <span class="nb">TransformerEncoderBlock</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, d_model, n_heads, d_ff, dropout=<span class="nu">0.1</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.attention = <span class="fn">MultiHeadAttention</span>(d_model, n_heads)
        self.norm1 = nn.<span class="fn">LayerNorm</span>(d_model)
        self.norm2 = nn.<span class="fn">LayerNorm</span>(d_model)
        self.ffn = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(d_model, d_ff),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Dropout</span>(dropout),
            nn.<span class="fn">Linear</span>(d_ff, d_model),
            nn.<span class="fn">Dropout</span>(dropout)
        )
        self.dropout = nn.<span class="fn">Dropout</span>(dropout)

    <span class="kw">def</span> <span class="fn">forward</span>(self, x, mask=<span class="kw">None</span>):
        <span class="cm"># Multi-Head Attention + Residual + LayerNorm</span>
        attn_out = self.<span class="fn">attention</span>(x, x, x, mask)
        x = self.<span class="fn">norm1</span>(x + self.<span class="fn">dropout</span>(attn_out))

        <span class="cm"># FFN + Residual + LayerNorm</span>
        ffn_out = self.<span class="fn">ffn</span>(x)
        x = self.<span class="fn">norm2</span>(x + ffn_out)
        <span class="kw">return</span> x
</pre>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 12: ê¸ˆìœµ Transformer ì‹œê³„ì—´ ì˜ˆì¸¡
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch12">Chapter 12. ê¸ˆìœµ Transformer ì‹œê³„ì—´ ì˜ˆì¸¡ â€” ì‹¤ì „ ì ìš©</h2>

<div class="info">
<p class="ni"><strong>ğŸ“š êµì¬ ì—°ë™:</strong> MLAT Ch.20 "Autoencoders for Conditional Risk Factors" â€” ë”¥ëŸ¬ë‹ìœ¼ë¡œ ìì‚° ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ì‹¤ì „ ì•„í‚¤í…ì²˜. Ch.21 "GANs for Synthetic Time-Series Data" â€” TimeGANìœ¼ë¡œ í•©ì„± ê¸ˆìœµ ë°ì´í„° ìƒì„±. ì´ ì±•í„°ì—ì„œëŠ” Transformerë¥¼ ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì§ì ‘ ì ìš©í•œë‹¤.</p>
</div>

<h3>12.1 ê¸ˆìœµ ì‹œê³„ì—´ì„ ìœ„í•œ Transformer ì„¤ê³„</h3>

<p>
NLPì—ì„œ íƒ„ìƒí•œ Transformerë¥¼ ê¸ˆìœµ ì‹œê³„ì—´ì— ì ìš©í•˜ë ¤ë©´ ëª‡ ê°€ì§€ ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤:
</p>

<ul>
<li><strong>ì…ë ¥ ì„ë² ë”©:</strong> NLPì—ì„œëŠ” ë‹¨ì–´ë¥¼ ì„ë² ë”©í•˜ì§€ë§Œ, ê¸ˆìœµì—ì„œëŠ” ì—°ì†ì ì¸ ìˆ˜ì¹˜ í”¼ì²˜(ê°€ê²©, ìˆ˜ìµë¥ , ê±°ë˜ëŸ‰ ë“±)ë¥¼ ì„ í˜• ë³€í™˜ìœ¼ë¡œ ì„ë² ë”©í•œë‹¤.</li>
<li><strong>Causal Masking:</strong> ë¯¸ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ë¯€ë¡œ, í˜„ì¬ ì‹œì  ì´í›„ì˜ ì •ë³´ë¥¼ ë§ˆìŠ¤í‚¹í•œë‹¤. ì´ê²ƒì€ GPT ìŠ¤íƒ€ì¼ì˜ ë””ì½”ë” ë§ˆìŠ¤í‚¹ê³¼ ë™ì¼í•˜ë‹¤.</li>
<li><strong>ì¶œë ¥ í—¤ë“œ:</strong> ë¶„ë¥˜(ìƒìŠ¹/í•˜ë½)ì—ëŠ” Sigmoid/Softmax, íšŒê·€(ìˆ˜ìµë¥  ì˜ˆì¸¡)ì—ëŠ” Linear ì¶œë ¥ì„ ì‚¬ìš©í•œë‹¤.</li>
</ul>

<h3>12.2 ì „ì²´ ëª¨ë¸ êµ¬í˜„</h3>

<p class="cc">â–¼ ê¸ˆìœµ ì‹œê³„ì—´ Transformer ëª¨ë¸ (PyTorch)</p>
<pre>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> math

<span class="kw">class</span> <span class="nb">FinancialTransformer</span>(nn.Module):
    <span class="st">"""ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ Transformer ëª¨ë¸"""</span>

    <span class="kw">def</span> <span class="fn">__init__</span>(self, n_features, d_model=<span class="nu">64</span>, n_heads=<span class="nu">4</span>,
                 n_layers=<span class="nu">2</span>, d_ff=<span class="nu">128</span>, dropout=<span class="nu">0.1</span>,
                 max_len=<span class="nu">256</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()

        <span class="cm"># ì…ë ¥ ì„ë² ë”©: ìˆ˜ì¹˜ í”¼ì²˜ â†’ d_model ì°¨ì›</span>
        self.input_proj = nn.<span class="fn">Linear</span>(n_features, d_model)
        self.pos_enc = <span class="fn">PositionalEncoding</span>(d_model, max_len)
        self.dropout = nn.<span class="fn">Dropout</span>(dropout)

        <span class="cm"># Transformer Encoder ìŠ¤íƒ</span>
        self.encoder_layers = nn.<span class="fn">ModuleList</span>([
            <span class="fn">TransformerEncoderBlock</span>(d_model, n_heads, d_ff, dropout)
            <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layers)
        ])

        <span class="cm"># ì¶œë ¥ í—¤ë“œ: ë‹¤ìŒ ì‹œì  ìˆ˜ìµë¥  ì˜ˆì¸¡ (íšŒê·€)</span>
        self.output_head = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(d_model, d_model // <span class="nu">2</span>),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(d_model // <span class="nu">2</span>, <span class="nu">1</span>)
        )

    <span class="kw">def</span> <span class="fn">_generate_causal_mask</span>(self, seq_len, device):
        <span class="st">"""ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨ ë§ˆìŠ¤í¬"""</span>
        mask = torch.<span class="fn">tril</span>(torch.<span class="fn">ones</span>(seq_len, seq_len, device=device))
        <span class="kw">return</span> mask.<span class="fn">unsqueeze</span>(<span class="nu">0</span>).<span class="fn">unsqueeze</span>(<span class="nu">0</span>)  <span class="cm"># (1, 1, seq, seq)</span>

    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        <span class="st">"""
        x: (batch, seq_len, n_features)
        Returns: (batch, 1) â€” ë‹¤ìŒ ì‹œì  ìˆ˜ìµë¥  ì˜ˆì¸¡
        """</span>
        seq_len = x.<span class="fn">size</span>(<span class="nu">1</span>)
        mask = self.<span class="fn">_generate_causal_mask</span>(seq_len, x.device)

        <span class="cm"># ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©</span>
        x = self.<span class="fn">input_proj</span>(x)
        x = self.<span class="fn">pos_enc</span>(x)
        x = self.<span class="fn">dropout</span>(x)

        <span class="cm"># Transformer Encoder</span>
        <span class="kw">for</span> layer <span class="kw">in</span> self.encoder_layers:
            x = <span class="fn">layer</span>(x, mask)

        <span class="cm"># ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ìœ¼ë¡œ ì˜ˆì¸¡</span>
        x = x[:, -<span class="nu">1</span>, :]  <span class="cm"># (batch, d_model)</span>
        <span class="kw">return</span> self.<span class="fn">output_head</span>(x)
</pre>

<h3>12.3 í•™ìŠµ íŒŒì´í”„ë¼ì¸</h3>

<p class="cc">â–¼ ë°ì´í„° ì¤€ë¹„ + í•™ìŠµ ë£¨í”„</p>
<pre>
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> torch
<span class="kw">from</span> torch.utils.data <span class="kw">import</span> Dataset, DataLoader

<span class="cm"># â”€â”€ 1. ë°ì´í„° ì¤€ë¹„ â”€â”€</span>
<span class="kw">class</span> <span class="nb">StockDataset</span>(Dataset):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, ticker, start, end, seq_len=<span class="nu">60</span>):
        data = yf.<span class="fn">download</span>(ticker, start=start, end=end)
        close = data[<span class="st">'Close'</span>]
        <span class="kw">if</span> <span class="fn">isinstance</span>(close.columns, pd.MultiIndex):
            close = close.<span class="fn">droplevel</span>(<span class="st">'Ticker'</span>, axis=<span class="nu">1</span>)

        <span class="cm"># í”¼ì²˜: ìˆ˜ìµë¥ , 5ì¼ MA ë¹„ìœ¨, 20ì¼ MA ë¹„ìœ¨, ë³€ë™ì„±</span>
        df = pd.<span class="fn">DataFrame</span>()
        df[<span class="st">'return'</span>] = close.<span class="fn">pct_change</span>()
        df[<span class="st">'ma5_ratio'</span>] = close / close.<span class="fn">rolling</span>(<span class="nu">5</span>).<span class="fn">mean</span>() - <span class="nu">1</span>
        df[<span class="st">'ma20_ratio'</span>] = close / close.<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">mean</span>() - <span class="nu">1</span>
        df[<span class="st">'volatility'</span>] = close.<span class="fn">pct_change</span>().<span class="fn">rolling</span>(<span class="nu">20</span>).<span class="fn">std</span>()
        df = df.<span class="fn">dropna</span>()

        <span class="cm"># ì •ê·œí™” (z-score)</span>
        self.mean = df.<span class="fn">mean</span>()
        self.std = df.<span class="fn">std</span>()
        df = (df - self.mean) / self.std

        self.data = df.<span class="fn">values</span>.<span class="fn">astype</span>(np.float32)
        self.seq_len = seq_len

    <span class="kw">def</span> <span class="fn">__len__</span>(self):
        <span class="kw">return</span> <span class="fn">len</span>(self.data) - self.seq_len

    <span class="kw">def</span> <span class="fn">__getitem__</span>(self, idx):
        x = self.data[idx : idx + self.seq_len]
        y = self.data[idx + self.seq_len, <span class="nu">0</span>]  <span class="cm"># ë‹¤ìŒ ì‹œì  ìˆ˜ìµë¥ </span>
        <span class="kw">return</span> torch.<span class="fn">tensor</span>(x), torch.<span class="fn">tensor</span>(y)

<span class="cm"># â”€â”€ 2. ë°ì´í„°ì…‹ ìƒì„± â”€â”€</span>
<span class="kw">import</span> pandas <span class="kw">as</span> pd
train_ds = <span class="fn">StockDataset</span>(<span class="st">'AAPL'</span>, <span class="st">'2018-01-01'</span>, <span class="st">'2023-01-01'</span>, seq_len=<span class="nu">60</span>)
test_ds = <span class="fn">StockDataset</span>(<span class="st">'AAPL'</span>, <span class="st">'2023-01-01'</span>, <span class="st">'2024-06-01'</span>, seq_len=<span class="nu">60</span>)

train_loader = <span class="fn">DataLoader</span>(train_ds, batch_size=<span class="nu">32</span>, shuffle=<span class="kw">True</span>)
test_loader = <span class="fn">DataLoader</span>(test_ds, batch_size=<span class="nu">32</span>)

<span class="cm"># â”€â”€ 3. ëª¨ë¸ + í•™ìŠµ â”€â”€</span>
device = torch.device(<span class="st">'cuda'</span> <span class="kw">if</span> torch.cuda.<span class="fn">is_available</span>() <span class="kw">else</span> <span class="st">'cpu'</span>)
model = <span class="fn">FinancialTransformer</span>(
    n_features=<span class="nu">4</span>,    <span class="cm"># return, ma5_ratio, ma20_ratio, volatility</span>
    d_model=<span class="nu">64</span>,
    n_heads=<span class="nu">4</span>,
    n_layers=<span class="nu">2</span>,
    d_ff=<span class="nu">128</span>,
    dropout=<span class="nu">0.1</span>
).<span class="fn">to</span>(device)

optimizer = torch.optim.<span class="fn">Adam</span>(model.<span class="fn">parameters</span>(), lr=<span class="nu">1e-3</span>)
criterion = nn.<span class="fn">MSELoss</span>()

<span class="cm"># í•™ìŠµ ë£¨í”„</span>
<span class="kw">for</span> epoch <span class="kw">in</span> <span class="fn">range</span>(<span class="nu">50</span>):
    model.<span class="fn">train</span>()
    total_loss = <span class="nu">0</span>
    <span class="kw">for</span> x_batch, y_batch <span class="kw">in</span> train_loader:
        x_batch = x_batch.<span class="fn">to</span>(device)
        y_batch = y_batch.<span class="fn">to</span>(device)

        pred = <span class="fn">model</span>(x_batch).<span class="fn">squeeze</span>()
        loss = <span class="fn">criterion</span>(pred, y_batch)

        optimizer.<span class="fn">zero_grad</span>()
        loss.<span class="fn">backward</span>()
        torch.nn.utils.<span class="fn">clip_grad_norm_</span>(model.<span class="fn">parameters</span>(), <span class="nu">1.0</span>)
        optimizer.<span class="fn">step</span>()
        total_loss += loss.<span class="fn">item</span>()

    <span class="kw">if</span> (epoch + <span class="nu">1</span>) % <span class="nu">10</span> == <span class="nu">0</span>:
        avg_loss = total_loss / <span class="fn">len</span>(train_loader)
        <span class="fn">print</span>(<span class="st">f"Epoch {epoch+1:3d} | Train Loss: {avg_loss:.6f}"</span>)

<span class="cm"># â”€â”€ 4. í‰ê°€ â”€â”€</span>
model.<span class="fn">eval</span>()
preds, actuals = [], []
<span class="kw">with</span> torch.<span class="fn">no_grad</span>():
    <span class="kw">for</span> x_batch, y_batch <span class="kw">in</span> test_loader:
        pred = <span class="fn">model</span>(x_batch.<span class="fn">to</span>(device)).<span class="fn">squeeze</span>().<span class="fn">cpu</span>()
        preds.<span class="fn">extend</span>(pred.<span class="fn">tolist</span>())
        actuals.<span class="fn">extend</span>(y_batch.<span class="fn">tolist</span>())

<span class="cm"># ë°©í–¥ ì •í™•ë„ (Direction Accuracy)</span>
preds = np.<span class="fn">array</span>(preds)
actuals = np.<span class="fn">array</span>(actuals)
direction_acc = np.<span class="fn">mean</span>(np.<span class="fn">sign</span>(preds) == np.<span class="fn">sign</span>(actuals))
<span class="fn">print</span>(<span class="st">f"\nDirection Accuracy: {direction_acc*100:.1f}%"</span>)
<span class="fn">print</span>(<span class="st">f"(50% = random, >55% = tradeable signal)"</span>)
</pre>
<div class="code-output"><span class="out-label">Output (ì˜ˆì‹œ):</span>
Epoch  10 | Train Loss: 0.982341
Epoch  20 | Train Loss: 0.951287
Epoch  30 | Train Loss: 0.934562
Epoch  40 | Train Loss: 0.921845
Epoch  50 | Train Loss: 0.912103

Direction Accuracy: 53.2%
(50% = random, >55% = tradeable signal)</div>

<h3>12.4 Transformer vs LSTM ë¹„êµ</h3>

<table>
<tr><th>íŠ¹ì„±</th><th>LSTM (R7)</th><th>Transformer (R8)</th></tr>
<tr><td>ì²˜ë¦¬ ë°©ì‹</td><td>ìˆœì°¨ì  (Sequential)</td><td>ë³‘ë ¬ (Parallel)</td></tr>
<tr><td>ì¥ê±°ë¦¬ ì˜ì¡´ì„±</td><td>ì œí•œì  (Vanishing Gradient)</td><td>ìš°ìˆ˜ (Direct Attention)</td></tr>
<tr><td>í•™ìŠµ ì†ë„</td><td>ëŠë¦¼ (GPU í™œìš© ì œí•œ)</td><td>ë¹ ë¦„ (ì™„ì „ ë³‘ë ¬í™”)</td></tr>
<tr><td>í•´ì„ ê°€ëŠ¥ì„±</td><td>ë‚®ìŒ (ë¸”ë™ë°•ìŠ¤)</td><td>ë†’ìŒ (Attention Map)</td></tr>
<tr><td>ë°ì´í„° ìš”êµ¬ëŸ‰</td><td>ì ìŒ</td><td>ë§ìŒ</td></tr>
<tr><td>ì§§ì€ ì‹œê³„ì—´</td><td>âœ… ìœ ë¦¬</td><td>âš ï¸ ê³¼ì í•© ìœ„í—˜</td></tr>
<tr><td>ê¸´ ì‹œê³„ì—´</td><td>âš ï¸ ì„±ëŠ¥ ì €í•˜</td><td>âœ… ìœ ë¦¬</td></tr>
<tr><td>ê¸ˆìœµ ì ìš©</td><td>ë‹¨ê¸° ì˜ˆì¸¡, ì†Œê·œëª¨ ë°ì´í„°</td><td>ë‹¤ë³€ëŸ‰, ëŒ€ê·œëª¨ ë°ì´í„°</td></tr>
</table>
<p class="ni tc">Table 12.1: LSTM vs Transformer ë¹„êµ</p>

<div class="ok">
<p class="ni"><strong>ğŸ’¡ ì‹¤ì „ íŒ:</strong> ê¸ˆìœµ ì‹œê³„ì—´ì€ NLPì— ë¹„í•´ ë°ì´í„°ê°€ ì ê³  ë…¸ì´ì¦ˆê°€ ë§ë‹¤. ë”°ë¼ì„œ (1) ëª¨ë¸ì„ ì‘ê²Œ ìœ ì§€í•˜ê³  (d_model=64, n_layers=2), (2) Dropoutì„ ì¶©ë¶„íˆ ì‚¬ìš©í•˜ë©°, (3) ë°©í–¥ ì •í™•ë„(Direction Accuracy)ë¥¼ ì£¼ìš” ì§€í‘œë¡œ ì‚¼ëŠ” ê²ƒì´ ì¢‹ë‹¤. 53%ë§Œ ë„˜ì–´ë„ ê±°ë˜ ë¹„ìš©ì„ ê³ ë ¤í•œ í›„ ìˆ˜ìµì„ ë‚¼ ê°€ëŠ¥ì„±ì´ ìˆë‹¤.</p>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     Chapter 13: Quiz + Mini Project
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ch13">Chapter 13. Quiz + Mini Project</h2>

<h3>13.1 í•µì‹¬ ê°œë… í€´ì¦ˆ</h3>

<div class="def">
<p class="ni"><strong>Q1.</strong> ë³¼ë¡ ìµœì í™” ë¬¸ì œì—ì„œ local minimum = global minimumì´ ë³´ì¥ë˜ëŠ” ì´ìœ ëŠ”?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: Ch.1ì˜ ê·¸ë¦‡ ë¹„ìœ ë¥¼ ë– ì˜¬ë ¤ë³´ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q2.</strong> KKT ì¡°ê±´ì˜ "ìƒë³´ ì´ì™„"(Complementary Slackness) \(\lambda_i g_i(x^*) = 0\)ì˜ ê²½ì œì  ì˜ë¯¸ëŠ”?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: ì œì•½ì´ í™œì„±(active)ì´ ì•„ë‹Œ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q3.</strong> Markowitz Mean-Variance ìµœì í™”ì˜ ê°€ì¥ í° ì‹¤ì „ì  í•œê³„ëŠ”? ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ëŒ€ì•ˆ 3ê°€ì§€ë¥¼ ë‚˜ì—´í•˜ë¼.</p>
</div>

<div class="def">
<p class="ni"><strong>Q4.</strong> Black-Litterman ëª¨ë¸ì´ Markowitzë³´ë‹¤ ì•ˆì •ì ì¸ ì´ìœ ëŠ”?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: ì¶œë°œì (starting point)ì˜ ì°¨ì´ë¥¼ ìƒê°í•´ë³´ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q5.</strong> Risk Parityì—ì„œ TSLA(ê³ ë³€ë™ì„±)ì™€ MSFT(ì €ë³€ë™ì„±)ì˜ ë¹„ì¤‘ì€ ì–´ë–»ê²Œ ê²°ì •ë˜ëŠ”ê°€? ì™œ ê·¸ëŸ°ê°€?</p>
</div>

<div class="def">
<p class="ni"><strong>Q6.</strong> HRPê°€ ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì™œ ì¥ì ì¸ê°€?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: R2ì˜ ì¡°ê±´ìˆ˜(condition number)ë¥¼ ë– ì˜¬ë ¤ë³´ì.</p>
</div>

<div class="def">
<p class="ni"><strong>Q7.</strong> Transformerì˜ Self-Attentionì—ì„œ \(\sqrt{d_k}\)ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ”?</p>
</div>

<div class="def">
<p class="ni"><strong>Q8.</strong> Positional Encodingì´ í•„ìš”í•œ ì´ìœ ëŠ”? RNNì—ì„œëŠ” ì™œ í•„ìš” ì—†ì—ˆëŠ”ê°€?</p>
</div>

<div class="def">
<p class="ni"><strong>Q9.</strong> Multi-Head Attentionì—ì„œ í—¤ë“œ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ì–´ë–¤ ì¥ë‹¨ì ì´ ìˆëŠ”ê°€?</p>
</div>

<div class="def">
<p class="ni"><strong>Q10.</strong> ê¸ˆìœµ ì‹œê³„ì—´ Transformerì—ì„œ Causal Maskingì´ í•„ìˆ˜ì¸ ì´ìœ ëŠ”?</p>
<p class="ni" style="color:#888;font-size:12px;margin-top:6px">íŒíŠ¸: ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ(data leakage)ì„ ìƒê°í•´ë³´ì.</p>
</div>

<h3>13.2 Mini Project: CVXPY ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” + Transformer ìˆ˜ìµë¥  ì˜ˆì¸¡</h3>

<div style="margin:25px 0;padding:25px;background:linear-gradient(135deg,#e8eaf6,#ede7f6);border-radius:12px;border:2px solid #5c6bc0;box-shadow:0 4px 15px rgba(0,0,0,.1)">
<p class="ni" style="font-weight:bold;font-size:16px;color:#283593;margin-bottom:15px">ğŸ¯ Mini Project: ì˜ˆì¸¡ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”</p>

<p class="ni" style="font-size:13px;margin-bottom:12px">
<strong>ëª©í‘œ:</strong> Transformerë¡œ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•˜ê³ , ê·¸ ì˜ˆì¸¡ê°’ì„ CVXPY í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì  ë¹„ì¤‘ì„ ê²°ì •í•œë‹¤. ì˜ˆì¸¡ â†’ ìµœì í™” â†’ ë°±í…ŒìŠ¤íŠ¸ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤.
</p>

<p class="ni" style="font-size:13px;font-weight:bold;color:#3949ab;margin-bottom:8px">ğŸ“‹ ê³¼ì œ ë‹¨ê³„:</p>

<ol style="font-size:13px">
<li style="margin-bottom:8px"><strong>ë°ì´í„° ìˆ˜ì§‘:</strong> 5ì¢…ëª©(AAPL, MSFT, GOOGL, AMZN, JPM)ì˜ 2018~2024 ì¼ê°„ ë°ì´í„°ë¥¼ yfinanceë¡œ ë‹¤ìš´ë¡œë“œ</li>
<li style="margin-bottom:8px"><strong>Transformer í•™ìŠµ:</strong> ê° ì¢…ëª©ë³„ë¡œ FinancialTransformer ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ë‹¤ìŒ ë‚  ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡</li>
<li style="margin-bottom:8px"><strong>ì˜ˆì¸¡ ê¸°ëŒ€ìˆ˜ìµë¥  ìƒì„±:</strong> í•™ìŠµëœ ëª¨ë¸ë¡œ í–¥í›„ ê¸°ëŒ€ìˆ˜ìµë¥  ë²¡í„° \(\hat{\mu}\)ë¥¼ ìƒì„±</li>
<li style="margin-bottom:8px"><strong>CVXPY ìµœì í™”:</strong> \(\hat{\mu}\)ì™€ ê³µë¶„ì‚° í–‰ë ¬ \(\Sigma\)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ ê³„ì‚°</li>
<li style="margin-bottom:8px"><strong>ë¹„êµ ë°±í…ŒìŠ¤íŠ¸:</strong> ë‹¤ìŒ ì „ëµë“¤ì˜ ì„±ê³¼ë¥¼ ë¹„êµ
<ul style="margin-top:4px">
<li>ê· ë“± ë¹„ì¤‘ (1/N)</li>
<li>Markowitz MVO</li>
<li>Risk Parity</li>
<li>Transformer + CVXPY (ìš°ë¦¬ì˜ ì „ëµ)</li>
</ul>
</li>
<li style="margin-bottom:8px"><strong>ì„±ê³¼ ì§€í‘œ:</strong> ëˆ„ì  ìˆ˜ìµë¥ , ì—°ê°„ ìƒ¤í”„ë¹„ìœ¨, ìµœëŒ€ ë‚™í­(MDD), ì—°ê°„ ë³€ë™ì„±ì„ ê³„ì‚°í•˜ì—¬ ë¹„êµ í…Œì´ë¸” ì‘ì„±</li>
</ol>

<p class="ni" style="font-size:12px;color:#5c6bc0;margin-top:12px">
<strong>ğŸ’¡ íŒíŠ¸:</strong> ë§¤ì£¼ ê¸ˆìš”ì¼ì— í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë¦¬ë°¸ëŸ°ì‹±í•œë‹¤ê³  ê°€ì •í•˜ì. Transformer ì˜ˆì¸¡ì€ ì£¼ê°„ ìˆ˜ìµë¥ ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•œë‹¤. ê±°ë˜ ë¹„ìš©ì€ í¸ë„ 10bp(0.1%)ë¡œ ê°€ì •í•œë‹¤.
</p>
</div>

<p class="cc">â–¼ Mini Project ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ</p>
<pre>
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> cvxpy <span class="kw">as</span> cp
<span class="kw">import</span> yfinance <span class="kw">as</span> yf
<span class="kw">import</span> torch

<span class="cm"># â”€â”€ 1. ë°ì´í„° ìˆ˜ì§‘ â”€â”€</span>
tickers = [<span class="st">'AAPL'</span>, <span class="st">'MSFT'</span>, <span class="st">'GOOGL'</span>, <span class="st">'AMZN'</span>, <span class="st">'JPM'</span>]
data = yf.<span class="fn">download</span>(tickers, start=<span class="st">'2018-01-01'</span>, end=<span class="st">'2024-06-01'</span>)
prices = data[<span class="st">'Close'</span>]
returns = prices.<span class="fn">pct_change</span>().<span class="fn">dropna</span>()

<span class="cm"># â”€â”€ 2. Transformer ì˜ˆì¸¡ (ê° ì¢…ëª©ë³„) â”€â”€</span>
<span class="cm"># ... (Ch.12ì˜ FinancialTransformer ì‚¬ìš©)</span>
<span class="cm"># predicted_returns: ê° ì¢…ëª©ì˜ ì˜ˆì¸¡ ìˆ˜ìµë¥  ë²¡í„°</span>

<span class="cm"># â”€â”€ 3. CVXPY ìµœì í™” â”€â”€</span>
<span class="kw">def</span> <span class="fn">optimize_portfolio</span>(mu_pred, Sigma, rf=<span class="nu">0.05</span>/<span class="nu">252</span>):
    <span class="st">"""Transformer ì˜ˆì¸¡ ê¸°ë°˜ ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™”"""</span>
    n = <span class="fn">len</span>(mu_pred)
    w = cp.<span class="fn">Variable</span>(n)

    <span class="cm"># ëª©ì : ìƒ¤í”„ë¹„ìœ¨ ìµœëŒ€í™” (ë³€ìˆ˜ ë³€í™˜)</span>
    ret = mu_pred @ w
    risk = cp.<span class="fn">quad_form</span>(w, Sigma)

    objective = cp.<span class="fn">Maximize</span>(ret - <span class="nu">0.5</span> * risk)
    constraints = [
        cp.<span class="fn">sum</span>(w) == <span class="nu">1</span>,
        w >= <span class="nu">0</span>,
        w <= <span class="nu">0.4</span>  <span class="cm"># ìµœëŒ€ 40% ì§‘ì¤‘ ì œí•œ</span>
    ]

    prob = cp.<span class="fn">Problem</span>(objective, constraints)
    prob.<span class="fn">solve</span>()
    <span class="kw">return</span> w.value

<span class="cm"># â”€â”€ 4. ë°±í…ŒìŠ¤íŠ¸ â”€â”€</span>
<span class="cm"># ë§¤ì£¼ ë¦¬ë°¸ëŸ°ì‹±, ê±°ë˜ë¹„ìš© 10bp</span>
<span class="cm"># ... (êµ¬í˜„ì€ ì—¬ëŸ¬ë¶„ì˜ ëª«!)</span>

<span class="cm"># â”€â”€ 5. ì„±ê³¼ ë¹„êµ â”€â”€</span>
<span class="fn">print</span>(<span class="st">"=== ì „ëµ ì„±ê³¼ ë¹„êµ ==="</span>)
<span class="fn">print</span>(<span class="st">f"{'ì „ëµ':<20} {'ìƒ¤í”„ë¹„ìœ¨':>10} {'ì—°ìˆ˜ìµë¥ ':>10} {'MDD':>10}"</span>)
<span class="fn">print</span>(<span class="st">"-"</span> * <span class="nu">52</span>)
<span class="cm"># ... ê²°ê³¼ ì¶œë ¥</span>
</pre>

<h3>13.3 ë‹¤ìŒ ë¼ìš´ë“œ ì˜ˆê³ : R9 â€” HFT ì‹œìŠ¤í…œ ì„¤ê³„ + ê°•í™”í•™ìŠµ</h3>

<div style="margin:25px 0;padding:20px;background:linear-gradient(135deg,#fce4ec,#f3e5f5);border-radius:12px;border:2px solid #ab47bc">
<p class="ni" style="font-weight:bold;font-size:14px;color:#6a1b9a;margin-bottom:10px">ğŸš€ R9 Preview: HFT + Reinforcement Learning</p>
<p class="ni" style="font-size:13px">
R8ì—ì„œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì™€ Transformerë¥¼ ë§ˆìŠ¤í„°í–ˆë‹¤. R9ì—ì„œëŠ” ì´ ëª¨ë“  ê²ƒì„ <strong>ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ</strong>ìœ¼ë¡œ í†µí•©í•œë‹¤:
</p>
<ul style="font-size:13px;margin-top:8px">
<li><strong>ì‹œì¥ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜:</strong> ì˜¤ë”ë¶, í˜¸ê°€ì°½, ì²´ê²° ë©”ì»¤ë‹ˆì¦˜, ë ˆì´í„´ì‹œ</li>
<li><strong>HFT ì „ëµ:</strong> ë§ˆì¼“ë©”ì´í‚¹, í†µê³„ì  ì°¨ìµê±°ë˜, ëª¨ë©˜í…€ ì´ê·¸ë‹ˆì…˜</li>
<li><strong>ê°•í™”í•™ìŠµ:</strong> Q-Learning, DQN, PPO â€” ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œ ë§¤ë§¤ ì „ëµì„ í•™ìŠµ</li>
<li><strong>ì €ì§€ì—° ì•„í‚¤í…ì²˜:</strong> ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì„¤ê³„, ë¹„ë™ê¸° ì²˜ë¦¬</li>
</ul>
<p class="ni" style="font-size:12px;color:#888;margin-top:10px">êµì¬: MLAT Ch.22~23 (ê°•í™”í•™ìŠµ, íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸) / MLDSF Ch.13~14</p>
</div>

</div><!-- paper-content -->
</div><!-- container -->
</div><!-- main-wrapper -->

</body>
</html>
